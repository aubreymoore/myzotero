<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <title>AI-powered banana diseases and pest detection | Plant Methods | Full Text</title>

        

    <meta name="citation_abstract" content="Banana (Musa spp.) is the most popular marketable fruit crop grown all over the world, and a dominant staple food in many developing countries. Worldwide, banana production is affected by numerous diseases and pests. Novel and rapid methods for the timely detection of pests and diseases will allow to surveil and develop control measures with greater efficiency. As deep convolutional neural networks (DCNN) and transfer learning has been successfully applied in various fields, it has freshly moved in the domain of just-in-time crop disease detection. The aim of this research is to develop an AI-based banana disease and pest detection system using a DCNN to support banana farmers. Large datasets of expert pre-screened banana disease and pest symptom/damage images were collected from various hotspots in Africa and Southern India. To build a detection model, we retrained three different convolutional neural network (CNN) architectures using a transfer learning approach. A total of six different models were developed from 18 different classes (disease by plant parts) using images collected from different parts of the banana plant. Our studies revealed ResNet50 and InceptionV2 based models performed better compared to MobileNetV1. These architectures represent the state-of-the-art results of banana diseases and pest detection with an accuracy of more than 90% in most of the models tested. These experimental results were comparable with other state-of-the-art models found in the literature. With a future view to run these detection capabilities on a mobile device, we evaluated the performance of SSD (single shot detector) MobileNetV1. Performance and validation metrics were also computed to measure the accuracy of different models in automated disease detection methods. Our results showed that the DCNN was a robust and easily deployable strategy for digital banana disease and pest detection. Using a pre-trained disease recognition model, we were able to perform deep transfer learning (DTL) to produce a network that can make accurate predictions. This significant high success rate makes the model a useful early disease and pest detection tool, and this research could be further extended to develop a fully automated mobile app to help millions of banana farmers in developing countries.">

    <meta name="journal_id" content="13007">

    <meta name="dc.title" content="AI-powered banana diseases and pest detection">

    <meta name="dc.source" content="Plant Methods 2019 15:1">

    <meta name="dc.format" content="text/html">

    <meta name="dc.publisher" content="BioMed Central">

    <meta name="dc.date" content="2019-08-12">

    <meta name="dc.type" content="OriginalPaper">

    <meta name="dc.language" content="En">

    <meta name="dc.copyright" content="2019 The Author(s)">

    <meta name="dc.rightsAgent" content="reprints@biomedcentral.com">

    <meta name="dc.description" content="Banana (Musa spp.) is the most popular marketable fruit crop grown all over the world, and a dominant staple food in many developing countries. Worldwide, banana production is affected by numerous diseases and pests. Novel and rapid methods for the timely detection of pests and diseases will allow to surveil and develop control measures with greater efficiency. As deep convolutional neural networks (DCNN) and transfer learning has been successfully applied in various fields, it has freshly moved in the domain of just-in-time crop disease detection. The aim of this research is to develop an AI-based banana disease and pest detection system using a DCNN to support banana farmers. Large datasets of expert pre-screened banana disease and pest symptom/damage images were collected from various hotspots in Africa and Southern India. To build a detection model, we retrained three different convolutional neural network (CNN) architectures using a transfer learning approach. A total of six different models were developed from 18 different classes (disease by plant parts) using images collected from different parts of the banana plant. Our studies revealed ResNet50 and InceptionV2 based models performed better compared to MobileNetV1. These architectures represent the state-of-the-art results of banana diseases and pest detection with an accuracy of more than 90% in most of the models tested. These experimental results were comparable with other state-of-the-art models found in the literature. With a future view to run these detection capabilities on a mobile device, we evaluated the performance of SSD (single shot detector) MobileNetV1. Performance and validation metrics were also computed to measure the accuracy of different models in automated disease detection methods. Our results showed that the DCNN was a robust and easily deployable strategy for digital banana disease and pest detection. Using a pre-trained disease recognition model, we were able to perform deep transfer learning (DTL) to produce a network that can make accurate predictions. This significant high success rate makes the model a useful early disease and pest detection tool, and this research could be further extended to develop a fully automated mobile app to help millions of banana farmers in developing countries.">

    <meta name="prism.issn" content="1746-4811">

    <meta name="prism.publicationName" content="Plant Methods">

    <meta name="prism.publicationDate" content="2019-08-12">

    <meta name="prism.volume" content="15">

    <meta name="prism.number" content="1">

    <meta name="prism.section" content="OriginalPaper">

    <meta name="prism.startingPage" content="1">

    <meta name="prism.endingPage" content="11">

    <meta name="prism.copyright" content="2019 The Author(s)">

    <meta name="prism.rightsAgent" content="reprints@biomedcentral.com">

    <meta name="prism.url" content="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z">

    <meta name="prism.doi" content="doi:10.1186/s13007-019-0475-z">

    <meta name="citation_pdf_url" content="https://plantmethods.biomedcentral.com/track/pdf/10.1186/s13007-019-0475-z">

    <meta name="citation_fulltext_html_url" content="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z">

    <meta name="citation_journal_title" content="Plant Methods">

    <meta name="citation_journal_abbrev" content="Plant Methods">

    <meta name="citation_publisher" content="BioMed Central">

    <meta name="citation_issn" content="1746-4811">

    <meta name="citation_title" content="AI-powered banana diseases and pest detection">

    <meta name="citation_volume" content="15">

    <meta name="citation_issue" content="1">

    <meta name="citation_publication_date" content="2019/12">

    <meta name="citation_online_date" content="2019/08/12">

    <meta name="citation_firstpage" content="1">

    <meta name="citation_lastpage" content="11">

    <meta name="citation_article_type" content="Research">

    <meta name="citation_fulltext_world_readable" content="">

    <meta name="citation_language" content="en">

    <meta name="dc.identifier" content="doi:10.1186/s13007-019-0475-z">

    <meta name="DOI" content="10.1186/s13007-019-0475-z">

    <meta name="citation_doi" content="10.1186/s13007-019-0475-z">

    <meta name="description" content="Banana (Musa spp.) is the most popular marketable fruit crop grown all over the world, and a dominant staple food in many developing countries. Worldwide, banana production is affected by numerous diseases and pests. Novel and rapid methods for the timely detection of pests and diseases will allow to surveil and develop control measures with greater efficiency. As deep convolutional neural networks (DCNN) and transfer learning has been successfully applied in various fields, it has freshly moved in the domain of just-in-time crop disease detection. The aim of this research is to develop an AI-based banana disease and pest detection system using a DCNN to support banana farmers. Large datasets of expert pre-screened banana disease and pest symptom/damage images were collected from various hotspots in Africa and Southern India. To build a detection model, we retrained three different convolutional neural network (CNN) architectures using a transfer learning approach. A total of six different models were developed from 18 different classes (disease by plant parts) using images collected from different parts of the banana plant. Our studies revealed ResNet50 and InceptionV2 based models performed better compared to MobileNetV1. These architectures represent the state-of-the-art results of banana diseases and pest detection with an accuracy of more than 90% in most of the models tested. These experimental results were comparable with other state-of-the-art models found in the literature. With a future view to run these detection capabilities on a mobile device, we evaluated the performance of SSD (single shot detector) MobileNetV1. Performance and validation metrics were also computed to measure the accuracy of different models in automated disease detection methods. Our results showed that the DCNN was a robust and easily deployable strategy for digital banana disease and pest detection. Using a pre-trained disease recognition model, we were able to perform deep transfer learning (DTL) to produce a network that can make accurate predictions. This significant high success rate makes the model a useful early disease and pest detection tool, and this research could be further extended to develop a fully automated mobile app to help millions of banana farmers in developing countries.">

    <meta name="dc.creator" content="Michael Gomez Selvaraj">

    <meta name="dc.creator" content="Alejandro Vergara">

    <meta name="dc.creator" content="Henry Ruiz">

    <meta name="dc.creator" content="Nancy Safari">

    <meta name="dc.creator" content="Sivalingam Elayabalan">

    <meta name="dc.creator" content="Walter Ocimati">

    <meta name="dc.creator" content="Guy Blomme">

    <meta name="dc.subject" content="Plant Sciences">

    <meta name="dc.subject" content="Biological Techniques">

    <meta name="citation_reference" content="FAO. Banana market review and banana statistics 2012–2013. Market and policy analyses of raw materials, horticulture and tropical (RAMHOT) Products Team. Rome; 2014.">

    <meta name="citation_reference" content="Lescot T. World plantain and banana production systems. In: Proceedings XX international meeting ACORBAT: 9–13 September 2013; Fortaleza; 2013. p. 26–34.">

    <meta name="citation_reference" content="citation_title=Food security in eastern Africa and the great lakes. Crop Crisis Control Project final report; citation_publication_date=2007; citation_id=CR3; citation_author=S Abele; citation_author=E Twine; citation_author=C Legg; citation_publisher=Int Instit Trop Agric">

    <meta name="citation_reference" content="Nagayets O. Small farms: current status and key trends. In: The future of small farms; 2005. p. 355.">

    <meta name="citation_reference" content="citation_journal_title=Front Plant Sci; citation_title=Bacterial diseases of bananas and enset: current state of knowledge and integrated approaches toward sustainable management; citation_author=G Blomme, M Dita, KS Jacobsen, L Perez Vicente, A Molina, W Ocimati, S Poussier, P Prior; citation_volume=8; citation_publication_date=2017; citation_pages=1290; citation_doi=10.3389/fpls.2017.01290; citation_id=CR5">

    <meta name="citation_reference" content="citation_journal_title=Gesunde Pflanzen.; citation_title=Early detection and localisation of sugar beet diseases: new approaches; citation_author=C Hillnhuetter, AK Mahlein; citation_volume=60; citation_issue=4; citation_publication_date=2008; citation_pages=143-149; citation_doi=10.1007/s10343-008-0196-0; citation_id=CR6">

    <meta name="citation_reference" content="citation_journal_title=Biosyst Eng; citation_title=An image-processing based algorithm to automatically identify plant disease visual symptoms; citation_author=A Camargo, J Smith; citation_volume=102; citation_issue=1; citation_publication_date=2009; citation_pages=9-21; citation_doi=10.1016/j.biosystemseng.2008.09.030; citation_id=CR7">

    <meta name="citation_reference" content="citation_journal_title=Front Plant Sci; citation_title=Using deep learning for image-based plant disease detection; citation_author=SP Mohanty, DP Hughes, M Salathe; citation_volume=7; citation_publication_date=2016; citation_pages=1419; citation_doi=10.3389/fpls.2016.01419; citation_id=CR8">

    <meta name="citation_reference" content="citation_title=The mobile economy Africa 2016; citation_publication_date=2016; citation_id=CR9; citation_author=G Intelligence; citation_publisher=GSMA">

    <meta name="citation_reference" content="citation_journal_title=Comput Elect Agric; citation_title=Deep learning in agriculture: a survey; citation_author=A Kamilaris, FX Prenafeta-Boldu; citation_volume=147; citation_publication_date=2018; citation_pages=70-90; citation_doi=10.1016/j.compag.2018.02.016; citation_id=CR10">

    <meta name="citation_reference" content="citation_journal_title=Front Plant Sci; citation_title=Deep learning for image-based cassava disease detection; citation_author=A Ramcharan, K Baranowski, P McCloskey, B Ahmed, J Legg, DP Hughes; citation_volume=8; citation_publication_date=2017; citation_pages=1852; citation_doi=10.3389/fpls.2017.01852; citation_id=CR11">

    <meta name="citation_reference" content="citation_title=A lightweight mobile system for crop disease diagnosis; citation_inbook_title=International conference on image analysis and recognition; citation_publication_date=2016; citation_pages=783-791; citation_id=CR12; citation_author=P Siricharoen; citation_author=B Scotney; citation_author=P Morrow; citation_author=G Parr; citation_publisher=Springer">

    <meta name="citation_reference" content="citation_journal_title=BMC Res Notes.; citation_title=Image set for deep learning: field images of maize annotated with disease symptoms; citation_author=T Wiesner-Hanks, EL Stewart, N Kaczmar, C DeChant, H Wu, RJ Nelson, H Lipson, MA Gore; citation_volume=11; citation_issue=1; citation_publication_date=2018; citation_pages=440; citation_doi=10.1186/s13104-018-3548-6; citation_id=CR13">

    <meta name="citation_reference" content="citation_title=Machine learning for plant disease incidence and severity measurements from leaf images; citation_inbook_title=2016 15th IEEE international conference on machine learning and applications (ICMLA); citation_publication_date=2016; citation_pages=158-163; citation_id=CR14; citation_author=E Mwebaze; citation_author=G Owomugisha; citation_publisher=IEEE">

    <meta name="citation_reference" content="Hughes D, Salathe M. An open access repository of images on plant health to enable the development of mobile disease diagnostics. arXiv preprint 
                    arXiv:1511.08060
                    
                  ; 2015.">

    <meta name="citation_reference" content="LabelImg Software. 
                    https://github.com/tzutalin/labelImg/
                    
                  . Accessed 1 Feb 2019.">

    <meta name="citation_reference" content="ImageNet Data Set. 
                    http://www.image-net.org/
                    
                  . Accessed 12 Mar 2019.">

    <meta name="citation_reference" content="He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 770–8.">

    <meta name="citation_reference" content="Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint 
                    arXiv:1502.03167
                    
                  . 2015.">

    <meta name="citation_reference" content="Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H. Mobilenets: efficient convolutional neural networks for mobile vision applications. arXiv preprint 
                    arXiv:1704.04861
                    
                  . 2017.">

    <meta name="citation_reference" content="Huang J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A, Fischer I, Wojna Z, Song Y, Guadarrama S. Speed/accuracy trade-offs for modern convolutional object detectors. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 7310–1.">

    <meta name="citation_reference" content="TensorFlow Python API. 
                    https://www.tensorflow.org/api_docs/python
                    
                  . Accessed 10 Feb 2019.">

    <meta name="citation_reference" content="COCO Data Set. 
                    http://cocodataset.org/
                    
                  . Accessed 15 Feb 2019.">

    <meta name="citation_reference" content="Reitermanova Z. Data splitting. In: WDS’10 proceedings of contributed papers, Part I, vol 10; 2010. p. 31–6.">

    <meta name="citation_reference" content="Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC. Ssd: Single shot multibox detector. In: European conference on computer vision. Springer; 2016. p. 21–37.">

    <meta name="citation_reference" content="Object Detection API Loss Functions Implementation, Tensorflow. 
                    https://github.com/tensorflow/models/blob/master/research/object_detection/core/losses.py
                    
                  . Accessed 5 Mar 2019.">

    <meta name="citation_reference" content="Confusion Matrix for Object Detection. 
                    https://github.com/svpino/tf_object_detectioncm/blob/master/confusion_matrix.py
                    
                  . Accessed 10 Mar 2019.">

    <meta name="citation_reference" content="Object Detection API, Tensorflow. 
                    https://github.com/tensorflow/models/tree/master/research/object_detection
                    
                  . Accessed 20 Feb 2019.">

    <meta name="citation_reference" content="Dandawate Y, Kokare R. An automated approach for classification of plant diseases towards development of futuristic decision support system in Indian perspective. In: 2015 international conference on advances in computing, communications and informatics (ICACCI), IEEE; 2015. p. 794–9.">

    <meta name="citation_reference" content="Mokhtar U, El Bendary N, Hassenian AE, Emary E, Mahmoud MA, Hefny H, Tolba MF. Svm-based detection of tomato leaves diseases. In: Intelligent Systems’ 2014. Springer; 2015. p. 641–52.">

    <meta name="citation_reference" content="citation_title=Deep Learning for Plant Diseases: Detection and Saliency Map Visualisation; citation_inbook_title=Human and Machine Learning; citation_publication_date=2018; citation_pages=93-117; citation_id=CR31; citation_author=Mohammed Brahimi; citation_author=Marko Arsenovic; citation_author=Sohaib Laraba; citation_author=Srdjan Sladojevic; citation_author=Kamel Boukhalfa; citation_author=Abdelouhab Moussaoui; citation_publisher=Springer International Publishing">

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=A survey on transfer learning; citation_author=SJ Pan, Q Yang; citation_volume=22; citation_issue=10; citation_publication_date=2010; citation_pages=1345-1359; citation_doi=10.1109/TKDE.2009.191; citation_id=CR32">

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=A robust deep-learning-based detector for real-time tomato plant diseases and pests recognition; citation_author=A Fuentes, S Yoon, S Kim, D Park; citation_volume=17; citation_issue=9; citation_publication_date=2017; citation_pages=2022; citation_doi=10.3390/s17092022; citation_id=CR33">

    <meta name="citation_reference" content="citation_journal_title=Agriculture; citation_title=Detection of key organs in tomato based on deep migration learning in a complex background; citation_author=J Sun, X He, X Ge, X Wu, J Shen, Y Song; citation_volume=8; citation_issue=12; citation_publication_date=2018; citation_pages=196; citation_doi=10.3390/agriculture8120196; citation_id=CR34">

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vision.; citation_title=The pascal visual object classes challenge: a retrospective; citation_author=M Everingham, SA Eslami, L Gool, CK Williams, J Winn, A Zisserman; citation_volume=111; citation_issue=1; citation_publication_date=2015; citation_pages=98-136; citation_doi=10.1007/s11263-014-0733-5; citation_id=CR35">

    <meta name="citation_reference" content="Zhang L, Lin L, Liang X, He K. Is faster r-cnn doing well for pedestrian detection? In: European conference on computer vision. Springer; 2016. p. 443–57.">

    <meta name="citation_reference" content="Cuellar W, Mwanzia L, Lourido D, Garcia C, Martínez A, Cruz P, Pino L, Tohme J. PestDisPlace: monitoring the distribution of pests and diseases, version 2.0. International Center for Tropical Agriculture (CIAT); 2018.">

    <meta name="citation_author" content="Michael Gomez Selvaraj">

    <meta name="citation_author_institution" content="International Center for Tropical Agriculture (CIAT), Cali, Colombia">

    <meta name="citation_author" content="Alejandro Vergara">

    <meta name="citation_author_institution" content="International Center for Tropical Agriculture (CIAT), Cali, Colombia">

    <meta name="citation_author" content="Henry Ruiz">

    <meta name="citation_author_institution" content="Department of Soil and Crop Sciences, Texas A&amp;M University, College Station, USA">

    <meta name="citation_author" content="Nancy Safari">

    <meta name="citation_author_institution" content="Bioversity International, Bukavu, Democratic Republic of Congo">

    <meta name="citation_author" content="Sivalingam Elayabalan">

    <meta name="citation_author_institution" content="Department of Biotechnology, Imayam Institute of Agriculture and Technology (IIAT), Affiliated to Tamil Nadu Agricultural University (TNAU), Tiruchirappalli, India">

    <meta name="citation_author" content="Walter Ocimati">

    <meta name="citation_author_institution" content="Bioversity International, Kampala, Uganda">

    <meta name="citation_author" content="Guy Blomme">

    <meta name="citation_author_institution" content="Bioversity International, Addis Ababa, Ethiopia">



        <meta name="format-detection" content="telephone=no">

        <link rel="shortcut icon" data-test="shortcut-icon" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/favicon-9ff1bf1161.ico">
<link rel="apple-touch-icon" sizes="114x114" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/app-icon-114x114-3b8894feb3.png">
<link rel="apple-touch-icon" sizes="144x144" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/app-icon-144x144-46be65d032.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/app-icon-180x180-d205ab10f5.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/favicon-16x16-5beafe3a97.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/favicon-32x32-103a91ebba.png">
<link rel="icon" type="image/png" sizes="96x96" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/favicon-96x96-6aff193bdb.png">
<link rel="icon" type="image/png" sizes="194x194" href="https://plantmethods.biomedcentral.com/static/images/favicons/bmc/favicon-194x194-501ed5e5ea.png">
<meta name="msapplication-TileColor" content="#e6e6e6">
<meta name="msapplication-TileImage" content="/static/images/favicons/bmc/app-icon-144x144-46be65d032.png">
<meta name="theme-color" content="#1b3051">


        <script src="osd.js"></script><script src="pubads_impl_modern_rendering_2019112101.js"></script><script type="text/javascript" src="get"></script><script type="text/javascript" src="optout_check"></script><script type="text/javascript" src="bd339c69-af54-4a21-b4f1-654bcfcd83ca_002"></script><script type="text/javascript" src="bd339c69-af54-4a21-b4f1-654bcfcd83ca"></script><script async="" src="controltag.js"></script><script type="text/javascript" async="" src="analytics.js"></script><script type="text/javascript" async="" src="KDqylSLE.js"></script><script type="text/javascript" async="" src="MathJax.js"></script><script async="" src="gtm.js"></script><script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>
        
    <link rel="stylesheet" media="screen" href="core-article-f837ea88d2.css">


<link rel="stylesheet" media="screen" href="core-d3fdd25a61.css">
<link rel="stylesheet" media="print" href="print-7680b1a80b.css">

<link rel="stylesheet" id="js-mustard" href="enhanced-69dc1e189b.css" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">


    <link rel="stylesheet" href="enhanced-article-8ee897c785.css" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">


        

        <script>
            window.airbrakeErrors = [];
            window.onerror = function(error) {window.airbrakeErrors.push(error);}
        </script>

        
    <script type="text/javascript">
        config = {
            env: 'live',
            site: 'plantmethods.biomedcentral.com',
            siteWithPath: 'plantmethods.biomedcentral.com' + window.location.pathname,
            twitterHashtag: '',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            doi: '10.1186/s13007-019-0475-z',
            figshareScriptUrl: 'https://widgets.figshare.com/static/figshare.js',
            hasFigshareInvoked: false,
            publisherBrand: 'BioMed Central',
            mustardcut: false
        };
    </script>

        <!-- SpringerLink's event tracker, as per: https://github.com/springernature/springerlink-event-tracker -->

<script type="text/javascript">
document.addEventListener('dataLayerCreated', function() {
    var script = document.createElement('script');
    script.id = 'springerlink-event-tracker';
    script.src = 'https://event-tracker.springernature.com/dist/eventTracker.js';
    script.onload = function () {
        var dL;
        var doi;
        var imprint;
        var bpids;
        var publisher;
        var pageType;
        var eventObject;

        for (var i = 0; i < window.dataLayer.length; i++) {
            if (window.dataLayer[i].event === "dataLayerCreated") {
                dL = window.dataLayer[i];
            }
        }

        if (!dL || !dL.content.article || !dL.content.article.doi) { return false;}

        doi = dL.content.article.doi;
        imprint = dL.content.contentInfo.imprint;
        bpids = dL.session.authentication.authenticationID;
        pageType = dL.page.category.pageType;

        if (pageType) {
            pageType = pageType.charAt(0).toUpperCase() + pageType.substring(1);
        }

        eventObject = {'content_type': pageType, 'doi': doi};

        if (imprint === 'BioMed Central') {
            publisher = 'BMC';
        }
        else if (imprint === 'SpringerOpen') {
            publisher = 'SpringerOpen';
        }
        else {
            throw new Error('Event Tracker Error: The publisher has an unexpected value');
        }

        if (bpids.length > 0) {
            eventObject['business_partner_ids'] = bpids;
        }

        new EventTracker({'platform': publisher}).sendEvent('display', eventObject);
    };
    document.head.appendChild(script);
});
</script>

        <!--Polyfills CustomEvent constructor in IE. Allows us to use events to manage race conditions in client side js-->
<script>
    (function () {
        if (typeof window.CustomEvent === "function") { return false; } // If IE polyfill not needed just return

        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: undefined };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script> 
        
    <script type="text/javascript">
        window.dataLayer = [{"content":{"article":{"doi":"10.1186/s13007-019-0475-z","articleType":"Research","peerReviewType":"Closed","supplement":null,"keywords":"Artificial intelligence;Banana;Deep learning;Disease detection;Transfer learning;Convolutional neural networks;Mobile app"},"contentInfo":{"imprint":"BioMed Central","title":"AI-powered banana diseases and pest detection","publishedAt":1565568000000,"publishedAtDate":"2019-08-12","author":["Michael Gomez Selvaraj","Alejandro Vergara","Henry Ruiz","Nancy Safari","Sivalingam Elayabalan","Walter Ocimati","Guy Blomme"],"collection":[]},"attributes":{"deliveryPlatform":"oscar","template":"rebrand","cms":null,"copyright":{"creativeCommonsType":"CC BY + CC0","openAccess":true},"environment":"live"},"journal":{"siteKey":"plantmethods.biomedcentral.com","volume":"15","issue":"1","title":"Plant Methods","type":"Life Sciences 2","journalID":13007,"gaCode":"UA-62072911-9","section":[]},"category":{"pmc":{"primarySubject":"Life Sciences"},"contentType":"Research","publishingSegment":"LS 33"}},"session":{"authentication":{"authenticationID":[]}},"version":"1.0.0","page":{"category":{"pageType":"article"},"attributes":{"featureFlags":[]},"name":null},"event":"dataLayerCreated"}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script><script id="springerlink-event-tracker" src="eventTracker.asc"></script>


        <script>
            
            var mustardcutlink = document.getElementById('js-mustard');
            if (mustardcutlink && window.matchMedia && window.matchMedia(mustardcutlink.media).matches) {
                window.config.mustardcut = true;
            }
        </script>
        <script>
            (function(d){
                var config = window.config;
                if (config && config.mustardcut && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                    d.className += ' webfonts-loaded';
                }
            })(document.documentElement);
        </script>

        
    
        <script src="jquery-220afd743d.js"></script>
    
    <script src="7d6f4b4f-77fb-43b9-b429-eb1e1eeac6aa.js" id="onetrust-bmc-control" charset="UTF-8"></script><link type="text/css" href="optanon.css" rel="stylesheet"><style>#optanon ul#optanon-menu li { background-color: #F8F8F8 !important }#optanon ul#optanon-menu li.menu-item-selected { background-color: #C5F6F9 !important }#optanon #optanon-popup-wrapper .optanon-white-button-middle { background-color: #1B3051 !important }.optanon-alert-box-wrapper .optanon-alert-box-button-middle { background-color: #1B3051 !important; border-color: #1B3051 !important; }#optanon #optanon-popup-wrapper .optanon-white-button-middle button { color: #ffffff !important }.optanon-alert-box-wrapper .optanon-alert-box-button-middle button { color: #ffffff !important }#optanon #optanon-popup-bottom { background-color: #FFFFFF !important }#optanon.modern #optanon-popup-top, #optanon.modern #optanon-popup-body-left-shading { background-color: #FFFFFF !important }.optanon-alert-box-wrapper { background-color:#EFF6FC !important }.optanon-alert-box-wrapper .optanon-alert-box-bg p { color:#333333 !important }</style><script type="text/javascript" src="jquery-1.js" integrity="sha256-Ls0pXSlb7AYs7evhd+VLnWsZ/AqEHcXBeMZUycz/CcA=" crossorigin="anonymous"></script>

        
    
        
    

        
    
        
            <!-- Google Tag Manager -->
            <script>
                (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-TDGJHK');
            </script>
            <!-- End Google Tag Manager -->
        
    


        
    
    <link rel="canonical" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z">
    

        
        
        <meta property="og:url" content="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z">
        <meta property="og:type" content="article">
        <meta property="og:site_name" content="Plant Methods">
        <meta property="og:title" content="AI-powered banana diseases and pest detection">
        <meta property="og:description" content="Banana (Musa spp.) is the most popular marketable fruit crop grown all over the world, and a dominant staple food in many developing countries. Worldwide, banana production is affected by numerous diseases and pests. Novel and rapid methods for the timely detection of pests and diseases will allow to surveil and develop control measures with greater efficiency. As deep convolutional neural networks (DCNN) and transfer learning has been successfully applied in various fields, it has freshly moved in the domain of just-in-time crop disease detection. The aim of this research is to develop an AI-based banana disease and pest detection system using a DCNN to support banana farmers. Large datasets of expert pre-screened banana disease and pest symptom/damage images were collected from various hotspots in Africa and Southern India. To build a detection model, we retrained three different convolutional neural network (CNN) architectures using a transfer learning approach. A total of six different models were developed from 18 different classes (disease by plant parts) using images collected from different parts of the banana plant. Our studies revealed ResNet50 and InceptionV2 based models performed better compared to MobileNetV1. These architectures represent the state-of-the-art results of banana diseases and pest detection with an accuracy of more than 90% in most of the models tested. These experimental results were comparable with other state-of-the-art models found in the literature. With a future view to run these detection capabilities on a mobile device, we evaluated the performance of SSD (single shot detector) MobileNetV1. Performance and validation metrics were also computed to measure the accuracy of different models in automated disease detection methods. Our results showed that the DCNN was a robust and easily deployable strategy for digital banana disease and pest detection. Using a pre-trained disease recognition model, we were able to perform deep transfer learning (DTL) to produce a network that can make accurate predictions. This significant high success rate makes the model a useful early disease and pest detection tool, and this research could be further extended to develop a fully automated mobile app to help millions of banana farmers in developing countries.">
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/13007.jpg">
    
        
    <script async="" src="core-bmc.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><link rel="preload" href="integrator.js" as="script"><script type="text/javascript" src="integrator.js"></script><script src="pubads_impl_modern_2019112101.js" async=""></script><style type="text/css">.MathJax_Display {text-align: center; margin: 0; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Blank; src: url('about:blank')}
#MathJax_Message {margin: 0}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css" media="print">.usabilla_live_button_container { display: none; }</style><link rel="prefetch" href="https://tpc.googlesyndication.com/safeframe/1-0-36/html/container.html"></head>

    <body class="journal journal-fulltext"><div class="optanon-alert-box-wrapper  " role="alertdialog" aria-labelledby="alert-box-title" aria-describedby="alert-box-message" style="bottom: 0px;"><div class="optanon-alert-box-bottom-top"></div><div class="optanon-alert-box-bg"><div class="optanon-alert-box-logo"> </div><div class="optanon-alert-box-body"><p class="optanon-alert-box-title legacy-banner-title sr-only" id="alert-box-title" role="heading" aria-level="2">Cookie Notice</p><p class="banner-content" id="alert-box-message">We
 use cookies to ensure the functionality of our website, to personalize 
content and advertising, to provide social media features, and to 
analyze our traffic. If you allow us to do so, we also inform our social
 media, advertising and analysis partners about your use of our website.
 You can decide for yourself which categories you want to deny or allow.
 Please note that based on your settings not all functionalities of the 
site are available. Further information can be found in our <a href="https://www.biomedcentral.com/privacy-statement" class="banner-policy-link" tabindex="1" aria-label="privacy policy">privacy policy</a>.
</p></div><div class="optanon-clearfix"></div><div class="optanon-alert-box-button-container"><div class="optanon-alert-box-button optanon-button-close"><div class="optanon-alert-box-button-middle"><button class="optanon-alert-box-close" aria-label="Close">Close</button></div></div><div class="optanon-alert-box-button optanon-button-allow"><div class="optanon-alert-box-button-middle accept-cookie-container"><button class="optanon-allow-all accept-cookies-button" title="Accept All Cookies" aria-label="Accept All Cookies" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Banner Accept Cookies');" tabindex="3">Accept All Cookies</button></div></div><div class="optanon-alert-box-button optanon-button-more"><div class="optanon-alert-box-button-middle"><button class="optanon-toggle-display cookie-settings-button" title="Manage Settings" aria-label="Manage Settings" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Banner Open Preferences');" tabindex="2">Manage Settings</button></div></div></div><div class="optanon-clearfix optanon-alert-box-bottom-padding"></div></div></div><div id="optanon" class="modern"><div id="optanon-popup-bg"></div><div id="optanon-popup-wrapper" role="dialog" aria-modal="true" tabindex="-1" lang="en-GB"><div id="optanon-popup-top"></div><div id="optanon-popup-body"><div id="optanon-popup-body-left"><div id="optanon-popup-body-left-shading"></div><div id="optanon-branding-top-logo" style="background-image: url('https://cdn.cookielaw.org/logos/5138/5138:biomedcentral.com/bmc-logo.png') !important;"></div><ul id="optanon-menu" aria-label="Navigation Menu" role="tablist"><li class="menu-item-on menu-item-about" title="Your Privacy"><p class="preference-menu-item"><button role="tab" aria-selected="true" aria-controls="Your-Privacy" id="Your-Privacy">Your Privacy</button></p></li><li class="menu-item-necessary menu-item-on" title="Strictly Necessary Cookies"><p class="preference-menu-item"><button role="tab" aria-selected="false" aria-controls="Strictly-Necessary-Cookies" id="Strictly-Necessary">Strictly Necessary Cookies</button></p></li><li class="menu-item-on menu-item-performance" title="Performance &amp; Analytics Cookies"><p class="preference-menu-item"><button role="tab" aria-selected="false" aria-controls="Performance-&amp;-Analytics-Cookies" id="Performance-&amp;">Performance &amp; Analytics Cookies</button></p></li><li class="menu-item-on menu-item-functional" title="Functional Cookies"><p class="preference-menu-item"><button role="tab" aria-selected="false" aria-controls="Functional-Cookies" id="Functional-Cookies">Functional Cookies</button></p></li><li class="menu-item-on menu-item-advertising" title="Targeting Cookies (1st Party)"><p class="preference-menu-item"><button role="tab" aria-selected="false" aria-controls="Targeting-Cookies-(1st-Party)" id="Targeting-Cookies">Targeting Cookies (1st Party)</button></p></li><li class="menu-item-necessary menu-item-off" title="Targeting Cookies (3rd Party)"><p class="preference-menu-item"><button role="tab" aria-selected="false" aria-controls="Targeting-Cookies-(3rd-Party)" id="Targeting-Cookies">Targeting Cookies (3rd Party)</button></p></li><li class="menu-item-moreinfo menu-item-off" title="More Information"><p class="preference-menu-item"><a target="_blank" aria-label="More Information" href="https://www.biomedcentral.com/cookies" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Preferences Cookie Policy');">More Information</a></p></li></ul></div><div><div id="optanon-popup-body-right"><p role="heading" aria-level="2" class="legacy-preference-banner-title h2" aria-label="Privacy Preference Centre">Privacy Preference Centre</p><div class="vendor-header-container"><p class="header-3" role="heading" aria-level="3"></p><div id="optanon-popup-more-info-bar"><div class="optanon-status"><div class="optanon-status-editable"><form><span class="fieldset"><p><input value="check" id="chkMain" checked="checked" class="legacy-group-status optanon-status-checkbox" type="checkbox"><label for="chkMain">Active</label></p></span></form></div><div class="optanon-status-always-active optanon-status-on"><p>Always Active</p></div></div></div></div><div class="optanon-main-info-text" role="tabpanel"></div></div><div class="optanon-bottom-spacer"></div></div></div><div id="optanon-popup-bottom"> <a href="https://onetrust.com/poweredbyonetrust" target="_blank" rel="noopener"><div id="optanon-popup-bottom-logo" alt="OneTrust Website" style="background: url(https://cdn.cookielaw.org/skins/5.8.0/default_flat_bottom_two_button_white/v2/images/cookie-collective-top-bottom.png);width:155px;height:35px;" title="powered by OneTrust"></div></a><div class="optanon-button-wrapper optanon-save-settings-button optanon-close optanon-close-consent"><div class="optanon-white-button-left"></div><div class="optanon-white-button-middle"><button title="Save Settings" aria-label="Save Settings" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Preferences Save Settings');">Save Settings</button></div><div class="optanon-white-button-right"></div></div><div class="optanon-button-wrapper optanon-allow-all-button optanon-allow-all" style="display: block;"><div class="optanon-white-button-left"></div><div class="optanon-white-button-middle"><button title="Allow All Cookies" aria-label="Allow All Cookies" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Preferences Allow All');">Allow All Cookies</button></div><div class="optanon-white-button-right"></div></div></div></div></div><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div><div id="lightningjs-usabilla_live" style="display: none;"><div><iframe id="lightningjs-frame-usabilla_live" frameborder="0"></iframe></div></div>
        
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript>
                <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TDGJHK" style="display:none;visibility:hidden" width="0" height="0"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    

        <div class="u-visually-hidden" aria-hidden="true">
    
    <!--?xml version="1.0" encoding="UTF-8"?--><svg xmlns="http://www.w3.org/2000/svg"><symbol id="icon-alert" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M4 10h2.5a.5.5 0 0 1 0 1H3.414l-1.121 1.121a1 1 0 0 0-.293.707V13h14v-.172a1 1 0 0 0-.293-.707L14 10.414V7A5 5 0 0 0 4 7zm3 4a2 2 0 1 0 4 0zm-5 0a1 1 0 0 1-1-1v-.172a2 2 0 0 1 .586-1.414L3 10V7a6 6 0 1 1 12 0v3l1.414 1.414A2 2 0 0 1 17 12.828V13a1 1 0 0 1-1 1h-4a3 3 0 0 1-6 0z"></path></symbol><symbol id="icon-arrow-left-bullet" viewBox="0 0 8 16"><path d="M3 8l5 5v3L0 8l8-8v3L3 8z"></path></symbol><symbol id="icon-arrow-left"><path d="M7.002 15.002a1 1 0 1 0 2 0V3.386l2.482 2.482a.994.994 0 0 0 1.403.02 1.001 1.001 0 0 0 .001-1.416l-.001-.001L8.71.295a1 1 0 0 0-1.415 0h-.001L3.118 4.472a.99.99 0 0 0-.016 1.4 1 1 0 0 0 1.414.003l.006-.006 2.48-2.482v11.615z"></path></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M4 13V2h1v11h11V2H3a1 1 0 0 0-1 1v10.268A1.99 1.99 0 0 1 3 13zm12 1H3a1 1 0 0 0 0 2h13zm0 3H3a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h13a1 1 0 0 1 1 1v14a1 1 0 0 1-1 1zM7.5 4h6a.5.5 0 1 1 0 1h-6a.5.5 0 0 1 0-1zm1 2h4a.5.5 0 1 1 0 1h-4a.5.5 0 0 1 0-1z"></path></symbol><symbol id="icon-chevron-down"><path d="M8 8.586l3.293-3.293a1 1 0 0 1 1.414 1.414l-4 4a1 1 0 0 1-1.414 0l-4-4a1 1 0 0 1 1.414-1.414z" fill-rule="evenodd"></path></symbol><symbol id="icon-chevron-right"><path d="M7.782 7L5.3 4.518a.994.994 0 0 1-.02-1.403 1.001 1.001 0 0 1 1.417 0l4.176 4.177a1.001 1.001 0 0 1 0 1.416l-4.176 4.177a.991.991 0 0 1-1.4.016 1 1 0 0 1 .003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"></path></symbol><symbol id="icon-chevron-up"><path d="M8 7.414l3.293 3.293a1 1 0 0 0 1.414-1.414l-4-4a1 1 0 0 0-1.414 0l-4 4a1 1 0 0 0 1.414 1.414z" fill-rule="evenodd"></path></symbol><symbol id="icon-download-rounded"><path d="M0 13c0-.556.449-1 1.002-1h9.996a.999.999 0 1 1 0 2H1.002A1.006 1.006 0 0 1 0 13zM7 1v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 0 1 0 1.417l-4.177 4.177a1.001 1.001 0 0 1-1.416 0L1.115 6.715a.991.991 0 0 1-.016-1.4 1 1 0 0 1 1.42.003L5 7.8V1c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z"></path></symbol><symbol id="icon-download" viewBox="-301 390 9 14"><path d="M-301 395.6l4.5 5.1 4.5-5.1h-3V390h-3v5.6h-3zm0 6.5h9v1.9h-9z"></path></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M8.726 2.546A3 3 0 1 0 5.37 7.519l.63.409v1.024l-.79.329A5.221 5.221 0 0 0 2 14.099V15H1v-.901a6.221 6.221 0 0 1 3.825-5.741 4 4 0 1 1 4.976-6.213 4.965 4.965 0 0 0-1.075.4zM6 17H5v-.901a6.221 6.221 0 0 1 3.825-5.741 4 4 0 1 1 4.349 0A6.221 6.221 0 0 1 17 16.099V17h-1v-.901a5.221 5.221 0 0 0-3.21-4.818l-.79-.33V9.929l.63-.409a3 3 0 1 0-3.26 0l.63.409v1.024l-.79.329A5.221 5.221 0 0 0 6 16.099z"></path></symbol><symbol id="icon-ethics"><path d="M12.182 2.272l1.038-1.038a4.014 4.014 0 0 1 5.677 0l1.038 1.038a3.21 3.21 0 0 0 2.271.941h2.684a4.014 4.014 0 0 1 4.014 4.014v2.684c0 .852.339 1.669.94 2.271l1.039 1.038a4.014 4.014 0 0 1 0 5.677l-1.038 1.038a3.211 3.211 0 0 0-.94 2.271v2.684a4.014 4.014 0 0 1-4.015 4.014h-2.684c-.852 0-1.669.339-2.27.94l-1.039 1.039a4.014 4.014 0 0 1-5.677 0l-1.038-1.038a3.211 3.211 0 0 0-2.27-.94H7.226a4.014 4.014 0 0 1-4.014-4.015v-2.684c0-.852-.338-1.669-.94-2.27l-1.039-1.039a4.014 4.014 0 0 1 0-5.677l1.038-1.038a3.208 3.208 0 0 0 .941-2.27V7.226a4.014 4.014 0 0 1 4.014-4.014H9.91c.852 0 1.669-.338 2.271-.94zm1.136 1.136a4.817 4.817 0 0 1-3.407 1.41H7.227a2.409 2.409 0 0 0-2.408 2.41V9.91a4.817 4.817 0 0 1-1.411 3.407L2.37 14.356a2.41 2.41 0 0 0 0 3.406L3.408 18.8a4.817 4.817 0 0 1 1.41 3.406v2.684a2.409 2.409 0 0 0 2.41 2.409H9.91a4.82 4.82 0 0 1 3.407 1.41l1.038 1.038c.94.941 2.465.941 3.406 0L18.8 28.71a4.817 4.817 0 0 1 3.406-1.41h2.684a2.409 2.409 0 0 0 2.409-2.409v-2.684c0-1.278.507-2.503 1.41-3.406l1.038-1.038a2.408 2.408 0 0 0 0-3.406l-1.038-1.038A4.817 4.817 0 0 1 27.3 9.91V7.227a2.409 2.409 0 0 0-2.41-2.407h-2.684A4.817 4.817 0 0 1 18.8 3.408L17.762 2.37a2.409 2.409 0 0 0-3.406 0l-1.038 1.038zM15.03 17.86l4.332-4.73a.803.803 0 1 1 1.224 1.04l-4.844 5.367a.803.803 0 0 1-1.112.076l-3.1-2.605a.803.803 0 0 1 1.027-1.233l2.473 2.085zm1.028 7.832c-.41 0-.41-1.482 0-1.482a8.152 8.152 0 0 0 6.657-12.858.741.741 0 1 1 1.21-.857 9.634 9.634 0 0 1-7.867 15.197zM21.68 8.234a.741.741 0 0 1-.866 1.203 8.152 8.152 0 0 0-12.908 6.578.741.741 0 0 1-1.483-.007A9.635 9.635 0 0 1 21.68 8.234zM6.907 19.077a.741.741 0 0 1 1.407-.464 8.155 8.155 0 0 0 7.745 5.598.741.741 0 1 1 0 1.482 9.637 9.637 0 0 1-9.152-6.616z"></path></symbol><symbol id="icon-explore"><path d="M15 28.3c7.3 0 13.3-6 13.3-13.3S22.3 1.7 15 1.7 1.7 7.7 1.7 15s6 13.3 13.3 13.3zm0 1.7C6.7 30 0 23.3 0 15S6.7 0 15 0s15 6.7 15 15-6.7 15-15 15zm0-4.2c-.5 0-.8-.3-.8-.8s.3-.8.8-.8c5 0 9-4 9.2-8.8 0-.5.3-.8.8-.8s.8.3.8.8c-.1 5.8-5 10.4-10.8 10.4zm-.5-21.6c.5 0 .8.3.8.8s-.3.8-.6.8c-5 .2-8.9 4.4-8.9 9.2 0 .5-.3.8-.8.8s-.8-.3-.8-.8c0-5.8 4.5-10.5 10.3-10.8zm1.8 13.5l-2-2c-.3-.3-.3-.8 0-1.2.3-.3.8-.3 1.2 0l2 2 2.7-6.7-7.5 2.8-3 7.5 6.6-2.4zm6.9-10.9L18.7 18c-.2.5-.5.8-1 1L6.5 23.5 11 12.3c.3-.7.7-1 1.2-1.2l11-4.3z"></path></symbol><symbol id="icon-ext-link" viewBox="0 0 16 16"><path fill="#676767" d="M12.9 16H3.1C1.4 16 0 14.6 0 12.9V3.2C0 1.4 1.4 0 3.1 0h3.7v1H3.1C2 1 1 2 1 3.2v9.7C1 14 2 15 3.1 15h9.7c1.2 0 2.1-1 2.1-2.1V8.7h1v4.2c.1 1.7-1.3 3.1-3 3.1z"></path><path fill="#676767" d="M12.8 2.5l.7.7-9 8.9-.7-.7 9-8.9z"></path><path fill="#676767" d="M9.7 0L16 6.2V0z"></path></symbol><symbol id="icon-info-bordered" viewBox="470.812 270.868 18 18"><path d="M479.812 270.868c-4.972 0-9 4.029-9 9s4.028 9 9 9c4.971 0 9-4.029 9-9s-4.029-9-9-9zm0 16.875a7.875 7.875 0 0 1-7.875-7.875 7.875 7.875 0 1 1 15.75 0 7.875 7.875 0 0 1-7.875 7.875z"></path><path d="M479.284 279.586c.089-.238-.024-.361-.13-.361-.489 0-1.123 1.16-1.359 1.16-.093 0-.173-.092-.173-.174 0-.238.581-.801.751-.971.526-.506 1.217-.895 1.979-.895.567 0 1.174.346.703 1.639l-.952 2.604c-.079.199-.224.531-.224.746 0 .092.054.186.159.186.395 0 1.121-1.135 1.304-1.135.067 0 .158.086.158.199 0 .385-1.542 2.043-2.874 2.043-.477 0-.804-.225-.804-.732 0-.641.447-1.734.538-1.963l.924-2.346zm.727-3.41c0-.586.498-1.066 1.082-1.066.527 0 .91.357.91.906 0 .615-.501 1.068-1.098 1.068-.541-.002-.894-.361-.894-.908z"></path></symbol><symbol id="icon-opr" viewBox="0 0 18 18"><path d="M.9 2.5c-.2-.2-.4-.3-.6-.2-.3.2-.3.4-.3.6 0 .8 0 1.7.1 2.5 0 .5.3.7.7.6.9 0 1.7-.2 2.5-.3h.2c.5-.2.6-.5.3-.8-.2-.2-.4-.4-.6-.5-.1-.1-.2-.2-.4-.3.5-.6 1-1.1 1.7-1.5 4.6-3 10.9-.5 12 4.9.9 4.4-2.2 8.6-6.7 9.2-3.7.5-7.2-1.6-8.4-5.1 0-.1-.1-.2-.1-.3-.1-.3-.5-.5-.8-.4-.3.1-.5.4-.4.8.4 1.3 1.1 2.4 2 3.4 1.4 1.5 3.1 2.4 5.1 2.8 3 .5 5.6-.2 7.8-2.2 2.1-1.9 3-4.2 3-6.9-.1-4-3.1-7.6-7.1-8.4-3.3-.9-6.2 0-8.6 2.3-.2.1-.3.3-.5.5-.3-.2-.6-.5-.9-.7z"></path><path d="M13 4.7c-2.6 1.6-4.5 3.6-5.3 4.6L5.6 7.7l-.9.7L8.3 12c.6-1.6 2.6-4.6 4.9-6.8l-.2-.5z"></path></symbol><symbol id="icon-remove" viewBox="-296 388 18 18"><path d="M-291.7 396.1h9v2h-9z"></path><path d="M-287 405.5c-4.7 0-8.5-3.8-8.5-8.5s3.8-8.5 8.5-8.5 8.5 3.8 8.5 8.5-3.8 8.5-8.5 8.5zm0-16c-4.1 0-7.5 3.4-7.5 7.5s3.4 7.5 7.5 7.5 7.5-3.4 7.5-7.5-3.4-7.5-7.5-7.5z"></path></symbol><symbol id="icon-rss"><ellipse cx="3.305" cy="20.702" rx="3.306" ry="3.298"></ellipse><path d="M15.978 24h-4.684c0-6.224-5.057-11.27-11.294-11.27V8.058c8.824 0 15.978 7.137 15.978 15.942z"></path><path d="M19.2 23.95C19.2 13.366 10.604 4.79 0 4.79V0c13.255 0 24 10.723 24 23.95h-4.8z"></path></symbol><symbol id="icon-search"><path d="M6.6 10.6c2.2 0 4-1.7 4-4s-1.7-4-4-4-4 1.7-4 4 1.8 4 4 4zm4 1.4l1.5-1.5 3.7 3.6c.4.5.4 1.2 0 1.6-.4.4-1.1.4-1.6 0L10.6 12zm-4 1.2C2.9 13.2 0 10.3 0 6.6S2.9 0 6.6 0s6.6 2.9 6.6 6.6-2.9 6.6-6.6 6.6z"></path></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 0 0 0-2H3.385l2.482-2.482a.994.994 0 0 0 .02-1.403 1.001 1.001 0 0 0-1.417 0L.294 5.292a1.001 1.001 0 0 0 0 1.416l4.176 4.177a.991.991 0 0 0 1.4.016 1 1 0 0 0-.003-1.42L3.385 7H15z"></path></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 0 1 0-2h11.615l-2.482-2.482a.994.994 0 0 1-.02-1.403 1.001 1.001 0 0 1 1.417 0l4.176 4.177a1.001 1.001 0 0 1 0 1.416l-4.176 4.177a.991.991 0 0 1-1.4.016 1 1 0 0 1 .003-1.42L12.615 7H1z"></path></symbol><symbol id="icon-updates" viewBox="0 0 18 18"><path d="M16.98 3.484h-.48c-2.52-.058-5.04 1.161-7.44 2.903-2.46-1.8-4.74-2.903-8.04-2.903-.3 0-.54.29-.54.58v9.813c0 .29.24.523.54.581 2.76.348 4.86 1.045 7.62 2.903.24.116.54.116.72 0 2.76-1.858 4.86-2.555 7.62-2.903.3-.058.54-.29.54-.58V4.064c0-.29-.24-.523-.54-.581zm-15.3 1.22c2.34 0 4.86 1.509 6.72 2.786v8.478c-2.34-1.394-4.38-2.09-6.72-2.439V4.703zm14.58 8.767c-2.34.348-4.38 1.045-6.72 2.439V7.374C12 5.632 14.1 4.645 16.26 4.645v8.826z"></path><path d="M9 .058c-1.56 0-2.76 1.22-2.76 2.671C6.24 4.181 7.5 5.4 9 5.4c1.5 0 2.76-1.22 2.76-2.671 0-1.452-1.2-2.67-2.76-2.67zm0 4.413c-.96 0-1.8-.755-1.8-1.742C7.2 1.8 7.98.987 9 .987s1.8.755 1.8 1.742c0 .93-.84 1.742-1.8 1.742z"></path></symbol><symbol id="icon-error" viewBox="2.002 0 14 14"><path d="M16.002 11.949L13.951 14 9.002 9.051 4.053 14l-2.051-2.051L6.951 7 2.002 2.05 4.053 0l4.949 4.95L13.951 0l2.051 2.049L11.053 7l4.949 4.949z"></path></symbol><symbol id="icon-info" viewBox="5.502 0 7 14"><path d="M8.165 5.923c.146-.384-.04-.577-.209-.577-.783 0-1.798 1.856-2.177 1.856-.15 0-.277-.147-.277-.279 0-.383.933-1.282 1.203-1.557.846-.811 1.949-1.434 3.174-1.434.908 0 1.879.555 1.125 2.63l-1.526 4.167c-.126.32-.358.854-.358 1.198 0 .148.088.298.255.298.632 0 1.796-1.818 2.091-1.818.105 0 .252.134.252.321C11.717 11.345 9.245 14 7.11 14c-.763 0-1.289-.361-1.289-1.176 0-1.027.717-2.778.865-3.145l1.479-3.756zm1.144-4.211C9.309.771 10.111 0 11.045 0c.845 0 1.457.577 1.457 1.456 0 .983-.804 1.709-1.757 1.709-.866-.001-1.436-.578-1.436-1.453z"></path></symbol><symbol id="icon-success"><path d="M15.592 0c-4.867 2.984-8.4 6.751-9.987 8.641L1.717 5.595 0 6.979l6.717 6.832c1.157-2.961 4.817-8.748 9.287-12.86L15.592 0z"></path></symbol><symbol id="icon-warning" viewBox="7.002 0 2.789 14"><path d="M7.002 11.211h2.789V14H7.002v-2.789zM7.002 0v3.436l.741 5.326h1.326l.723-5.326V0h-2.79z"></path></symbol><symbol id="icon-facebook-bordered" viewBox="463.812 263.868 32 32"><path d="M479.812 263.868c-8.837 0-16 7.163-16 16s7.163 16 16 16 16-7.163 16-16-7.163-16-16-16zm0 30c-7.732 0-14-6.269-14-14s6.268-14 14-14 14 6.269 14 14-6.267 14-14 14z"></path><path d="M483.025 280.48l.32-2.477h-2.453v-1.582c0-.715.199-1.207 1.227-1.207h1.311v-2.213a17.753 17.753 0 0 0-1.907-.098c-1.894 0-3.186 1.154-3.186 3.271V278h-2.142v2.477h2.142v6.354h2.557v-6.354l2.131.003z"></path></symbol><symbol id="icon-twitter-bordered" viewBox="463.812 263.868 32 32"><path d="M486.416 276.191a5.622 5.622 0 0 1-1.554.429 2.718 2.718 0 0 0 1.19-1.502 5.456 5.456 0 0 1-1.72.657 2.71 2.71 0 0 0-1.979-.854 2.711 2.711 0 0 0-2.642 3.326 7.681 7.681 0 0 1-5.586-2.831 2.714 2.714 0 0 0 .839 3.618 2.748 2.748 0 0 1-1.227-.339v.031a2.71 2.71 0 0 0 2.174 2.656 2.735 2.735 0 0 1-1.229.049 2.726 2.726 0 0 0 2.531 1.883 5.442 5.442 0 0 1-4.01 1.123 7.672 7.672 0 0 0 4.155 1.215c4.983 0 7.71-4.129 7.71-7.711 0-.115-.004-.232-.006-.351a5.41 5.41 0 0 0 1.354-1.399z"></path><path d="M479.812 263.868c-8.837 0-16 7.163-16 16s7.163 16 16 16 16-7.163 16-16-7.163-16-16-16zm0 30c-7.732 0-14-6.269-14-14s6.268-14 14-14 14 6.269 14 14-6.267 14-14 14z"></path></symbol><symbol id="icon-weibo-bordered" viewBox="463.812 263.868 32 32"><path d="M479.812 263.868c-8.838 0-16 7.163-16 16s7.162 16 16 16c8.837 0 16-7.163 16-16s-7.163-16-16-16zm0 30c-7.732 0-14-6.269-14-14s6.268-14 14-14c7.731 0 14 6.269 14 14s-6.267 14-14 14z"></path><path d="M478.552 285.348c-2.616.261-4.876-.926-5.044-2.649-.167-1.722 1.814-3.33 4.433-3.588 2.609-.263 4.871.926 5.041 2.647.165 1.721-1.818 3.331-4.43 3.59m5.23-5.718c-.226-.065-.374-.109-.259-.403.25-.639.276-1.188.005-1.581-.515-.734-1.915-.693-3.521-.021 0 0-.508.224-.378-.181.247-.798.209-1.468-.178-1.852-.87-.878-3.194.032-5.183 2.027-1.489 1.494-2.357 3.082-2.357 4.453 0 2.619 3.354 4.213 6.631 4.213 4.297 0 7.154-2.504 7.154-4.493.001-1.198-1.007-1.881-1.914-2.162m2.855-4.797a4.176 4.176 0 0 0-3.982-1.291.608.608 0 0 0-.465.72.604.604 0 0 0 .72.466 2.968 2.968 0 0 1 2.827.92 3 3 0 0 1 .625 2.918.602.602 0 0 0 .39.762.603.603 0 0 0 .763-.391v-.001a4.218 4.218 0 0 0-.878-4.103"></path><path d="M485.041 276.276a2.037 2.037 0 0 0-1.938-.63.518.518 0 0 0-.396.621.517.517 0 0 0 .617.398c.336-.071.702.03.947.307s.312.649.207.979a.52.52 0 0 0 .336.654.523.523 0 0 0 .657-.336 2.038 2.038 0 0 0-.43-1.993m-6.347 5.951c-.09.156-.293.233-.451.166-.151-.062-.204-.235-.115-.389.093-.155.284-.229.44-.168.157.056.214.235.126.391m-.832 1.074c-.253.405-.795.58-1.202.396-.403-.186-.521-.655-.27-1.051.248-.39.771-.566 1.176-.393.413.17.543.636.296 1.048m.95-2.864c-1.244-.326-2.65.294-3.19 1.396-.553 1.119-.021 2.369 1.236 2.775 1.303.42 2.84-.225 3.374-1.436.526-1.183-.132-2.402-1.42-2.735"></path></symbol></svg>
</div>


        

        

    <div class="u-vh-full">
        <div class="u-visually-hidden" aria-hidden="true">
    
    <!--?xml version="1.0" encoding="UTF-8"?--><svg xmlns="http://www.w3.org/2000/svg"><symbol id="icon-alert" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M4 10h2.5a.5.5 0 0 1 0 1H3.414l-1.121 1.121a1 1 0 0 0-.293.707V13h14v-.172a1 1 0 0 0-.293-.707L14 10.414V7A5 5 0 0 0 4 7zm3 4a2 2 0 1 0 4 0zm-5 0a1 1 0 0 1-1-1v-.172a2 2 0 0 1 .586-1.414L3 10V7a6 6 0 1 1 12 0v3l1.414 1.414A2 2 0 0 1 17 12.828V13a1 1 0 0 1-1 1h-4a3 3 0 0 1-6 0z"></path></symbol><symbol id="icon-arrow-left-bullet" viewBox="0 0 8 16"><path d="M3 8l5 5v3L0 8l8-8v3L3 8z"></path></symbol><symbol id="icon-arrow-left"><path d="M7.002 15.002a1 1 0 1 0 2 0V3.386l2.482 2.482a.994.994 0 0 0 1.403.02 1.001 1.001 0 0 0 .001-1.416l-.001-.001L8.71.295a1 1 0 0 0-1.415 0h-.001L3.118 4.472a.99.99 0 0 0-.016 1.4 1 1 0 0 0 1.414.003l.006-.006 2.48-2.482v11.615z"></path></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M4 13V2h1v11h11V2H3a1 1 0 0 0-1 1v10.268A1.99 1.99 0 0 1 3 13zm12 1H3a1 1 0 0 0 0 2h13zm0 3H3a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h13a1 1 0 0 1 1 1v14a1 1 0 0 1-1 1zM7.5 4h6a.5.5 0 1 1 0 1h-6a.5.5 0 0 1 0-1zm1 2h4a.5.5 0 1 1 0 1h-4a.5.5 0 0 1 0-1z"></path></symbol><symbol id="icon-chevron-down"><path d="M8 8.586l3.293-3.293a1 1 0 0 1 1.414 1.414l-4 4a1 1 0 0 1-1.414 0l-4-4a1 1 0 0 1 1.414-1.414z" fill-rule="evenodd"></path></symbol><symbol id="icon-chevron-right"><path d="M7.782 7L5.3 4.518a.994.994 0 0 1-.02-1.403 1.001 1.001 0 0 1 1.417 0l4.176 4.177a1.001 1.001 0 0 1 0 1.416l-4.176 4.177a.991.991 0 0 1-1.4.016 1 1 0 0 1 .003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"></path></symbol><symbol id="icon-chevron-up"><path d="M8 7.414l3.293 3.293a1 1 0 0 0 1.414-1.414l-4-4a1 1 0 0 0-1.414 0l-4 4a1 1 0 0 0 1.414 1.414z" fill-rule="evenodd"></path></symbol><symbol id="icon-download-rounded"><path d="M0 13c0-.556.449-1 1.002-1h9.996a.999.999 0 1 1 0 2H1.002A1.006 1.006 0 0 1 0 13zM7 1v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 0 1 0 1.417l-4.177 4.177a1.001 1.001 0 0 1-1.416 0L1.115 6.715a.991.991 0 0 1-.016-1.4 1 1 0 0 1 1.42.003L5 7.8V1c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z"></path></symbol><symbol id="icon-download" viewBox="-301 390 9 14"><path d="M-301 395.6l4.5 5.1 4.5-5.1h-3V390h-3v5.6h-3zm0 6.5h9v1.9h-9z"></path></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M8.726 2.546A3 3 0 1 0 5.37 7.519l.63.409v1.024l-.79.329A5.221 5.221 0 0 0 2 14.099V15H1v-.901a6.221 6.221 0 0 1 3.825-5.741 4 4 0 1 1 4.976-6.213 4.965 4.965 0 0 0-1.075.4zM6 17H5v-.901a6.221 6.221 0 0 1 3.825-5.741 4 4 0 1 1 4.349 0A6.221 6.221 0 0 1 17 16.099V17h-1v-.901a5.221 5.221 0 0 0-3.21-4.818l-.79-.33V9.929l.63-.409a3 3 0 1 0-3.26 0l.63.409v1.024l-.79.329A5.221 5.221 0 0 0 6 16.099z"></path></symbol><symbol id="icon-ethics"><path d="M12.182 2.272l1.038-1.038a4.014 4.014 0 0 1 5.677 0l1.038 1.038a3.21 3.21 0 0 0 2.271.941h2.684a4.014 4.014 0 0 1 4.014 4.014v2.684c0 .852.339 1.669.94 2.271l1.039 1.038a4.014 4.014 0 0 1 0 5.677l-1.038 1.038a3.211 3.211 0 0 0-.94 2.271v2.684a4.014 4.014 0 0 1-4.015 4.014h-2.684c-.852 0-1.669.339-2.27.94l-1.039 1.039a4.014 4.014 0 0 1-5.677 0l-1.038-1.038a3.211 3.211 0 0 0-2.27-.94H7.226a4.014 4.014 0 0 1-4.014-4.015v-2.684c0-.852-.338-1.669-.94-2.27l-1.039-1.039a4.014 4.014 0 0 1 0-5.677l1.038-1.038a3.208 3.208 0 0 0 .941-2.27V7.226a4.014 4.014 0 0 1 4.014-4.014H9.91c.852 0 1.669-.338 2.271-.94zm1.136 1.136a4.817 4.817 0 0 1-3.407 1.41H7.227a2.409 2.409 0 0 0-2.408 2.41V9.91a4.817 4.817 0 0 1-1.411 3.407L2.37 14.356a2.41 2.41 0 0 0 0 3.406L3.408 18.8a4.817 4.817 0 0 1 1.41 3.406v2.684a2.409 2.409 0 0 0 2.41 2.409H9.91a4.82 4.82 0 0 1 3.407 1.41l1.038 1.038c.94.941 2.465.941 3.406 0L18.8 28.71a4.817 4.817 0 0 1 3.406-1.41h2.684a2.409 2.409 0 0 0 2.409-2.409v-2.684c0-1.278.507-2.503 1.41-3.406l1.038-1.038a2.408 2.408 0 0 0 0-3.406l-1.038-1.038A4.817 4.817 0 0 1 27.3 9.91V7.227a2.409 2.409 0 0 0-2.41-2.407h-2.684A4.817 4.817 0 0 1 18.8 3.408L17.762 2.37a2.409 2.409 0 0 0-3.406 0l-1.038 1.038zM15.03 17.86l4.332-4.73a.803.803 0 1 1 1.224 1.04l-4.844 5.367a.803.803 0 0 1-1.112.076l-3.1-2.605a.803.803 0 0 1 1.027-1.233l2.473 2.085zm1.028 7.832c-.41 0-.41-1.482 0-1.482a8.152 8.152 0 0 0 6.657-12.858.741.741 0 1 1 1.21-.857 9.634 9.634 0 0 1-7.867 15.197zM21.68 8.234a.741.741 0 0 1-.866 1.203 8.152 8.152 0 0 0-12.908 6.578.741.741 0 0 1-1.483-.007A9.635 9.635 0 0 1 21.68 8.234zM6.907 19.077a.741.741 0 0 1 1.407-.464 8.155 8.155 0 0 0 7.745 5.598.741.741 0 1 1 0 1.482 9.637 9.637 0 0 1-9.152-6.616z"></path></symbol><symbol id="icon-explore"><path d="M15 28.3c7.3 0 13.3-6 13.3-13.3S22.3 1.7 15 1.7 1.7 7.7 1.7 15s6 13.3 13.3 13.3zm0 1.7C6.7 30 0 23.3 0 15S6.7 0 15 0s15 6.7 15 15-6.7 15-15 15zm0-4.2c-.5 0-.8-.3-.8-.8s.3-.8.8-.8c5 0 9-4 9.2-8.8 0-.5.3-.8.8-.8s.8.3.8.8c-.1 5.8-5 10.4-10.8 10.4zm-.5-21.6c.5 0 .8.3.8.8s-.3.8-.6.8c-5 .2-8.9 4.4-8.9 9.2 0 .5-.3.8-.8.8s-.8-.3-.8-.8c0-5.8 4.5-10.5 10.3-10.8zm1.8 13.5l-2-2c-.3-.3-.3-.8 0-1.2.3-.3.8-.3 1.2 0l2 2 2.7-6.7-7.5 2.8-3 7.5 6.6-2.4zm6.9-10.9L18.7 18c-.2.5-.5.8-1 1L6.5 23.5 11 12.3c.3-.7.7-1 1.2-1.2l11-4.3z"></path></symbol><symbol id="icon-ext-link" viewBox="0 0 16 16"><path fill="#676767" d="M12.9 16H3.1C1.4 16 0 14.6 0 12.9V3.2C0 1.4 1.4 0 3.1 0h3.7v1H3.1C2 1 1 2 1 3.2v9.7C1 14 2 15 3.1 15h9.7c1.2 0 2.1-1 2.1-2.1V8.7h1v4.2c.1 1.7-1.3 3.1-3 3.1z"></path><path fill="#676767" d="M12.8 2.5l.7.7-9 8.9-.7-.7 9-8.9z"></path><path fill="#676767" d="M9.7 0L16 6.2V0z"></path></symbol><symbol id="icon-info-bordered" viewBox="470.812 270.868 18 18"><path d="M479.812 270.868c-4.972 0-9 4.029-9 9s4.028 9 9 9c4.971 0 9-4.029 9-9s-4.029-9-9-9zm0 16.875a7.875 7.875 0 0 1-7.875-7.875 7.875 7.875 0 1 1 15.75 0 7.875 7.875 0 0 1-7.875 7.875z"></path><path d="M479.284 279.586c.089-.238-.024-.361-.13-.361-.489 0-1.123 1.16-1.359 1.16-.093 0-.173-.092-.173-.174 0-.238.581-.801.751-.971.526-.506 1.217-.895 1.979-.895.567 0 1.174.346.703 1.639l-.952 2.604c-.079.199-.224.531-.224.746 0 .092.054.186.159.186.395 0 1.121-1.135 1.304-1.135.067 0 .158.086.158.199 0 .385-1.542 2.043-2.874 2.043-.477 0-.804-.225-.804-.732 0-.641.447-1.734.538-1.963l.924-2.346zm.727-3.41c0-.586.498-1.066 1.082-1.066.527 0 .91.357.91.906 0 .615-.501 1.068-1.098 1.068-.541-.002-.894-.361-.894-.908z"></path></symbol><symbol id="icon-opr" viewBox="0 0 18 18"><path d="M.9 2.5c-.2-.2-.4-.3-.6-.2-.3.2-.3.4-.3.6 0 .8 0 1.7.1 2.5 0 .5.3.7.7.6.9 0 1.7-.2 2.5-.3h.2c.5-.2.6-.5.3-.8-.2-.2-.4-.4-.6-.5-.1-.1-.2-.2-.4-.3.5-.6 1-1.1 1.7-1.5 4.6-3 10.9-.5 12 4.9.9 4.4-2.2 8.6-6.7 9.2-3.7.5-7.2-1.6-8.4-5.1 0-.1-.1-.2-.1-.3-.1-.3-.5-.5-.8-.4-.3.1-.5.4-.4.8.4 1.3 1.1 2.4 2 3.4 1.4 1.5 3.1 2.4 5.1 2.8 3 .5 5.6-.2 7.8-2.2 2.1-1.9 3-4.2 3-6.9-.1-4-3.1-7.6-7.1-8.4-3.3-.9-6.2 0-8.6 2.3-.2.1-.3.3-.5.5-.3-.2-.6-.5-.9-.7z"></path><path d="M13 4.7c-2.6 1.6-4.5 3.6-5.3 4.6L5.6 7.7l-.9.7L8.3 12c.6-1.6 2.6-4.6 4.9-6.8l-.2-.5z"></path></symbol><symbol id="icon-remove" viewBox="-296 388 18 18"><path d="M-291.7 396.1h9v2h-9z"></path><path d="M-287 405.5c-4.7 0-8.5-3.8-8.5-8.5s3.8-8.5 8.5-8.5 8.5 3.8 8.5 8.5-3.8 8.5-8.5 8.5zm0-16c-4.1 0-7.5 3.4-7.5 7.5s3.4 7.5 7.5 7.5 7.5-3.4 7.5-7.5-3.4-7.5-7.5-7.5z"></path></symbol><symbol id="icon-rss"><ellipse cx="3.305" cy="20.702" rx="3.306" ry="3.298"></ellipse><path d="M15.978 24h-4.684c0-6.224-5.057-11.27-11.294-11.27V8.058c8.824 0 15.978 7.137 15.978 15.942z"></path><path d="M19.2 23.95C19.2 13.366 10.604 4.79 0 4.79V0c13.255 0 24 10.723 24 23.95h-4.8z"></path></symbol><symbol id="icon-search"><path d="M6.6 10.6c2.2 0 4-1.7 4-4s-1.7-4-4-4-4 1.7-4 4 1.8 4 4 4zm4 1.4l1.5-1.5 3.7 3.6c.4.5.4 1.2 0 1.6-.4.4-1.1.4-1.6 0L10.6 12zm-4 1.2C2.9 13.2 0 10.3 0 6.6S2.9 0 6.6 0s6.6 2.9 6.6 6.6-2.9 6.6-6.6 6.6z"></path></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 0 0 0-2H3.385l2.482-2.482a.994.994 0 0 0 .02-1.403 1.001 1.001 0 0 0-1.417 0L.294 5.292a1.001 1.001 0 0 0 0 1.416l4.176 4.177a.991.991 0 0 0 1.4.016 1 1 0 0 0-.003-1.42L3.385 7H15z"></path></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 0 1 0-2h11.615l-2.482-2.482a.994.994 0 0 1-.02-1.403 1.001 1.001 0 0 1 1.417 0l4.176 4.177a1.001 1.001 0 0 1 0 1.416l-4.176 4.177a.991.991 0 0 1-1.4.016 1 1 0 0 1 .003-1.42L12.615 7H1z"></path></symbol><symbol id="icon-updates" viewBox="0 0 18 18"><path d="M16.98 3.484h-.48c-2.52-.058-5.04 1.161-7.44 2.903-2.46-1.8-4.74-2.903-8.04-2.903-.3 0-.54.29-.54.58v9.813c0 .29.24.523.54.581 2.76.348 4.86 1.045 7.62 2.903.24.116.54.116.72 0 2.76-1.858 4.86-2.555 7.62-2.903.3-.058.54-.29.54-.58V4.064c0-.29-.24-.523-.54-.581zm-15.3 1.22c2.34 0 4.86 1.509 6.72 2.786v8.478c-2.34-1.394-4.38-2.09-6.72-2.439V4.703zm14.58 8.767c-2.34.348-4.38 1.045-6.72 2.439V7.374C12 5.632 14.1 4.645 16.26 4.645v8.826z"></path><path d="M9 .058c-1.56 0-2.76 1.22-2.76 2.671C6.24 4.181 7.5 5.4 9 5.4c1.5 0 2.76-1.22 2.76-2.671 0-1.452-1.2-2.67-2.76-2.67zm0 4.413c-.96 0-1.8-.755-1.8-1.742C7.2 1.8 7.98.987 9 .987s1.8.755 1.8 1.742c0 .93-.84 1.742-1.8 1.742z"></path></symbol><symbol id="icon-error" viewBox="2.002 0 14 14"><path d="M16.002 11.949L13.951 14 9.002 9.051 4.053 14l-2.051-2.051L6.951 7 2.002 2.05 4.053 0l4.949 4.95L13.951 0l2.051 2.049L11.053 7l4.949 4.949z"></path></symbol><symbol id="icon-info" viewBox="5.502 0 7 14"><path d="M8.165 5.923c.146-.384-.04-.577-.209-.577-.783 0-1.798 1.856-2.177 1.856-.15 0-.277-.147-.277-.279 0-.383.933-1.282 1.203-1.557.846-.811 1.949-1.434 3.174-1.434.908 0 1.879.555 1.125 2.63l-1.526 4.167c-.126.32-.358.854-.358 1.198 0 .148.088.298.255.298.632 0 1.796-1.818 2.091-1.818.105 0 .252.134.252.321C11.717 11.345 9.245 14 7.11 14c-.763 0-1.289-.361-1.289-1.176 0-1.027.717-2.778.865-3.145l1.479-3.756zm1.144-4.211C9.309.771 10.111 0 11.045 0c.845 0 1.457.577 1.457 1.456 0 .983-.804 1.709-1.757 1.709-.866-.001-1.436-.578-1.436-1.453z"></path></symbol><symbol id="icon-success"><path d="M15.592 0c-4.867 2.984-8.4 6.751-9.987 8.641L1.717 5.595 0 6.979l6.717 6.832c1.157-2.961 4.817-8.748 9.287-12.86L15.592 0z"></path></symbol><symbol id="icon-warning" viewBox="7.002 0 2.789 14"><path d="M7.002 11.211h2.789V14H7.002v-2.789zM7.002 0v3.436l.741 5.326h1.326l.723-5.326V0h-2.79z"></path></symbol><symbol id="icon-facebook-bordered" viewBox="463.812 263.868 32 32"><path d="M479.812 263.868c-8.837 0-16 7.163-16 16s7.163 16 16 16 16-7.163 16-16-7.163-16-16-16zm0 30c-7.732 0-14-6.269-14-14s6.268-14 14-14 14 6.269 14 14-6.267 14-14 14z"></path><path d="M483.025 280.48l.32-2.477h-2.453v-1.582c0-.715.199-1.207 1.227-1.207h1.311v-2.213a17.753 17.753 0 0 0-1.907-.098c-1.894 0-3.186 1.154-3.186 3.271V278h-2.142v2.477h2.142v6.354h2.557v-6.354l2.131.003z"></path></symbol><symbol id="icon-twitter-bordered" viewBox="463.812 263.868 32 32"><path d="M486.416 276.191a5.622 5.622 0 0 1-1.554.429 2.718 2.718 0 0 0 1.19-1.502 5.456 5.456 0 0 1-1.72.657 2.71 2.71 0 0 0-1.979-.854 2.711 2.711 0 0 0-2.642 3.326 7.681 7.681 0 0 1-5.586-2.831 2.714 2.714 0 0 0 .839 3.618 2.748 2.748 0 0 1-1.227-.339v.031a2.71 2.71 0 0 0 2.174 2.656 2.735 2.735 0 0 1-1.229.049 2.726 2.726 0 0 0 2.531 1.883 5.442 5.442 0 0 1-4.01 1.123 7.672 7.672 0 0 0 4.155 1.215c4.983 0 7.71-4.129 7.71-7.711 0-.115-.004-.232-.006-.351a5.41 5.41 0 0 0 1.354-1.399z"></path><path d="M479.812 263.868c-8.837 0-16 7.163-16 16s7.163 16 16 16 16-7.163 16-16-7.163-16-16-16zm0 30c-7.732 0-14-6.269-14-14s6.268-14 14-14 14 6.269 14 14-6.267 14-14 14z"></path></symbol><symbol id="icon-weibo-bordered" viewBox="463.812 263.868 32 32"><path d="M479.812 263.868c-8.838 0-16 7.163-16 16s7.162 16 16 16c8.837 0 16-7.163 16-16s-7.163-16-16-16zm0 30c-7.732 0-14-6.269-14-14s6.268-14 14-14c7.731 0 14 6.269 14 14s-6.267 14-14 14z"></path><path d="M478.552 285.348c-2.616.261-4.876-.926-5.044-2.649-.167-1.722 1.814-3.33 4.433-3.588 2.609-.263 4.871.926 5.041 2.647.165 1.721-1.818 3.331-4.43 3.59m5.23-5.718c-.226-.065-.374-.109-.259-.403.25-.639.276-1.188.005-1.581-.515-.734-1.915-.693-3.521-.021 0 0-.508.224-.378-.181.247-.798.209-1.468-.178-1.852-.87-.878-3.194.032-5.183 2.027-1.489 1.494-2.357 3.082-2.357 4.453 0 2.619 3.354 4.213 6.631 4.213 4.297 0 7.154-2.504 7.154-4.493.001-1.198-1.007-1.881-1.914-2.162m2.855-4.797a4.176 4.176 0 0 0-3.982-1.291.608.608 0 0 0-.465.72.604.604 0 0 0 .72.466 2.968 2.968 0 0 1 2.827.92 3 3 0 0 1 .625 2.918.602.602 0 0 0 .39.762.603.603 0 0 0 .763-.391v-.001a4.218 4.218 0 0 0-.878-4.103"></path><path d="M485.041 276.276a2.037 2.037 0 0 0-1.938-.63.518.518 0 0 0-.396.621.517.517 0 0 0 .617.398c.336-.071.702.03.947.307s.312.649.207.979a.52.52 0 0 0 .336.654.523.523 0 0 0 .657-.336 2.038 2.038 0 0 0-.43-1.993m-6.347 5.951c-.09.156-.293.233-.451.166-.151-.062-.204-.235-.115-.389.093-.155.284-.229.44-.168.157.056.214.235.126.391m-.832 1.074c-.253.405-.795.58-1.202.396-.403-.186-.521-.655-.27-1.051.248-.39.771-.566 1.176-.393.413.17.543.636.296 1.048m.95-2.864c-1.244-.326-2.65.294-3.19 1.396-.553 1.119-.021 2.369 1.236 2.775 1.303.42 2.84-.225 3.374-1.436.526-1.183-.132-2.402-1.42-2.735"></path></symbol></svg>
</div>

        <a class="u-visually-hidden u-visually-hidden-focus" href="#main-content">
    <span class="c-banner">Skip to main content</span>
</a>

        
            
    <div class="adsbox c-ad c-ad--LB1">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/bmc/plantmethods/articles" data-gpt-sizes="728x90,970x90" data-gpt-targeting="pos=LB1;doi=10.1186/s13007-019-0475-z;kwrd=Artificial intelligence,Banana,Deep learning,Disease detection,Transfer learning,Convolutional neural networks,Mobile app;pmc=L24000,L28000;" data-ad-type="LB1" data-google-query-id="CN2u-c3onOYCFQm1fgodFS0MBQ">
                
            <div id="google_ads_iframe_/270604982/bmc/plantmethods/articles_0__container__" style="border: 0pt none;"><iframe id="google_ads_iframe_/270604982/bmc/plantmethods/articles_0" title="3rd party ad content" name="google_ads_iframe_/270604982/bmc/plantmethods/articles_0" scrolling="no" marginwidth="0" marginheight="0" srcdoc="" data-google-container-id="2" style="border: 0px; vertical-align: bottom;" data-load-complete="true" width="728" height="90" frameborder="0"></iframe></div></div>
        </div>
    </div>

        
         
    
        <div id="membership-message-loader-desktop" class="placeholder" data-placeholder="/placeholder/v1/membership/message"></div>
    
    
        <div id="top" class="c-popup-search">
    <header class="c-header" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand u-margin-right-xxl" itemscope="" itemtype="http://schema.org/Organization" data-test="navbar-logo-header">
                <div class="c-logo">
    <a href="https://www.biomedcentral.com/" itemprop="url">
        <img alt="BMC" itemprop="logo" role="img" src="logo-bmc-white-aj-e3a70ae848.svg" width="76" height="18">
        <div class="c-logo__strapline">
            <img alt="Part of Springer Nature" role="img" src="logo-bmc-white-strapline-sn-e7b1d6e67f.svg" width="173" height="16">
        </div>
    </a>
</div>

            </div>
            <div class="c-header__navigation">
                <button type="button" class="c-header__link u-button-reset js-publisher-search-button u-margin-right-lg" data-toggle="collapse" data-test="header-search-button" data-target="publisher-header-search" aria-controls="publisher-header-search" aria-expanded="false">
                    <span class="u-display-flex u-flex-align-center">
                        Search
                        <svg class="c-icon u-margin-left-xs" width="14" height="14" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-search"></use>
                        </svg>
                    </span>
                </button>
                <nav>
                    <ul class="c-header__menu u-hide-at-lt-lg" data-header-menu="" data-test="publisher-navigation">
                        
                            
                                <li class="c-header__item">
                                    <a class="c-header__link" href="https://www.biomedcentral.com/journals">
                                        Explore journals
                                    </a>
                                </li>
                            
                                <li class="c-header__item">
                                    <a class="c-header__link" href="https://www.biomedcentral.com/getpublished">
                                        Get published
                                    </a>
                                </li>
                            
                                <li class="c-header__item">
                                    <a class="c-header__link" href="https://www.biomedcentral.com/about">
                                        About BMC
                                    </a>
                                </li>
                            
                        
                        <li class="c-header__item">
                            <a data-header-account="" class="c-header__link" href="https://www.biomedcentral.com/login" data-test="login-link">Login</a>
                        </li>
                    </ul><div class="c-dropdown u-hide-at-lg"><button class="c-dropdown__button c-header__item" aria-expanded="false" aria-haspopup="true">Menu</button><ul class="c-dropdown__menu u-hide c-dropdown__menu--right"><li class="c-dropdown__item"><a class="c-dropdown__link" href="https://www.biomedcentral.com/journals">
                                        Explore journals
                                    </a></li><li class="c-dropdown__item"><a class="c-dropdown__link" href="https://www.biomedcentral.com/getpublished">
                                        Get published
                                    </a></li><li class="c-dropdown__item"><a class="c-dropdown__link" href="https://www.biomedcentral.com/about">
                                        About BMC
                                    </a></li><li class="c-dropdown__item"><a class="c-dropdown__link" href="https://www.biomedcentral.com/login">Login</a></li></ul></div>
                </nav>
            </div>
        </div>
    </header>
    <div class="c-popup-search__content c-collapse js-publisher-search-bar" id="publisher-header-search">
        <div class="u-container">
            <div class="c-popup-search__container">
                <div class="ctx-search">
    <form role="search" class="c-form-field" method="GET" action="//www.biomedcentral.com/search" data-track="submit" data-track-category="Search and Results" data-track-action="Submit search" data-dynamic-track-label="" data-track-label="" data-test="global-search" novalidate="">
        <label for="publisherSearch" class="c-form-field__label">Search all BMC articles</label>
        <div class="u-display-flex">
            <input id="publisherSearch" class="c-form-field__input js-publisher-search-input" autocomplete="off" role="textbox" data-test="search-input" name="query" type="text">
            <div>
                <button class="c-button" type="submit" data-test="search-submit-button">
    <span class="u-visually-hidden">Search</span>
    <svg class="c-icon" width="16" height="16" aria-hidden="true" focusable="false">
        <use xlink:href="#icon-search"></use>
    </svg>
</button>


            </div>
        </div>
        <input name="searchType" value="publisherSearch" type="hidden">
    </form>
</div>

            </div>
        </div>
    </div>
</div>

    

        
            <header class="c-journal-header c-journal-header--plant-methods ctx-journal-header">
                <div class="u-container">
                    <div class="c-journal-header__inner ">
                        
                        <div class="c-journal-title" id="journalTitle">
                            <a href="https://plantmethods.biomedcentral.com/">
    
        
            <img class="c-journal-title__logo" src="logo.svg" alt="Plant Methods logo">
        
    
    <span class="c-journal-title__text ">Plant Methods</span>
    
</a>
                        </div>
                        
                    </div>
                </div>
                <div class="c-navbar">
                    <div class="c-navbar__container">
                        
                            <div class="c-navbar__content">
                                <nav class="c-navbar__nav">
                                    <ul class="c-navbar__nav c-navbar__nav--journal" role="menu" data-test="site-navigation">
                                        
                                            <li class="c-navbar__item" role="menuitem">
                                                <a class="c-navbar__link" data-track="click" data-track-category="Home" data-track-action="Clicked journal navigation link" href="https://plantmethods.biomedcentral.com/">Home</a>
                                            </li>
                                        
                                            <li class="c-navbar__item" role="menuitem">
                                                <a class="c-navbar__link" data-track="click" data-track-category="About" data-track-action="Clicked journal navigation link" href="https://plantmethods.biomedcentral.com/about">About</a>
                                            </li>
                                        
                                            <li class="c-navbar__item" role="menuitem">
                                                <a class="c-navbar__link c-navbar__link--is-shown" data-track="click" data-track-category="Articles" data-track-action="Clicked journal navigation link" href="https://plantmethods.biomedcentral.com/articles">Articles</a>
                                            </li>
                                        
                                            <li class="c-navbar__item" role="menuitem">
                                                <a class="c-navbar__link" data-track="click" data-track-category="Submission Guidelines" data-track-action="Clicked journal navigation link" href="https://plantmethods.biomedcentral.com/submission-guidelines">Submission Guidelines</a>
                                            </li>
                                        
                                    </ul>
                                </nav>
                            </div>
                        
                    </div>
                </div>
                <div class="c-journal-header__identity c-journal-header__identity--jungle-yellow">
                    
                </div>
            </header>
            
        

        <div class="u-container u-margin-top-xl u-margin-bottom-xl c-article-container" id="main-content" data-component="article-container">
            <div class="c-page-layout c-page-layout--article" data-component="sticky-container">

                <main class="c-page-layout__main">
                    <article itemscope="" itemtype="http://schema.org/ScholarlyArticle" lang="en">
                        <div class="c-article-header">
                            

                            <ul class="c-article-identifiers" data-test="article-identifier">
                                
    <li class="c-article-identifiers__item" data-test="article-category">Research</li>
    
        
            <li class="c-article-identifiers__item">
                <span class="c-article-identifiers__open" data-test="open-access">Open Access</span>
            </li>
        
        
    
    

                                <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-category="article body" data-track-label="link">Published: <time datetime="2019-08-12" itemprop="datePublished">12 August 2019</time></a></li>
                            </ul>

                            <h1 class="c-article-title u-h1" data-test="article-title" data-article-title="" itemprop="name headline">AI-powered banana diseases and pest detection</h1>
                            <ul class="c-author-list js-list-authors js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-1" data-corresp-id="c1" class="js-no-scroll">Michael Gomez Selvaraj<svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide">&nbsp;
            <a class="js-orcid js-no-scroll" itemprop="url" href="http://orcid.org/0000-0003-2394-0399"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-2394-0399</a></span><sup class="u-js-hide"><a href="#Aff1" class="js-no-scroll" tabindex="-1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="International Center for Tropical Agriculture (CIAT)"><meta itemprop="address" content="0000 0001 0943 556X, grid.418348.2, International Center for Tropical Agriculture (CIAT), A.A. 6713, Cali, Colombia"></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-2" class="js-no-scroll">Alejandro Vergara</a></span><sup class="u-js-hide"><a href="#Aff1" class="js-no-scroll" tabindex="-1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="International Center for Tropical Agriculture (CIAT)"><meta itemprop="address" content="0000 0001 0943 556X, grid.418348.2, International Center for Tropical Agriculture (CIAT), A.A. 6713, Cali, Colombia"></span></sup>, </li><li class="c-author-list__item js-smaller-author-etal" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-3" class="js-no-scroll">Henry Ruiz</a></span><sup class="u-js-hide"><a href="#Aff2" class="js-no-scroll" tabindex="-1">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Texas A&amp;M University"><meta itemprop="address" content="0000 0004 4687 2082, grid.264756.4, Department of Soil and Crop Sciences, Texas A&amp;M University, College Station, TX, USA"></span></sup>, </li><li class="c-author-list__item js-smaller-author-etal" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-4" class="js-no-scroll">Nancy Safari</a></span><sup class="u-js-hide"><a href="#Aff3" class="js-no-scroll" tabindex="-1">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Bioversity International"><meta itemprop="address" content="Bioversity International, Bukavu, South Kivu Province, Democratic Republic of Congo"></span></sup>, </li><li class="c-author-list__item js-smaller-author-etal" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-5" class="js-no-scroll">Sivalingam Elayabalan</a></span><sup class="u-js-hide"><a href="#Aff4" class="js-no-scroll" tabindex="-1">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Imayam Institute of Agriculture and Technology (IIAT), Affiliated to Tamil Nadu Agricultural University (TNAU)"><meta itemprop="address" content="0000 0001 2155 9899, grid.412906.8, Department of Biotechnology, Imayam Institute of Agriculture and Technology (IIAT), Affiliated to Tamil Nadu Agricultural University (TNAU), Tiruchirappalli, Tamil Nadu, India"></span></sup>, </li><li class="c-author-list__item js-smaller-author-etal" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-6" class="js-no-scroll">Walter Ocimati</a></span><sup class="u-js-hide"><a href="#Aff5" class="js-no-scroll" tabindex="-1">5</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Bioversity International"><meta itemprop="address" content="Bioversity International, Kampala, Uganda"></span></sup> &amp; </li><li class="c-author-list__show-more u-js-hide js-mq480-show-inline"><a href="javascript:;" class="js-etal js-no-scroll" title="Show all 7 authors" aria-label="Show all 7 authors for this article">[…]</a></li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-7" class="js-no-scroll">Guy Blomme</a></span><sup class="u-js-hide"><a href="#Aff6" class="js-no-scroll" tabindex="-1">6</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Bioversity International"><meta itemprop="address" content="Bioversity International, c/o ILRI, Addis Ababa, Ethiopia"></span></sup>&nbsp;</li><li class="c-author-list__show-less u-js-hide"><a href="javascript:;" class="js-etal js-no-scroll" aria-label="Show fewer authors for this article">- Show fewer authors</a></li></ul>
                            <p class="c-article-info-details" data-container-section="info">
                                
    <a data-test="journal-link" href="https://plantmethods.biomedcentral.com/"><i data-test="journal-title">Plant Methods</i></a>

                                <b data-test="journal-volume"><span class="u-visually-hidden">volume</span>&nbsp;15</b>, Article&nbsp;number:&nbsp;<span data-test="article-number">92</span> (<span data-test="article-publication-year">2019</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                            </p>
                            
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <h2 class="u-visually-hidden">Article metrics</h2>
            <ul class="c-article-metrics-bar u-list-inline">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">7173 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">171 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__details"><a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/metrics" data-track="click" data-track-action="view metrics" data-track-category="article body" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                        </li>
                    
                
            </ul>
        </div>
    

                            

                            
                        </div>

                        <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><h3 class="c-article__sub-heading u-h3" data-test="abstract-sub-heading">Background</h3><p>Banana (<i>Musa</i>
 spp.) is the most popular marketable fruit crop grown all over the 
world, and a dominant staple food in many developing countries. 
Worldwide, banana production is affected by numerous diseases and pests.
 Novel and rapid methods for the timely detection of pests and diseases 
will allow to surveil and develop control measures with greater 
efficiency. As deep convolutional neural networks (DCNN) and transfer 
learning has been successfully applied in various fields, it has freshly
 moved in the domain of just-in-time crop disease detection. The aim of 
this research is to develop an AI-based banana disease and pest 
detection system using a DCNN to support banana farmers.</p><h3 class="c-article__sub-heading u-h3" data-test="abstract-sub-heading">Results</h3><p>Large
 datasets of expert pre-screened banana disease and pest symptom/damage 
images were collected from various hotspots in Africa and Southern 
India. To build a detection model, we retrained three different 
convolutional neural network (CNN) architectures using a transfer 
learning approach. A total of six different models were developed from 
18 different classes (disease by plant parts) using images collected 
from different parts of the banana plant. Our studies revealed ResNet50 
and InceptionV2 based models performed better compared to MobileNetV1. 
These architectures represent the state-of-the-art results of banana 
diseases and pest detection with an accuracy of more than 90% in most of
 the models tested. These experimental results were comparable with 
other state-of-the-art models found in the literature. With a future 
view to run these detection capabilities on a mobile device, we 
evaluated the performance of SSD (single shot detector) MobileNetV1. 
Performance and validation metrics were also computed to measure the 
accuracy of different models in automated disease detection methods.</p><h3 class="c-article__sub-heading u-h3" data-test="abstract-sub-heading">Conclusion</h3><p>Our
 results showed that the DCNN was a robust and easily deployable 
strategy for digital banana disease and pest detection. Using a 
pre-trained disease recognition model, we were able to perform deep 
transfer learning (DTL) to produce a network that can make accurate 
predictions. This significant high success rate makes the model a useful
 early disease and pest detection tool, and this research could be 
further extended to develop a fully automated mobile app to help 
millions of banana farmers in developing countries.</p></div></div></section>
                        
    


                        <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec1">Background</h2><div class="c-article-section__content" id="Sec1-content"><p>Bananas (<i>Musa</i> spp.) are one of the world’s most important fruit crops in terms of production volume and trade [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="FAO. Banana market review and banana statistics 2012–2013. Market and policy analyses of raw materials, horticulture and tropical (RAMHOT) Products Team. Rome; 2014." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR1" id="ref-link-section-d65006e927">1</a>]. Though a major staple food in Africa, Asia, and Latin America, only 13% of bananas produced are globally traded [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Lescot T. World plantain and banana production systems. In: Proceedings XX international meeting ACORBAT: 9–13 September 2013; Fortaleza; 2013. p. 26–34." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR2" id="ref-link-section-d65006e930">2</a>],
 clearly indicating the fruit’s importance in domestic markets and food 
security. In East and Central Africa, it is a substantial dietary 
component, accounting for over 50% of daily total food intake in parts 
of Uganda and Rwanda [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Abele S, Twine E, Legg C. Food security in eastern Africa and the great lakes. Crop Crisis Control Project final report. Ibadan: Int Instit Trop Agric; 2007." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR3" id="ref-link-section-d65006e933">3</a>]. Smallholder farmers, representing 85% of the world’s farms [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Nagayets O. Small farms: current status and key trends. In: The future of small farms; 2005. p. 355." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR4" id="ref-link-section-d65006e936">4</a>],
 face many abiotic and biotic constraints. Several banana pests and 
diseases have caused significant yield losses across production 
landscapes [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blomme G, Dita M, Jacobsen KS, Perez Vicente L, Molina A, Ocimati W, Poussier S, Prior P. Bacterial diseases of bananas and enset: current state of knowledge and integrated approaches toward sustainable management. Front Plant Sci. 2017;8:1290." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR5" id="ref-link-section-d65006e940">5</a>]
 and are a significant threat to global food security. Therefore, early 
detection of pests and diseases in the field is a first crucial step. 
Traditional pest and disease identification approaches rely on 
agricultural extension specialists, but these approaches are limited in 
developing countries with low human infrastructure capacity. Many 
smallholder farmers rely on empirical knowledge, which is less effective
 in overcoming farming challenges [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Hillnhuetter C, Mahlein AK. Early detection and localisation of sugar beet diseases: new approaches. Gesunde Pflanzen. 2008;60(4):143–9." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR6" id="ref-link-section-d65006e943">6</a>].
 The early identification of a crop disease or pest can lead to faster 
interventions with resulting reduced impacts on food supply chains.</p><p>Artificial
 intelligence (AI) with deep learning models which help to identify 
plant diseases by the plant’s appearance and visual symptoms that mimic 
human behavior should be considered [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Camargo A, Smith J. An image-processing based algorithm to automatically identify plant disease visual symptoms. Biosyst Eng. 2009;102(1):9–21." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR7" id="ref-link-section-d65006e949">7</a>].
 Smartphone-based AI apps could alert farmers and expedite disease 
diagnosis, thus preventing the possible outbreak of pests and diseases [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Mohanty SP, Hughes DP, Salathe M. Using deep learning for image-based plant disease detection. Front Plant Sci. 2016;7:1419." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR8" id="ref-link-section-d65006e952">8</a>].
 Even though many farmers of developing countries do not have access to 
these advanced tools, internet infiltration and smartphone penetration 
offer new outfits for in-field crop disease detection. The Global System
 for Mobile Association (GMSA) predicted that global smartphone 
subscriptions would reach 5 billion by 2020, of which nearly one billion
 in Africa [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Intelligence G. The mobile economy Africa 2016. London: GSMA; 2016." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR9" id="ref-link-section-d65006e955">9</a>].
 We do believe that cutting-edge technologies like AI, IoT (Internet of 
Things), robotics, satellites, cloud computing, and machine learning are
 transfiguring agriculture and helping farmers foresee their near 
future.</p><p>Deep learning is a novel method for image processing and 
object detection with greater accuracy in the classification of various 
crop diseases [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Kamilaris A, Prenafeta-Boldu FX. Deep learning in agriculture: a survey. Comput Elect Agric. 2018;147:70–90." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR10" id="ref-link-section-d65006e961">10</a>].
 Transfer learning is one such popular approach in deep learning, where 
pre-trained models are adapted to do a new task. Deep transfer learning 
(DTL) generates a fresh framework for digital image processing and 
predictive analytics, with greater accuracy and has huge potential in 
crop disease detection. DTL approach also offers a promising avenue for 
in-field disease recognition using large trained image datasets and bids
 a shortcut to the developed models to meet the restrictions that are 
offered by mobile application [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Ramcharan A, Baranowski K, McCloskey P, Ahmed B, Legg J, Hughes DP. Deep learning for image-based cassava disease detection. Front Plant Sci. 2017;8:1852." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR11" id="ref-link-section-d65006e964">11</a>]. This would have a distinct practical value for real field environment.</p><p>Earlier investigations have validated AI-based recognition of crop diseases in wheat [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Siricharoen P, Scotney B, Morrow P, Parr G. A lightweight mobile system for crop disease diagnosis. International conference on image analysis and recognition. Berlin: Springer; 2016. p. 783–91." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR12" id="ref-link-section-d65006e970">12</a>], cassava [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Ramcharan A, Baranowski K, McCloskey P, Ahmed B, Legg J, Hughes DP. Deep learning for image-based cassava disease detection. Front Plant Sci. 2017;8:1852." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR11" id="ref-link-section-d65006e973">11</a>] and on datasets of healthy and diseased plants [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Mohanty SP, Hughes DP, Salathe M. Using deep learning for image-based plant disease detection. Front Plant Sci. 2016;7:1419." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR8" id="ref-link-section-d65006e976">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Wiesner-Hanks T, Stewart EL, Kaczmar N, DeChant C, Wu H, Nelson RJ, Lipson H, Gore MA. Image set for deep learning: field images of maize annotated with disease symptoms. BMC Res Notes. 2018;11(1):440." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR13" id="ref-link-section-d65006e979">13</a>]. Crop disease recognition based on a computerized image system through feature extraction has revealed promising results [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Mwebaze E, Owomugisha G. Machine learning for plant disease incidence and severity measurements from leaf images.&nbsp;2016 15th IEEE international conference on machine learning and applications (ICMLA). New York: IEEE; 2016. p. 158–63." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR14" id="ref-link-section-d65006e982">14</a>]
 but extracting features is computationally rigorous and involves expert
 knowledge for robust depiction. Only few restricted large, curated 
image datasets of crop disease library exists [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Kamilaris A, Prenafeta-Boldu FX. Deep learning in agriculture: a survey. Comput Elect Agric. 2018;147:70–90." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR10" id="ref-link-section-d65006e986">10</a>]. The PlantVillage platform holds over 50,000 images of different crops and diseases [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Hughes D, Salathe M. An open access repository of images on plant health to enable the development of mobile disease diagnostics. arXiv preprint 
                    arXiv:1511.08060
                    
                  ; 2015." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR15" id="ref-link-section-d65006e989">15</a>].
 However, most of these images were taken with detached leaves on a 
plain background, and CNN trained on these images did not achieve well 
when using real field images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Mohanty SP, Hughes DP, Salathe M. Using deep learning for image-based plant disease detection. Front Plant Sci. 2016;7:1419." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR8" id="ref-link-section-d65006e992">8</a>].
 To build robust and more practical detection models, plenty of healthy 
and diseased images taken from different infected parts of the plants, 
and growing under different environmental conditions are needed. These 
images subsequently need to be labeled and pre-screened by plant 
pathology experts. So far, existing crop disease detection models are 
mostly focusing on leaf symptoms. Unfortunately, numerous symptoms also 
appear in other parts of the plant and the best examples are banana pest
 and disease linked symptoms.</p><p>The objective of this study was to 
apply state-of-the-art deep learning techniques for the detection of 
visible banana disease and pest symptoms on different parts of the 
banana plant. We also considered the potential for adapting pre-trained 
deep learning CNN models to detect banana disease and pest symptoms 
using a large dataset of experts’ pre-screened real field images 
collected from Africa and India.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec2">Materials and methods</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading u-h3" id="Sec3">System description</h3><p>Our
 DTL system dataset consists of five major banana diseases along with 
their respective healthy classes; dried/old age leaves and banana corm 
weevil (<i>Cosmopolites sordidus</i>) damage symptom classes (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab1">1</a>).
 Since these major diseases and pest can affect different parts of the 
banana plant, we ended up with six different models (entire plant, 
leaves, pseudostem, fruit bunch, cut fruits and corm) and 18 different 
classes (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab1">1</a>) to achieve maximum accuracy. An overview of the DTL system is illustrated in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table&nbsp;1 Description of annotated banana datasets used in this study</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-category="article body" data-track-label="button" rel="nofollow" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig.&nbsp;1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig1_HTML.png?as=webp"><img aria-describedby="figure-1-desc" src="13007_2019_475_Fig1_HTML.png" alt="figure1" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Overview of deep transfer learning (DTL) system for banana disease and pest detection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading u-h3" id="Sec4">Dataset collection</h3><p>Our
 dataset comprises of about 18,000 field images of banana, collected by 
banana experts, from Bioversity International (Africa) and Tamil Nadu 
Agricultural University (TNAU, Southern India) (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>:
 Table S1). These field images were captured under different 
environmental conditions to build a robust model. For that purpose, 
various banana experts visited several banana farms located in 
disease/pest hotspots of Africa (Eastern Democratic Republic of Congo, 
Central Uganda, Burundi and Benin Republic) and Southern India (Tamil 
Nadu and Kerala). Our current dataset consists of various types of data,
 including images with various resolutions (cell phone, tablets, 
standard RGB camera); light conditions depending on time of image taking
 (e.g., illumination), season (e.g., temperature, humidity), and 
different environmental locations (e.g., Africa, India). We have 
collected the images at different growing phases of the crop (i.e., 
vegetative and reproductive). To prevent our model from being confused 
between dried/old leaves and diseased leaves, we also collected numerous
 images of dried and old age leaves at different plant growth stages. 
Images of a specific disease were collected from different varieties, at
 different plant growth stages and in different environments (Africa and
 India) in order to enrich the image library (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>: Table S1).</p><p>Our
 current CIAT banana image library consists of approximately 18,000 real
 field images. But in this present study, our datasets cover healthy 
plants (HP), dried/old age leaves (DOL) and a balanced number of images 
(700 images) from five major diseases such as, Xanthomonas wilt of 
banana (BXW), Fusarium wilt of banana (FWB), black sigatoka (BS), yellow
 sigatoka (YS) and banana bunchy top disease (BBTV) along with the 
banana corm weevil (BCW) pest class. The major pest (corm weevil) and 
disease class symptoms and their control measures are presented in 
Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>:
 Table S2. Since symptoms of different diseases and pests are seen at 
different parts of the banana plants, we captured images of all the 
plant parts (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig2">2</a>).
 Our current library was structured based on the disease and the 
affected plant parts so each part of the plant represents a model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig.&nbsp;2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig2_HTML.jpg?as=webp"><img aria-describedby="figure-2-desc" src="13007_2019_475_Fig2_HTML.jpg" alt="figure2" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Detected classes and expected output from each model. <b>a</b> Entire plant affected by banana bunchy top virus (BBTV), <b>b</b> leaves affected by black sigatoka (BS), <b>c</b> cut pseudostem of Xanthomonas wilt (BXW) affected plant showing yellow bacterial ooze, <b>d</b> fruit bunch affected by Xanthomonas wilt (BXW), <b>e</b> cut fruit affected by Xanthomonas wilt (BXW), <b>f</b> corm affected by banana corm weevil (BCW)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading u-h3" id="Sec5">Data labeling</h3><p>The image tagging process was done using LabelImg software [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="LabelImg Software. 
                    https://github.com/tzutalin/labelImg/
                    
                  . Accessed 1 Feb 2019." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR16" id="ref-link-section-d65006e1501">16</a>]. Labels and coordinates of the boxes were saved as an XML file, in the same format (PASCAL VOC) used by ImageNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="ImageNet Data Set. 
                    http://www.image-net.org/
                    
                  . Accessed 12 Mar 2019." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR17" id="ref-link-section-d65006e1504">17</a>].
 The number of annotated samples corresponded to the number of bounding 
boxes labeled in each image. Every image could contain more than one 
annotation depending on the number of infected areas of the plant parts 
(Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig.&nbsp;3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig3_HTML.jpg?as=webp"><img aria-describedby="figure-3-desc" src="13007_2019_475_Fig3_HTML.jpg" alt="figure3" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Demonstration of the disease detection process during training. <b>a</b> Original raw images, <b>b</b> labeled process (desired output), <b>c</b> disease detection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading u-h3" id="Sec6">CNN architectures</h3><p>To train the models, we used three different architectures, such as ResNet50 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 770–8." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR18" id="ref-link-section-d65006e1546">18</a>], InceptionV2 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint 
                    arXiv:1502.03167
                    
                  . 2015." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR19" id="ref-link-section-d65006e1549">19</a>] and MobileNetV1 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H. Mobilenets: efficient convolutional neural networks for mobile vision applications. arXiv preprint 
                    arXiv:1704.04861
                    
                  . 2017." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR20" id="ref-link-section-d65006e1552">20</a>].
 For the object detector model architecture, we chose Faster RCNN with 
ResNet50 and InceptionV2 due to their accuracy. Single Shot Multibox 
(SSD) model was selected with the MobileNetV1 since this was one of the 
fastest object detection models available in TensorFlow [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Huang J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A, Fischer I, Wojna Z, Song Y, Guadarrama S. Speed/accuracy trade-offs for modern convolutional object detectors. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 7310–1." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR21" id="ref-link-section-d65006e1555">21</a>].
 To train these models, we used a python deep learning library called 
TensorFlow and its object detection Application Programming Interface 
(API) with the Graphics Process Unit (GPU) version [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="TensorFlow Python API. 
                    https://www.tensorflow.org/api_docs/python
                    
                  . Accessed 10 Feb 2019." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR22" id="ref-link-section-d65006e1558">22</a>]. Pre-trained models were trained with COCO (Common objects in context) data set [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="COCO Data Set. 
                    http://cocodataset.org/
                    
                  . Accessed 15 Feb 2019." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR23" id="ref-link-section-d65006e1562">23</a>],
 and it is openly available in the TensorFlow object detection API zoo 
models. These three architectures were re-trained using the transfer 
learning approach from the pre-trained versions. To finetune the 
original hyperparameters, the following configuration changes were 
executed, batch size and epoch number. The batch size was changed only 
in the MobileNetV1 from 24 to 6, and the epoch number was kept 15,000 
for all the architectures trained.</p><h3 class="c-article__sub-heading u-h3" id="Sec7">Training</h3><p>One
 of the most challenging tasks in machine learning is splitting the data
 without suffering from overfitting, under fitting or generalization 
hitches. Nevertheless, there are several refined statistical sampling 
methods which provide a path to deal with these common disputes [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Reitermanova Z. Data splitting. In: WDS’10 proceedings of contributed papers, Part I, vol 10; 2010. p. 31–6." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR24" id="ref-link-section-d65006e1574">24</a>].
 For developing banana model, our dataset was divided into the following
 proportions of 70%, 20%, 10%, for training (Ttr), validation (Tv) and 
testing (Tt), respectively. The simple random sampling (SRS) technique 
was selected, considering that it is efficient and simple to implement [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Reitermanova Z. Data splitting. In: WDS’10 proceedings of contributed papers, Part I, vol 10; 2010. p. 31–6." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR24" id="ref-link-section-d65006e1577">24</a>].</p><h3 class="c-article__sub-heading u-h3" id="Sec8">Performance metrics</h3><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec9">Loss function</h4><p>Classification loss is used to measure the model’s confidence by classifying the pixels region delimitated by the bounding box [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC. Ssd: Single shot multibox detector. In: European conference on computer vision. Springer; 2016. p. 21–37." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR25" id="ref-link-section-d65006e1592">25</a>]
 and the localization loss measures the geometric distance between the 
predicted bounding box and the ground truth annotation (validation 
bounding boxes). In this paper, we used the object detection API [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Object Detection API Loss Functions Implementation, Tensorflow. 
                    https://github.com/tensorflow/models/blob/master/research/object_detection/core/losses.py
                    
                  . Accessed 5 Mar 2019." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR26" id="ref-link-section-d65006e1595">26</a>]
 to estimate the total loss function to measure model performance. The 
overall loss function or total loss was a weighted combination of the 
classification loss (classif) and the localization loss (loc).</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec10">MaP score</h4><p>The
 mean average precision (mAP) was used as the validation metric for 
banana disease and pest detection. Precision refers to the accuracy. mAP
 score was calculated as follows: Average across the number of classes 
of the true positive divided by the true positives plus false positive 
as in the following equation</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"><span class="MathJax_Preview" style=""></span><div class="MathJax_Display" style="text-align: left;"><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0023;&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mrow class=&quot;MJX-TeXAtom-OP MJX-fixedlimits&quot;&gt;&lt;mo movablelimits=&quot;false&quot;&gt;&amp;#x2211;&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0023;&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0023;&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0023;&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0023;&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: left; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 19.626em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.818em; height: 0px; font-size: 124%;"><span style="position: absolute; clip: rect(0.585em, 1015.73em, 3.9em, -999.998em); top: -2.507em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: STIXGeneral; font-style: italic;">𝑚</span><span class="mi" id="MathJax-Span-4" style="font-family: STIXGeneral; font-style: italic;">𝐴</span><span class="mi" id="MathJax-Span-5" style="font-family: STIXGeneral; font-style: italic;">𝑃<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.092em;"></span></span><span class="mo" id="MathJax-Span-6" style="font-family: STIXGeneral; padding-left: 0.316em;">=</span><span class="mfrac" id="MathJax-Span-7" style="padding-left: 0.316em;"><span style="display: inline-block; position: relative; width: 3.631em; height: 0px; margin-right: 0.137em; margin-left: 0.137em;"><span style="position: absolute; clip: rect(3.183em, 1000.4em, 4.124em, -999.998em); top: -4.657em; left: 50%; margin-left: -0.267em;"><span class="mn" id="MathJax-Span-8" style="font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.183em, 1003.45em, 4.124em, -999.998em); top: -3.313em; left: 50%; margin-left: -1.745em;"><span class="mrow" id="MathJax-Span-9"><span class="mi" id="MathJax-Span-10" style="font-family: STIXGeneral;">#</span><span class="mi" id="MathJax-Span-11" style="font-family: STIXGeneral; font-style: italic;">𝑐</span><span class="mi" id="MathJax-Span-12" style="font-family: STIXGeneral; font-style: italic;">𝑙</span><span class="mi" id="MathJax-Span-13" style="font-family: STIXGeneral; font-style: italic;">𝑎</span><span class="mi" id="MathJax-Span-14" style="font-family: STIXGeneral; font-style: italic;">𝑠</span><span class="mi" id="MathJax-Span-15" style="font-family: STIXGeneral; font-style: italic;">𝑠</span><span class="mi" id="MathJax-Span-16" style="font-family: STIXGeneral; font-style: italic;">𝑒</span><span class="mi" id="MathJax-Span-17" style="font-family: STIXGeneral; font-style: italic;">𝑠</span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(0.898em, 1003.63em, 1.212em, -999.998em); top: -1.297em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.631em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.078em;"></span></span></span></span><span class="munderover" id="MathJax-Span-18" style="padding-left: 0.181em;"><span style="display: inline-block; position: relative; width: 2.466em; height: 0px;"><span style="position: absolute; clip: rect(2.87em, 1001.21em, 4.617em, -999.998em); top: -3.985em; left: 0.585em;"><span class="texatom" id="MathJax-Span-19"><span class="mrow" id="MathJax-Span-20"><span class="mo" id="MathJax-Span-21" style="font-family: STIXSizeOneSym; vertical-align: -0.535em;">∑</span></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.362em, 1000.27em, 4.214em, -999.998em); top: -2.82em; left: 1.078em;"><span class="texatom" id="MathJax-Span-22"><span class="mrow" id="MathJax-Span-23"><span class="mn" id="MathJax-Span-24" style="font-size: 70.7%; font-family: STIXGeneral;">1</span></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.273em, 1002.42em, 4.124em, -999.998em); top: -5.195em; left: 0em;"><span class="texatom" id="MathJax-Span-25"><span class="mrow" id="MathJax-Span-26"><span class="mi" id="MathJax-Span-27" style="font-size: 70.7%; font-family: STIXGeneral;">#</span><span class="mi" id="MathJax-Span-28" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">𝑐</span><span class="mi" id="MathJax-Span-29" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">𝑙</span><span class="mi" id="MathJax-Span-30" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">𝑎</span><span class="mi" id="MathJax-Span-31" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">𝑠</span><span class="mi" id="MathJax-Span-32" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">𝑠</span><span class="mi" id="MathJax-Span-33" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">𝑒</span><span class="mi" id="MathJax-Span-34" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">𝑠</span></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-35"></span><span class="mfrac" id="MathJax-Span-36" style="padding-left: 0.181em;"><span style="display: inline-block; position: relative; width: 5.155em; height: 0px; margin-right: 0.137em; margin-left: 0.137em;"><span style="position: absolute; clip: rect(3.183em, 1001.88em, 4.124em, -999.998em); top: -4.657em; left: 50%; margin-left: -0.939em;"><span class="mrow" id="MathJax-Span-37"><span class="mi" id="MathJax-Span-38" style="font-family: STIXGeneral;">#</span><span class="mi" id="MathJax-Span-39" style="font-family: STIXGeneral; font-style: italic;">𝑇<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.137em;"></span></span><span class="mi" id="MathJax-Span-40" style="font-family: STIXGeneral; font-style: italic;">𝑃<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.092em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.183em, 1005.02em, 4.169em, -999.998em); top: -3.313em; left: 50%; margin-left: -2.507em;"><span class="mrow" id="MathJax-Span-41"><span class="mi" id="MathJax-Span-42" style="font-family: STIXGeneral;">#</span><span class="mi" id="MathJax-Span-43" style="font-family: STIXGeneral; font-style: italic;">𝑇<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.137em;"></span></span><span class="mi" id="MathJax-Span-44" style="font-family: STIXGeneral; font-style: italic;">𝑃<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.092em;"></span></span><span class="mo" id="MathJax-Span-45" style="font-family: STIXGeneral; padding-left: 0.271em;">+</span><span class="mi" id="MathJax-Span-46" style="font-family: STIXGeneral; padding-left: 0.271em;">#</span><span class="mi" id="MathJax-Span-47" style="font-family: STIXGeneral; font-style: italic;">𝐹<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.092em;"></span></span><span class="mi" id="MathJax-Span-48" style="font-family: STIXGeneral; font-style: italic;">𝑃<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.092em;"></span></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(0.898em, 1005.15em, 1.212em, -999.998em); top: -1.297em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.155em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.078em;"></span></span></span></span><span class="mo" id="MathJax-Span-49" style="font-family: STIXGeneral;">.</span></span><span style="display: inline-block; width: 0px; height: 2.511em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.608em; border-left: 0px solid; width: 0px; height: 3.836em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>m</mi><mi>A</mi><mi>P</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">#</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow></mfrac><munderover><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><mo movablelimits="false">∑</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">#</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow></munderover><mo>⁡</mo><mfrac><mrow><mi mathvariant="normal">#</mi><mi>T</mi><mi>P</mi></mrow><mrow><mi mathvariant="normal">#</mi><mi>T</mi><mi>P</mi><mo>+</mo><mi mathvariant="normal">#</mi><mi>F</mi><mi>P</mi></mrow></mfrac><mo>.</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-1">mAP = \frac{1}{\# classes} \mathop \sum \limits_{1}^{\# classes} \frac{\# TP}{\# TP + \# FP}.</script></span></div></div>
<h3 class="c-article__sub-heading u-h3" id="Sec11">Confusion matrix</h3><p>In
 addition to mAP score, we also computed a confusion matrix (CM) for 
each selected model based on the object detection script [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Confusion Matrix for Object Detection. 
                    https://github.com/svpino/tf_object_detectioncm/blob/master/confusion_matrix.py
                    
                  . Accessed 10 Mar 2019." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR27" id="ref-link-section-d65006e1732">27</a>].
 Computation of CM protocol is described below. For each detection, the 
algorithm mines all the ground-truth boxes and classes, along with the 
detected boxes, classes, and scores of Intersection over Union (IoU). 
Only detections with a score ≥ 0.5 were considered and anything under 
this threshold were excluded. For each ground-truth box, the algorithm 
creates the IoU with each detected box. A match was found if both boxes 
had an IoU ≥ 0.5. The list of matches was trimmed to remove duplicates 
(ground-truth boxes that match with more than one detection box or vice 
versa). If there are duplicates, the best match (greater IoU) was 
continually selected. The CM was updated to reflect the resultant 
matches between ground-truth and detections. A detected box was 
reflected as correct where the intersection over union (IoU) of that box
 and the corresponding ground-truth box was ≥ 0.5. The formula for 
calculating IoU is shown in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig4">4</a>. In the final step, the CM was normalized.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig.&nbsp;4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig4_HTML.png?as=webp"><img aria-describedby="figure-4-desc" src="13007_2019_475_Fig4_HTML.png" alt="figure4" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Diagram explaining intersect over union (IOU) calculation. <b>a</b> Intersection over union (IoU) formula where B<sub>1</sub>: ground truth bounding box and B<sub>2</sub>: predicted bounding box, <b>b</b> samples of calculated scores</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading u-h3" id="Sec12">Software and hardware system</h3><p>The list of hardware and software used in this study was depicted in Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab2">2</a>.
 For algorithm implementation, and data wrangling scripts, python 3.6 
was used. Then models were re-trained using the powerful library called 
TensorFlow object detection API [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Object Detection API, Tensorflow. 
                    https://github.com/tensorflow/models/tree/master/research/object_detection
                    
                  . Accessed 20 Feb 2019." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR28" id="ref-link-section-d65006e1778">28</a>] developed by Google, this library support control process unit (CPU) and GPU training and inference.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table&nbsp;2 Lists of hardware and software used in this study</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-category="article body" data-track-label="button" rel="nofollow" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec13">Results and discussion</h2><div class="c-article-section__content" id="Sec13-content"><h3 class="c-article__sub-heading u-h3" id="Sec14">Banana dataset collection and annotation</h3><p>Banana is liable to various types of pests and diseases for which symptoms occur in different parts of the plant (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab1">1</a>).
 The occurrences of these diseases depends on many factors, such as 
environment, temperature, humidity, rainfall, variety, season, 
nutrition, etc. For instance, certain diseases are localized in a 
particular country, region or continent, such as, Xanthomonas wilt of 
banana which is very specific to Africa. Therefore, reliable and 
accurate image collection at hotspots and strong labeling is very 
important. Since we are aiming for a global solution, we collected the 
image dataset of major banana diseases from different disease hotspots 
through our CGIAR network. Publicly available datasets poorly cover 
banana disease/pest symptom images, and the PlantVillage public dataset 
so far doesn’t include banana images. We collected our own datasets of 
leaves infected by specific pathogens at different infection stages and 
other infected plant parts such as entire plants, fruit bunch, cut 
fruits, pseudostem and corms etc. with the help of well-trained banana 
experts using different cameras with various resolutions (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab1">1</a>, Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig2">2</a>).
 Currently our CIAT-Bioversity, CGIAR dataset contains more than 18,000 
expert pre-screened original field images, but in this study we utilized
 only 12,600 images to create banana image data sets. Since our ultimate
 aim is to develop a mobile-assisted banana disease detection tool 
targeting banana farmers across the globe/wordwide and the scientific 
community around the world, we enriched our image library with a diverse
 collection of images from different disease hot spots (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>:
 Table S1). To build a robust model, images were captured in real field 
scenarios on banana farms. A heterogeneous background is an essential 
feature of any real field images, most of the publicly available 
datasets are images of leaves in a controlled environment and simple 
background. For this reason, we tried to create many variations while 
collecting data from the field. The more the variation in the dataset, 
the better is the generalization of the trained model. The images were 
captured with different camera devices (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>:
 Table S1) with diverse background. Furthermore, the challenging part of
 our image dataset is the background variations caused by the 
surroundings of the field, dried leaves on the floor, overlapping leaves
 from neighboring plants etc. This made our model more robust to adapt 
any changes in the real-time background.</p><p>We annotated the images 
to train our CNN by setting the images of different classes in distinct 
folders. We randomly picked 75% of images of each class and put them 
into a training set. Likewise, another 25% of images of each class were 
put into a test set. The training and the test set both contained 700 
real field images per class (700 × 18 classes = 12,600) which has made 
the data set well balanced. The categories and the number of annotated 
samples used in our system can be seen in Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab1">1</a>.
 We carried out a strong labeling approach whereby the banana experts 
confirmed the typical symptoms on each and every image of the data set, 
as a result we ended up with a total of 30,952 annotations (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab1">1</a>).
 Even though this strategy is time-consuming, we worked with three human
 experts to annotate the whole banana dataset which took almost 
4&nbsp;weeks. The tediousness of data collection and labeling had forced
 earlier studies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Dandawate Y, Kokare R. An automated approach for classification of plant diseases towards development of futuristic decision support system in Indian perspective. In: 2015 international conference on advances in computing, communications and informatics (ICACCI), IEEE; 2015. p. 794–9." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR29" id="ref-link-section-d65006e1949">29</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Mokhtar U, El Bendary N, Hassenian AE, Emary E, Mahmoud MA, Hefny H, Tolba MF. Svm-based detection of tomato leaves diseases. In: Intelligent Systems’ 2014. Springer; 2015. p. 641–52." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR30" id="ref-link-section-d65006e1952">30</a>]
 to use small datasets to train and test classifiers. The use of small 
labeled datasets is also a limiting factor in machine learning, and it 
can lead to over or underfitting [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Brahimi M, Arsenovic M, Laraba S, Sladojevic S, Boukhalfa K, Moussaoui A. Deep learning for plant diseases: detection and saliency map visualisation. In: Human and machine learning. springer; 2018. p. 93–117." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR31" id="ref-link-section-d65006e1955">31</a>]. Most of the publicly available data sets are weakly labeled and resulted in poor performance.</p><h3 class="c-article__sub-heading u-h3" id="Sec15">Loss function</h3><p>We summarized the total loss function (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM1">1</a>: Fig. S1a–f) only for the winner models (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>:
 Table S3). In general, we could observe that the accuracy increased 
while loss decreased gradually with epoch. For Corm damage images, the 
reported error was high until the 1500th iteration, then started to go 
down and after the 4000th step remained constant (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM1">1</a>: Fig. S1f), the same behavior was noticed in Pseudostem and Cut Fruits (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM1">1</a>:
 Fig. S1c, e), where after 2000 iterations the error remained constant 
until the end. For the entire plant and leaves (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM1">1</a>:
 Fig. S1a, b), although a loss was found to be below 0.3 in the last 
iteration, it suffered due to lot of variations, which was evident since
 these two models (entire plant and leaf) were found to be low accurate 
compared to other models studied (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab3">3</a>). The probable reason was clearly explained further by other performance metrics below.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table&nbsp;3 mAP metric score for different models developed from this study</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-category="article body" data-track-label="button" rel="nofollow" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading u-h3" id="Sec16">Performance metrics and validation of developed models</h3><p>In
 recent years, deep learning techniques, and in particular convolutional
 neural networks (CNNs), recurrent neural networks and long-short term 
memories (LSTMs), have shown great success in visual data recognition, 
classification, and sequence learning tasks. In the field of computer 
vision specifically, a set of CNN architectures have been emerging and 
they have proved to achieve tasks like object classification, detection 
and segmentation. In this paper, we retrained MobileNetV1, InceptionV2 
and RestNet50 architectures using transfer learning to detect the banana
 pest and diseases. In order to improve the accuracy, the diseases were 
grouped by plant parts, and a different model was trained for each plant
 part (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab1">1</a>).
 Transfer learning is a progress that has the huge potential of being 
extensively used in crop phenomics and pest and disease detection. 
Transfer learning is particularly interesting, as its improved 
performance of deep neural networks by evading intricate data mining and
 labeling efforts [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR32" id="ref-link-section-d65006e2467">32</a>].</p><p>There
 are different metrics to measure the accuracy and effectiveness in 
object detection models. In this study, we used mAP which is one of the 
widely used metrics in the literature [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Fuentes A, Yoon S, Kim S, Park D. A robust deep-learning-based detector for real-time tomato plant diseases and pests recognition. Sensors. 2017;17(9):2022." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR33" id="ref-link-section-d65006e2473">33</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Sun J, He X, Ge X, Wu X, Shen J, Song Y. Detection of key organs in tomato based on deep migration learning in a complex background. Agriculture. 2018;8(12):196." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR34" id="ref-link-section-d65006e2476">34</a>],
 especially for detection. Additionally, for each best model, a 
confusion matrix was generated. Earlier studies on detection revealed 
that the mAP score had become the accepted and standard way in 
competitions such as PASCAL VOC [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Everingham M, Eslami SA, Van Gool L, Williams CK, Winn J, Zisserman A. The pascal visual object classes challenge: a retrospective. Int J Comput Vision. 2015;111(1):98–136." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR35" id="ref-link-section-d65006e2479">35</a>], ImageNet, and COCO datasets. More detail results are described below.</p><h3 class="c-article__sub-heading u-h3" id="Sec17">mAP score</h3><p>The accuracy of the models based on mAP score is presented in Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab3">3</a>.
 For the entire plant, leaves, pseudostem and fruit bunch models 
performed better in Faster R-CNN (faster regions with convolutional 
neural network) ResNet50 than others tested, which achieved an mAP score
 of 73%, 70%, 99%, and 97%, respectively. For cut fruits and corm, 
Faster R-CNN InceptionV2 worked better with the mAP accuracy of 95% and 
98%, respectively. Fuentes et al. 2017 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Fuentes A, Yoon S, Kim S, Park D. A robust deep-learning-based detector for real-time tomato plant diseases and pests recognition. Sensors. 2017;17(9):2022." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR33" id="ref-link-section-d65006e2493">33</a>],
 used three CNN-based systems (Faster R-CNN, R-FCN and SSD) which 
performed object localization and disease diagnosis processes 
simultaneously and their system achieved more than 86.0% mean average 
precision on annotated tomato leaf images. In this present study, 
ResNet50 and InceptionV2 models have almost similar performance in all 
the cases compared to MobileNetV1 (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab3">3</a>). In generalized recognition, Faster R-CNN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Zhang L, Lin L, Liang X, He K. Is faster r-cnn doing well for pedestrian detection? In: European conference on computer vision. Springer; 2016. p. 443–57." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR36" id="ref-link-section-d65006e2499">36</a>], models have been widely used and have achieved good results.</p><p>In
 this research, to achieve greater accuracy, we considered the 
complexity of the model as an important factor to select the best 
architectures for the training set. This characteristic could be 
measured by counting the total amount of learnable parameters or the 
number of operations. As a result, we selected three architectures 
(Inception, ResNet, MobileNet). Since complexity is associated with the 
capacity of the model to extract more features from the images, it is 
expected inceptionV2 to be the most accurate among three architectures. 
However, it is always a trade-off between complex and simple 
architecture especially when you specifically think about mobile 
application.</p><p>We also noticed higher accuracy (more than 95%) for 
pseudostem, fruit bunch and corm compared to entire plant (73%) and leaf
 models (70%). This was expected in the entire plant model due to 
background noise in field environment, multiple classes (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig5">5</a>b)
 in the single image and wide angle. Wide-angle images are often more 
complex due to the substantial overlap of multiple leaves and symptoms 
are scattered in different leaves. In the case of banana it is much more
 complex because of the specific plant morphology and large leaf size. 
We also observed that during the labeling process, a single class per 
image was working as a ground-truth for the model (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig5">5</a>a, b). But in real life scenario, one image could have multiple classes as seen in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig3">3</a>. Developed entire plant and leaf model from this study is finding multiple classes in single image (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig5">5</a>b)
 which is more practical and useful in the real-time field application, 
since our ground-truth data is solely grouped on single class 
(Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig5">5</a>a)
 which brought the mAP score lower than expected and it was the main 
cause. It was a little surprise for leaf model where we expect more than
 90% accuracy since it is not very wide angle like the entire plant 
class. But these wide-angle images from field environment expected to 
have more background noises. To confirm these results in leaf model, we 
did an additional test to select images containing only one class per 
image, which reflected on higher accuracy more than 70% (Additional file
 <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>:
 Table S4). Moreover, this accuracy was further increased (more than 
90%) when the new image dataset contain only one focused leaf per image 
(Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM2">2</a>:
 Table S4). From these results, it is clear the complexity of the banana
 leaf morphology, disease symptoms, multiple classes in single image, 
field background noises etc. Unlike other crops such as rice, wheat, and
 cassava, banana leaves are very big, that makes the angle wider than 
other crops which increases the complexity in real-time field images 
(Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig5">5</a>).
 In the case of pseudostem, fruit bunch, cut fruits and corm field 
images used in this study have more focused images towards the object 
with less background variation and single class per image which reduced 
the complexity and improved mAP score (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Tab3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig.&nbsp;5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig5_HTML.jpg?as=webp"><img aria-describedby="figure-5-desc" src="13007_2019_475_Fig5_HTML.jpg" alt="figure5" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Comparison between ground-truth labeled image and the predicted classes by model. <b>a</b> Ground-truth labeled image of FWB, <b>b</b> image after predicted by a model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading u-h3" id="Sec18">Confusion matrix</h3><p>In
 the field of deep learning, specifically the problem of statistical 
classification, the confusion matrix, also known as an error matrix, is a
 specific table layout that allows visualization of the performance of 
an algorithm. It considers different metrics: the true positives (TP), 
true negatives (TN), false positives (FP) and false negatives (FN) etc. 
Based on the results obtained on the test dataset, we generated a 
confusion matrix for each of the best architectures (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig6">6</a>a–f).
 Each confusion matrix gave us a accuracy per disease (classes) and 
quantitative representation of the classes in which the model is 
misclassified or confused (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig6">6</a>a–f).
 Due to the complexity of the patterns shown in each class from 
different plant parts, the system tends to be confused on several 
classes that results in lower performance. Based on the results, we can 
visually evaluate the performance of the classifier and determine which 
classes and features are more prone to confusion. If the number of 
misclassifications between two particular classes becomes high, it 
indicates that we need to collect more data on those classes to properly
 train the convolutional architecture so that it can differentiate 
between those two classes. For this purpose, we also generated confusion
 matrix on our validation set for each best CNN architecture. 
Furthermore, it helps us to identify a future solution in order to avoid
 those inter-class confusions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig.&nbsp;6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig6_HTML.png?as=webp"><img aria-describedby="figure-6-desc" src="13007_2019_475_Fig6_HTML.png" alt="figure6" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Confusion matrix for the best models identified in this study. <b>a</b> Entire plant—ResNet, <b>b</b> leaves—ResNet, <b>c</b> pseudostem—ResNet, <b>d</b> fruit bunch—ResNet, <b>e</b> cut fruits—inception, <b>f</b> corm—inception</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>On comparing among the models, leaves produced a lot of confusion and
 low accuracy (57%) especially yellow sigatoka leaf spot classes 
(Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig6">6</a>b),
 this was expected since YS and BS commonly produce similar symptoms in 
advanced stages, but early stage symptoms are unique (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM1">1</a>:
 Fig. S2). It is worth mentioning that yellow leaf spot disease 
appearing more frequently in Asia and Latin America and black leaf spot 
in Africa, and their treatment and disease controlling measures are 
almost similar. To handle these issues, we are currently collecting and 
labeling images of early stage symptoms for improving the accuracy of 
the model, and the ability to generalize. Because the dataset is not big
 enough, it was not considered in this study. We also observed medium 
prediction accuracy in the dried/old age leaf classes (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig6">6</a>b),
 it was obvious that, advanced stages of all leaf diseases will turn to 
be like dried/old age leaves and we expected this results. So early and 
mid-stage leaf symptoms are very important to detect the diseases with 
more accuracy. As we expected, the entire plant, corm, pseudostem, fruit
 bunch and cut fruits models, we had not found any accuracy or 
misclassification problems (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#Fig6">6</a>a, c–f), which was ranged between 90 and 100% accuracy.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusions and future directions</h2><div class="c-article-section__content" id="Sec19-content"><p>Many
 computer visioned approaches for automated crop disease detection and 
classification have been reported, but still, a detailed exploration of 
real-time pest and diseases recognition is lagging. In this paper, a 
novel method of using deep transfer learning method was explored in 
order to automatically detect banana pest and disease symptoms on 
different parts of the banana plants using real-time field images. This 
system introduces a practical and applicable solution for detecting the 
class and location of diseases in banana plants, which represents a main
 comparable difference with other methods for plant diseases 
classification. The developed model was able to detect the difference 
between healthy and infected plant parts for different banana diseases. 
All images used in this study are available upon formal request through 
PestDisPlace (<a href="https://pestdisplace.org/">https://pestdisplace.org/</a>) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Cuellar W, Mwanzia L, Lourido D, Garcia C, Martínez A, Cruz P, Pino L, Tohme J. PestDisPlace: monitoring the distribution of pests and diseases, version 2.0. International Center for Tropical Agriculture (CIAT); 2018." href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#ref-CR37" id="ref-link-section-d65006e2645">37</a>].
 It consists of more than 18,000 original expertly pre-screened banana 
images collected on real farmer’s field in Africa, Latin America and 
South India and was extended to more than 30,952 annotations. The 
experimental results achieved accuracy between 70 and 99%, of the 
different models tested. The robust models developed from this research 
will be more useful to develop the decision-support system to help early
 identification of pest and diseases and their management. Models 
developed in this study are currently utilized to develop a banana 
mobile app which is currently being tested by collaborative partners in 
Benin, DR Congo, Uganda, Colombia, and India (Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z#MOESM1">1</a>: Fig. S3). The developed model system from this study is easily transferable to other CGIAR mandatory crops.</p><p>Future
 work will comprise the development of a broad structure consisting of 
server side machinery containing a trained model and an application for 
smartphone devices with features such as displaying recognized diseases 
in other CGIAR mandatory crop such as Brachiaria, common bean, cassava, 
potato and sweet potato. Additionally, future work will involve 
disseminating the usage of the model by training it for banana disease 
recognition on wider applications, merging aerial images of banana 
growing regions captured by drones and convolution neural networks for 
instant segmentation of multiple diseases. By extending this research, 
we are hoping to achieve a valuable impact on sustainable development 
and strengthen banana value chains.</p></div></div></section>
                        <section aria-labelledby="availability-of-data-and-materials"><div class="c-article-section" id="availability-of-data-and-materials-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="availability-of-data-and-materials">Availability of data and materials</h2><div class="c-article-section__content" id="availability-of-data-and-materials-content">
              
              <p>The remotely sensed and field sampling data used in 
this study is available from the corresponding author upon reasonable 
request.</p>
            </div></div></section><section aria-labelledby="abbreviations"><div class="c-article-section" id="abbreviations-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="abbreviations">Abbreviations</h2><div class="c-article-section__content" id="abbreviations-content"><dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>AI:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>artificial intelligence</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>API:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Application Programming Interface</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>BBTV:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>banana bunchy top virus</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>BCW:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>banana corm weevil</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>BS:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>black sigatoka</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>BXW:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Xanthomonas wilt of banana</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>CGIAR:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Consultative Group on International Agricultural Research</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>CIAT:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>International Center for Tropical Agriculture</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>CNN:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>convolutional neural network</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>COCO:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>common objects in context</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>CM:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>confusion matrix</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>CPU:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>control process unit</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>DCNN:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>deep convolutional neural networks</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>DOL:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>dried/old leaves</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>DTL:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>deep transfer learning</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>Faster R-CNN:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>faster regions with convolutional neural network</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>FN:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>false negatives</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>FP:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>false positives</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>FWB:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Fusarium wilt of banana</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>GPU:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>graphics process unit</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>HP:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>healthy plant</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>IoT:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Internet of Things</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>IoU:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>intersection over union</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>LSTM:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>long-short term memories (LSTMs)</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>mAP:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>mean average precision</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>SRS:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>single random sampling</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>SSD:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>single shot detector</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>TN:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>true negatives</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>TP:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>true positives</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>Ttr:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>training</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>Tt:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>testing</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>Tv:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>validation</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>YS:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>yellow sigatoka</p>
                  </dd></dl></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">1.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR1">FAO.
 Banana market review and banana statistics 2012–2013. Market and policy
 analyses of raw materials, horticulture and tropical (RAMHOT) Products 
Team. Rome; 2014.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">2.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR2">Lescot
 T. World plantain and banana production systems. In: Proceedings XX 
international meeting ACORBAT: 9–13 September 2013; Fortaleza; 2013. p. 
26–34.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><span class="c-article-references__counter">3.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR3">Abele
 S, Twine E, Legg C. Food security in eastern Africa and the great 
lakes. Crop Crisis Control Project final report. Ibadan: Int Instit Trop
 Agric; 2007.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Food%20security%20in%20eastern%20Africa%20and%20the%20great%20lakes.%20Crop%20Crisis%20Control%20Project%20final%20report&amp;publication_year=2007&amp;author=Abele%2CS&amp;author=Twine%2CE&amp;author=Legg%2CC">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">4.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR4">Nagayets O. Small farms: current status and key trends. In: The future of small farms; 2005. p. 355.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">5.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR5">Blomme
 G, Dita M, Jacobsen KS, Perez Vicente L, Molina A, Ocimati W, Poussier 
S, Prior P. Bacterial diseases of bananas and enset: current state of 
knowledge and integrated approaches toward sustainable management. Front
 Plant Sci. 2017;8:1290.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.3389%2Ffpls.2017.01290" aria-label="View reference 5">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bacterial%20diseases%20of%20bananas%20and%20enset%3A%20current%20state%20of%20knowledge%20and%20integrated%20approaches%20toward%20sustainable%20management&amp;journal=Front%20Plant%20Sci&amp;volume=8&amp;publication_year=2017&amp;author=Blomme%2CG&amp;author=Dita%2CM&amp;author=Jacobsen%2CKS&amp;author=Perez%20Vicente%2CL&amp;author=Molina%2CA&amp;author=Ocimati%2CW&amp;author=Poussier%2CS&amp;author=Prior%2CP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">6.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR6">Hillnhuetter
 C, Mahlein AK. Early detection and localisation of sugar beet diseases:
 new approaches. Gesunde Pflanzen. 2008;60(4):143–9.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs10343-008-0196-0" aria-label="View reference 6">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Early%20detection%20and%20localisation%20of%20sugar%20beet%20diseases%3A%20new%20approaches&amp;journal=Gesunde%20Pflanzen.&amp;volume=60&amp;issue=4&amp;pages=143-149&amp;publication_year=2008&amp;author=Hillnhuetter%2CC&amp;author=Mahlein%2CAK">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">7.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR7">Camargo
 A, Smith J. An image-processing based algorithm to automatically 
identify plant disease visual symptoms. Biosyst Eng. 2009;102(1):9–21.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.biosystemseng.2008.09.030" aria-label="View reference 7">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20image-processing%20based%20algorithm%20to%20automatically%20identify%20plant%20disease%20visual%20symptoms&amp;journal=Biosyst%20Eng&amp;volume=102&amp;issue=1&amp;pages=9-21&amp;publication_year=2009&amp;author=Camargo%2CA&amp;author=Smith%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">8.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR8">Mohanty SP, Hughes DP, Salathe M. Using deep learning for image-based plant disease detection. Front Plant Sci. 2016;7:1419.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.3389%2Ffpls.2016.01419" aria-label="View reference 8">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20deep%20learning%20for%20image-based%20plant%20disease%20detection&amp;journal=Front%20Plant%20Sci&amp;volume=7&amp;publication_year=2016&amp;author=Mohanty%2CSP&amp;author=Hughes%2CDP&amp;author=Salathe%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><span class="c-article-references__counter">9.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR9">Intelligence G. The mobile economy Africa 2016. London: GSMA; 2016.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20mobile%20economy%20Africa%202016&amp;publication_year=2016&amp;author=Intelligence%2CG">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">10.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR10">Kamilaris A, Prenafeta-Boldu FX. Deep learning in agriculture: a survey. Comput Elect Agric. 2018;147:70–90.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.compag.2018.02.016" aria-label="View reference 10">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20in%20agriculture%3A%20a%20survey&amp;journal=Comput%20Elect%20Agric&amp;volume=147&amp;pages=70-90&amp;publication_year=2018&amp;author=Kamilaris%2CA&amp;author=Prenafeta-Boldu%2CFX">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">11.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR11">Ramcharan
 A, Baranowski K, McCloskey P, Ahmed B, Legg J, Hughes DP. Deep learning
 for image-based cassava disease detection. Front Plant Sci. 
2017;8:1852.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.3389%2Ffpls.2017.01852" aria-label="View reference 11">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20for%20image-based%20cassava%20disease%20detection&amp;journal=Front%20Plant%20Sci&amp;volume=8&amp;publication_year=2017&amp;author=Ramcharan%2CA&amp;author=Baranowski%2CK&amp;author=McCloskey%2CP&amp;author=Ahmed%2CB&amp;author=Legg%2CJ&amp;author=Hughes%2CDP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><span class="c-article-references__counter">12.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR12">Siricharoen
 P, Scotney B, Morrow P, Parr G. A lightweight mobile system for crop 
disease diagnosis. International conference on image analysis and 
recognition. Berlin: Springer; 2016. p. 783–91.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=International%20conference%20on%20image%20analysis%20and%20recognition&amp;pages=783-791&amp;publication_year=2016&amp;author=Siricharoen%2CP&amp;author=Scotney%2CB&amp;author=Morrow%2CP&amp;author=Parr%2CG">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">13.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR13">Wiesner-Hanks
 T, Stewart EL, Kaczmar N, DeChant C, Wu H, Nelson RJ, Lipson H, Gore 
MA. Image set for deep learning: field images of maize annotated with 
disease symptoms. BMC Res Notes. 2018;11(1):440.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1186%2Fs13104-018-3548-6" aria-label="View reference 13">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20set%20for%20deep%20learning%3A%20field%20images%20of%20maize%20annotated%20with%20disease%20symptoms&amp;journal=BMC%20Res%20Notes.&amp;volume=11&amp;issue=1&amp;publication_year=2018&amp;author=Wiesner-Hanks%2CT&amp;author=Stewart%2CEL&amp;author=Kaczmar%2CN&amp;author=DeChant%2CC&amp;author=Wu%2CH&amp;author=Nelson%2CRJ&amp;author=Lipson%2CH&amp;author=Gore%2CMA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><span class="c-article-references__counter">14.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR14">Mwebaze
 E, Owomugisha G. Machine learning for plant disease incidence and 
severity measurements from leaf images.&nbsp;2016 15th IEEE 
international conference on machine learning and applications (ICMLA). 
New York: IEEE; 2016. p. 158–63.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=2016%2015th%20IEEE%20international%20conference%20on%20machine%20learning%20and%20applications%20%28ICMLA%29&amp;pages=158-163&amp;publication_year=2016&amp;author=Mwebaze%2CE&amp;author=Owomugisha%2CG">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">15.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR15">Hughes
 D, Salathe M. An open access repository of images on plant health to 
enable the development of mobile disease diagnostics. arXiv preprint <a href="http://arxiv.org/abs/1511.08060">arXiv:1511.08060</a>; 2015.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">16.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR16">LabelImg Software. <a href="https://github.com/tzutalin/labelImg/">https://github.com/tzutalin/labelImg/</a>. Accessed 1 Feb 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">17.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR17">ImageNet Data Set. <a href="http://www.image-net.org/">http://www.image-net.org/</a>. Accessed 12 Mar 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">18.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR18">He
 K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition.
 In: Proceedings of the IEEE conference on computer vision and pattern 
recognition; 2016. p. 770–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">19.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR19">Ioffe
 S, Szegedy C. Batch normalization: Accelerating deep network training 
by reducing internal covariate shift. arXiv preprint <a href="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</a>. 2015.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">20.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR20">Howard
 AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam 
H. Mobilenets: efficient convolutional neural networks for mobile vision
 applications. arXiv preprint <a href="http://arxiv.org/abs/1704.04861">arXiv:1704.04861</a>. 2017.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">21.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR21">Huang
 J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A, Fischer I, Wojna Z, 
Song Y, Guadarrama S. Speed/accuracy trade-offs for modern convolutional
 object detectors. In: Proceedings of the IEEE conference on computer 
vision and pattern recognition; 2017. p. 7310–1.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">22.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR22">TensorFlow Python API. <a href="https://www.tensorflow.org/api_docs/python">https://www.tensorflow.org/api_docs/python</a>. Accessed 10 Feb 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">23.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR23">COCO Data Set. <a href="http://cocodataset.org/">http://cocodataset.org/</a>. Accessed 15 Feb 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">24.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR24">Reitermanova Z. Data splitting. In: WDS’10 proceedings of contributed papers, Part I, vol 10; 2010. p. 31–6.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">25.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR25">Liu
 W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC. Ssd: Single 
shot multibox detector. In: European conference on computer vision. 
Springer; 2016. p. 21–37.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">26.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR26">Object Detection API Loss Functions Implementation, Tensorflow. <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/core/losses.py">https://github.com/tensorflow/models/blob/master/research/object_detection/core/losses.py</a>. Accessed 5 Mar 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">27.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR27">Confusion Matrix for Object Detection. <a href="https://github.com/svpino/tf_object_detectioncm/blob/master/confusion_matrix.py">https://github.com/svpino/tf_object_detectioncm/blob/master/confusion_matrix.py</a>. Accessed 10 Mar 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">28.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR28">Object Detection API, Tensorflow. <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</a>. Accessed 20 Feb 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">29.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR29">Dandawate
 Y, Kokare R. An automated approach for classification of plant diseases
 towards development of futuristic decision support system in Indian 
perspective. In: 2015 international conference on advances in computing,
 communications and informatics (ICACCI), IEEE; 2015. p. 794–9.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">30.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR30">Mokhtar
 U, El Bendary N, Hassenian AE, Emary E, Mahmoud MA, Hefny H, Tolba MF. 
Svm-based detection of tomato leaves diseases. In: Intelligent Systems’ 
2014. Springer; 2015. p. 641–52.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><span class="c-article-references__counter">31.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR31">Brahimi
 M, Arsenovic M, Laraba S, Sladojevic S, Boukhalfa K, Moussaoui A. Deep 
learning for plant diseases: detection and saliency map visualisation. 
In: Human and machine learning. springer; 2018. p. 93–117.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20and%20Machine%20Learning&amp;pages=93-117&amp;publication_year=2018&amp;author=Brahimi%2CMohammed&amp;author=Arsenovic%2CMarko&amp;author=Laraba%2CSohaib&amp;author=Sladojevic%2CSrdjan&amp;author=Boukhalfa%2CKamel&amp;author=Moussaoui%2CAbdelouhab">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">32.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR32">Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2009.191" aria-label="View reference 32">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20transfer%20learning&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=22&amp;issue=10&amp;pages=1345-1359&amp;publication_year=2010&amp;author=Pan%2CSJ&amp;author=Yang%2CQ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">33.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR33">Fuentes
 A, Yoon S, Kim S, Park D. A robust deep-learning-based detector for 
real-time tomato plant diseases and pests recognition. Sensors. 
2017;17(9):2022.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.3390%2Fs17092022" aria-label="View reference 33">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20robust%20deep-learning-based%20detector%20for%20real-time%20tomato%20plant%20diseases%20and%20pests%20recognition&amp;journal=Sensors&amp;volume=17&amp;issue=9&amp;publication_year=2017&amp;author=Fuentes%2CA&amp;author=Yoon%2CS&amp;author=Kim%2CS&amp;author=Park%2CD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">34.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR34">Sun
 J, He X, Ge X, Wu X, Shen J, Song Y. Detection of key organs in tomato 
based on deep migration learning in a complex background. Agriculture. 
2018;8(12):196.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.3390%2Fagriculture8120196" aria-label="View reference 34">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Detection%20of%20key%20organs%20in%20tomato%20based%20on%20deep%20migration%20learning%20in%20a%20complex%20background&amp;journal=Agriculture&amp;volume=8&amp;issue=12&amp;publication_year=2018&amp;author=Sun%2CJ&amp;author=He%2CX&amp;author=Ge%2CX&amp;author=Wu%2CX&amp;author=Shen%2CJ&amp;author=Song%2CY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">35.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR35">Everingham
 M, Eslami SA, Van Gool L, Williams CK, Winn J, Zisserman A. The pascal 
visual object classes challenge: a retrospective. Int J Comput Vision. 
2015;111(1):98–136.</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-014-0733-5" aria-label="View reference 35">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20pascal%20visual%20object%20classes%20challenge%3A%20a%20retrospective&amp;journal=Int%20J%20Comput%20Vision.&amp;volume=111&amp;issue=1&amp;pages=98-136&amp;publication_year=2015&amp;author=Everingham%2CM&amp;author=Eslami%2CSA&amp;author=Gool%2CL&amp;author=Williams%2CCK&amp;author=Winn%2CJ&amp;author=Zisserman%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">36.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR36">Zhang
 L, Lin L, Liang X, He K. Is faster r-cnn doing well for pedestrian 
detection? In: European conference on computer vision. Springer; 2016. 
p. 443–57.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><span class="c-article-references__counter">37.</span><p class="c-article-references__text" itemprop="headline" id="ref-CR37">Cuellar
 W, Mwanzia L, Lourido D, Garcia C, Martínez A, Cruz P, Pino L, Tohme J.
 PestDisPlace: monitoring the distribution of pests and diseases, 
version 2.0. International Center for Tropical Agriculture (CIAT); 2018.</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-category="article body" data-track-label="link" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The
 authors would like to thank the International Center for Tropical 
Agriculture (CIAT) IT unit for providing facilities and logistics 
support. Joe Tohme, Manabu Ishitani and Wilmer Cuellar from CIAT for 
guidance and support to do the research. The authors would also like to 
acknowledge Milton Valencia, Jorge Casas, Maria Montoya, Crysthian 
Delgado and Frank Montenegro for their help in image annotation and data
 collection. Thanks to Jules Ntamwira, Jean-Pierre Mafuta and Aman 
Omondi of Bioversity International, Africa and Deo Kantungeko of IITA, 
Burundifor their immense support to collect smartphone images. The 
farmers of Tamil Nadu Banana grower’s federation, Trichy and planters of
 Tamil Nadu Hill Banana Growers Federation, Lower Palani Hills, Tamil 
Nadu India are also acknowledged for helping to collect data images. The
 authors also thank two anonymous reviewers for their detailed 
suggestions for improving the manuscript. As well as Angela Fernando, 
CIAT and Escalin Fernando, India for formatting and technical editing.</p><h3 class="c-article__sub-heading u-h3">Funding</h3><p>Funding
 for field smartphone image collection was provided by Bioversity 
International in the framework of the RTB-CC3.1 cluster and by the CIAT 
Agrobiodiversity Research Area to carry out the image processing work 
&amp; preliminary app development (AGBIO1). This study was supported by 
the CGIAR Research Program on Roots, Tubers and Bananas (RTB). We thank 
the RTB Program Management Unit that supported this study and the CGIAR 
Fund Donors who support RTB (<a href="http://www.cgiar.org/who-we-are/cgiar-fund/fund-donors-2">www.cgiar.org/who-we-are/cgiar-fund/fund-donors-2</a>).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article-author-information__subtitle u-h3" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><h4 class="c-article-author-affiliation__address u-h3">International Center for Tropical Agriculture (CIAT), A.A. 6713, Cali, Colombia</h4><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Michael Gomez Selvaraj</li><li class="c-article-author-affiliation__authors-item">&nbsp;&amp;&nbsp;Alejandro Vergara</li></ul></li><li id="Aff2"><h4 class="c-article-author-affiliation__address u-h3">Department of Soil and Crop Sciences, Texas A&amp;M University, College Station, TX, USA</h4><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Henry Ruiz</li></ul></li><li id="Aff3"><h4 class="c-article-author-affiliation__address u-h3">Bioversity International, Bukavu, South Kivu Province, Democratic Republic of Congo</h4><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Nancy Safari</li></ul></li><li id="Aff4"><h4 class="c-article-author-affiliation__address u-h3">Department
 of Biotechnology, Imayam Institute of Agriculture and Technology 
(IIAT), Affiliated to Tamil Nadu Agricultural University (TNAU), 
Tiruchirappalli, Tamil Nadu, India</h4><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Sivalingam Elayabalan</li></ul></li><li id="Aff5"><h4 class="c-article-author-affiliation__address u-h3">Bioversity International, Kampala, Uganda</h4><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Walter Ocimati</li></ul></li><li id="Aff6"><h4 class="c-article-author-affiliation__address u-h3">Bioversity International, c/o ILRI, Addis Ababa, Ethiopia</h4><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Guy Blomme</li></ul></li></ol><div class="js-hide u-hide-print" data-test="author-info"><h3 class="c-article-author-information__subtitle u-h3">Authors</h3><ol class="c-article-author-authors-search"><li id="auth-1"><h3 class="c-article-author-authors-search__title u-h3 js-search-name">Search for Michael Gomez Selvaraj in:</h3><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michael%20Gomez+Selvaraj">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michael%20Gomez+Selvaraj%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-2"><h3 class="c-article-author-authors-search__title u-h3 js-search-name">Search for Alejandro Vergara in:</h3><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Alejandro+Vergara">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Alejandro+Vergara%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-3"><h3 class="c-article-author-authors-search__title u-h3 js-search-name">Search for Henry Ruiz in:</h3><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Henry+Ruiz">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Henry+Ruiz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-4"><h3 class="c-article-author-authors-search__title u-h3 js-search-name">Search for Nancy Safari in:</h3><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nancy+Safari">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nancy+Safari%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-5"><h3 class="c-article-author-authors-search__title u-h3 js-search-name">Search for Sivalingam Elayabalan in:</h3><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sivalingam+Elayabalan">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sivalingam+Elayabalan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-6"><h3 class="c-article-author-authors-search__title u-h3 js-search-name">Search for Walter Ocimati in:</h3><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Walter+Ocimati">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Walter+Ocimati%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-7"><h3 class="c-article-author-authors-search__title u-h3 js-search-name">Search for Guy Blomme in:</h3><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guy+Blomme">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guy+Blomme%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li></ol></div><h3 class="c-article-author-information__subtitle u-h3" id="contributions">Contributions</h3><p>MGS,
 HR and AV designed the study, performed the experiments and are the 
main contributing authors of the paper. HR, AV, and MGS carried out data
 annotations, trained algorithms and analyzed the data. GB, SE, NS and 
WO collected over 17,000 images of disease and pest symptoms/damage, 
confirmed the symptoms and pre-screened all the images collected in 
Africa, Malaysia and India. MGS written the paper.&nbsp;All authors read
 and approved the final manuscript.</p><h3 class="c-article-author-information__subtitle u-h3" id="corresponding-author">Corresponding author</h3><p>Correspondence to
                <a id="corresp-c1" rel="nofollow" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/email/correspondent/c1/new">Michael Gomez Selvaraj</a>.</p></div></div></section><section aria-labelledby="ethics"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading u-h3">Ethics approval and consent to participate</h3>
                <p>Not applicable.</p>
              
              
                <h3 class="c-article__sub-heading u-h3">Consent for publication</h3>
                <p>All authors agreed to publish this manuscript.</p>
              
              
                <h3 class="c-article__sub-heading u-h3">Competing interests</h3>
                <p>The authors declare that they have no competing interests.</p>
              
            </div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading u-h3">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec20">Additional files</h2><div class="c-article-section__content" id="Sec20-content"><div data-test="supplementary-info"><div id="qa-widgetContainer" data-test="figshare-container"></div>
                  
                  
                <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-category="article body" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_MOESM1_ESM.pptx" data-supp-info-image="">13007_2019_475_MOESM1_ESM.pptx</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><a href="https://doi.org/10.1186/s13007-019-0475-z">
                                <b>Additional file 1: Figure S1.</b>
                              </a> Loss function curve for the winner models. <b>a</b> Entire plant—ResNet, <b>b</b> Leaves—ResNet, <b>c</b> Pseudostem—ResNet, <b>d</b> Fuit bunch—ResNet, <b>e</b> Cut fruits—Inception, <b>f</b> Corm—Inception. <b>Fig. S2.</b> Early and late stage symptoms of banana leafspots. <b>a</b> Black sigatoka (BS) late stage, <b>b</b> Yellow sigatoka (YS) late stage, <b>c</b> Black sigatoka (BS) early stage, <b>d</b> Yellow sigatoka (YS) early stage. <b>Fig. S3.</b> Developed mobile application for Banana disease and pest detection. <b>a</b> Initial screen, <b>b</b> Image taking and Scan, <b>c</b> Diagnostic screen, <b>d</b> Recommendations and management.</div></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-category="article body" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_MOESM2_ESM.docx" data-supp-info-image="">13007_2019_475_MOESM2_ESM.docx</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><a href="https://doi.org/10.1186/s13007-019-0475-z">
                                <b>Additional file 2: Table S1.</b>
                              </a> Overview of banana data set collections, locations and image acquisition. <b>Table S2.</b> Description of major banana diseases and pest symptoms with their control measures. <b>Table S3.</b> Winner architecture for the models developed in this study. <b>Table S4.</b> mAP score metrics of leaf classes before and after segmentation.</div></div></div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><div class="c-article-license">
                <p><b>Open Access</b> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<a href="http://creativecommons.org/licenses/by/4.0/" rel="license" itemprop="license">http://creativecommons.org/licenses/by/4.0/</a>),
 which permits unrestricted use, distribution, and reproduction in any 
medium, provided you give appropriate credit to the original author(s) 
and the source, provide a link to the Creative Commons license, and 
indicate if changes were made. The Creative Commons Public Domain 
Dedication waiver (<a href="http://creativecommons.org/publicdomain/zero/1.0/" rel="license" itemprop="license">http://creativecommons.org/publicdomain/zero/1.0/</a>) applies to the data made available in this article, unless otherwise stated.</p>
              </div><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-category="article body" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?imprint=Nature&amp;oa=CC%20BY%20%2B%20CC0&amp;title=AI-powered%20banana%20diseases%20and%20pest%20detection&amp;author=Michael%20Gomez%20Selvaraj%20et%20al&amp;contentID=10.1186%2Fs13007-019-0475-z&amp;publication=Plant%20Methods&amp;publicationDate=2019-08-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1186/s13007-019-0475-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1186/s13007-019-0475-z" data-track="click" data-track-action="Click Crossmark" data-track-category="article body" data-track-label="link" data-test="crossmark"><img alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57" height="81"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Selvaraj, M.G., Vergara, A., Ruiz, H. <i>et al.</i> AI-powered banana diseases and pest detection.
                    <i>Plant Methods</i> <b>15, </b>92 (2019)  doi:10.1186/s13007-019-0475-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-category="article body" data-track-label="link" href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><h4 class="u-h4">Received</h4><p class="c-bibliographic-information__value"><time datetime="2019-05-03">03 May 2019</time></p></li><li class="c-bibliographic-information__list-item"><h4 class="u-h4">Accepted</h4><p class="c-bibliographic-information__value"><time datetime="2019-07-30">30 July 2019</time></p></li><li class="c-bibliographic-information__list-item"><h4 class="u-h4">Published</h4><p class="c-bibliographic-information__value"><time datetime="2019-08-12">12 August 2019</time></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><h4 class="u-h4"><abbr title="Digital Object Identifier">DOI</abbr></h4><p class="c-bibliographic-information__value"><a href="https://doi.org/10.1186/s13007-019-0475-z" data-track="click" data-track-action="view doi" data-track-category="article body" data-track-label="link" itemprop="sameAs">https://doi.org/10.1186/s13007-019-0475-z</a></p></li></ul><div data-component="share-box"><div class="c-article-share-box"><h3 class="c-article-share-box__title u-h3">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" id="get-share-url" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="get shareable link">Get shareable link</button>			<div class="js-no-share-url-container u-display-none" aria-hidden="true"><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div> 			<div class="js-share-url-container u-display-none" aria-hidden="true"><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="select share url"> 		</p><button class="js-copy-share-url c-article-share-box__button--link-like" id="copy-share-url" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="copy share url">Copy to clipboard</button><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">Provided by the Springer Nature SharedIt content-sharing initiative</p></div> 		</div></div><h3 class="c-article__sub-heading u-h3">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Artificial intelligence</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Banana</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Deep learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Disease detection</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Transfer learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Convolutional neural networks</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Mobile app</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>

                        


                    </article>
                </main>

                <div class="c-page-layout__side u-text-sm">
                    <aside>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://plantmethods.biomedcentral.com/track/pdf/10.1186/s13007-019-0475-z" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-category="article body" data-track-label="link">
            <span>Download PDF</span>
            <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"></use></svg>
        </a>
    </div>
    

                        

                        
        
    

                        <div class="c-reading-companion">
                            <div class="c-reading-companion__sticky c-reading-companion__sticky--stuck" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="width: 243px; top: 10px;"><ul class="c-reading-companion__tabs" role="tablist" style="max-width: 243px;"><li role="presentation"><button data-tab-target="sections" role="tab" id="tab-sections" aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active">Sections</button></li><li role="presentation"><button data-tab-target="figures" role="tab" id="tab-figures" aria-controls="tabpanel-figures" aria-selected="false" tabindex="-1" class="c-reading-companion__tab">Figures</button></li><li role="presentation"><button data-tab-target="references" role="tab" id="tab-references" aria-controls="tabpanel-references" aria-selected="false" tabindex="-1" class="c-reading-companion__tab">References</button></li></ul>
                                <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" aria-labelledby="tab-sections" tabindex="0"><div class="c-reading-companion__scroll-pane" style="max-height: 513px;"><ul class="c-reading-companion__sections-list"><li id="rc-sec-Abs1" class="c-reading-companion__section-item"><a href="#Abs1" data-track="click" data-track-action="section anchor" data-track-label="link:Abstract">Abstract</a></li><li id="rc-sec-Sec1" class="c-reading-companion__section-item"><a href="#Sec1" data-track="click" data-track-action="section anchor" data-track-label="link:Background">Background</a></li><li id="rc-sec-Sec2" class="c-reading-companion__section-item c-reading-companion__section-item--active"><a href="#Sec2" data-track="click" data-track-action="section anchor" data-track-label="link:Materials and methods">Materials and methods</a></li><li id="rc-sec-Sec13" class="c-reading-companion__section-item"><a href="#Sec13" data-track="click" data-track-action="section anchor" data-track-label="link:Results and discussion">Results and discussion</a></li><li id="rc-sec-Sec19" class="c-reading-companion__section-item"><a href="#Sec19" data-track="click" data-track-action="section anchor" data-track-label="link:Conclusions and future directions">Conclusions and future directions</a></li><li id="rc-sec-availability-of-data-and-materials" class="c-reading-companion__section-item"><a href="#availability-of-data-and-materials" data-track="click" data-track-action="section anchor" data-track-label="link:Availability of data and materials">Availability of data and materials</a></li><li id="rc-sec-abbreviations" class="c-reading-companion__section-item"><a href="#abbreviations" data-track="click" data-track-action="section anchor" data-track-label="link:Abbreviations">Abbreviations</a></li><li id="rc-sec-Bib1" class="c-reading-companion__section-item"><a href="#Bib1" data-track="click" data-track-action="section anchor" data-track-label="link:References">References</a></li><li id="rc-sec-Ack1" class="c-reading-companion__section-item"><a href="#Ack1" data-track="click" data-track-action="section anchor" data-track-label="link:Acknowledgements">Acknowledgements</a></li><li id="rc-sec-author-information" class="c-reading-companion__section-item"><a href="#author-information" data-track="click" data-track-action="section anchor" data-track-label="link:Author information">Author information</a></li><li id="rc-sec-ethics" class="c-reading-companion__section-item"><a href="#ethics" data-track="click" data-track-action="section anchor" data-track-label="link:Ethics declarations">Ethics declarations</a></li><li id="rc-sec-additional-information" class="c-reading-companion__section-item"><a href="#additional-information" data-track="click" data-track-action="section anchor" data-track-label="link:Additional information">Additional information</a></li><li id="rc-sec-Sec20" class="c-reading-companion__section-item"><a href="#Sec20" data-track="click" data-track-action="section anchor" data-track-label="link:Additional files">Additional files</a></li><li id="rc-sec-rightslink" class="c-reading-companion__section-item"><a href="#rightslink" data-track="click" data-track-action="section anchor" data-track-label="link:Rights and permissions">Rights and permissions</a></li><li id="rc-sec-article-info" class="c-reading-companion__section-item"><a href="#article-info" data-track="click" data-track-action="section anchor" data-track-label="link:About this article">About this article</a></li></ul></div>
                                    <div class="js-ad">
    <div class="adsbox c-ad c-ad--MPU1">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/bmc/plantmethods/articles" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;doi=10.1186/s13007-019-0475-z;kwrd=Artificial intelligence,Banana,Deep learning,Disease detection,Transfer learning,Convolutional neural networks,Mobile app;pmc=L24000,L28000;" data-ad-type="MPU1" data-google-query-id="CLWy-c3onOYCFcK3fgodIIEJzg">
                
            <div id="google_ads_iframe_/270604982/bmc/plantmethods/articles_1__container__" style="border: 0pt none;"><iframe id="google_ads_iframe_/270604982/bmc/plantmethods/articles_1" title="3rd party ad content" name="google_ads_iframe_/270604982/bmc/plantmethods/articles_1" scrolling="no" marginwidth="0" marginheight="0" srcdoc="" data-google-container-id="1" style="border: 0px; vertical-align: bottom;" data-load-complete="true" width="300" height="250" frameborder="0"></iframe></div></div>
        </div>
    </div>
</div>
                                </div>
                                <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" aria-labelledby="tab-figures"><div class="c-reading-companion__scroll-pane"><ul class="c-reading-companion__figures-list"><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title" id="rc-Fig1">Fig.&nbsp;1</b></figcaption><picture><source type="image/webp" data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig1_HTML.png?as=webp"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig1_HTML.png" alt="figure1"></picture><p class="c-reading-companion__figure-links"><a href="#Fig1" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/1" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link">Full size image<svg width="16" height="16" class="u-icon"><use href="#global-icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title" id="rc-Fig2">Fig.&nbsp;2</b></figcaption><picture><source type="image/webp" data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig2_HTML.jpg?as=webp"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig2_HTML.jpg" alt="figure2"></picture><p class="c-reading-companion__figure-links"><a href="#Fig2" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/2" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link">Full size image<svg width="16" height="16" class="u-icon"><use href="#global-icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title" id="rc-Fig3">Fig.&nbsp;3</b></figcaption><picture><source type="image/webp" data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig3_HTML.jpg?as=webp"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig3_HTML.jpg" alt="figure3"></picture><p class="c-reading-companion__figure-links"><a href="#Fig3" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/3" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link">Full size image<svg width="16" height="16" class="u-icon"><use href="#global-icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title" id="rc-Fig4">Fig.&nbsp;4</b></figcaption><picture><source type="image/webp" data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig4_HTML.png?as=webp"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig4_HTML.png" alt="figure4"></picture><p class="c-reading-companion__figure-links"><a href="#Fig4" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/4" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link">Full size image<svg width="16" height="16" class="u-icon"><use href="#global-icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title" id="rc-Fig5">Fig.&nbsp;5</b></figcaption><picture><source type="image/webp" data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig5_HTML.jpg?as=webp"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig5_HTML.jpg" alt="figure5"></picture><p class="c-reading-companion__figure-links"><a href="#Fig5" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/5" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link">Full size image<svg width="16" height="16" class="u-icon"><use href="#global-icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title" id="rc-Fig6">Fig.&nbsp;6</b></figcaption><picture><source type="image/webp" data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig6_HTML.png?as=webp"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13007-019-0475-z/MediaObjects/13007_2019_475_Fig6_HTML.png" alt="figure6"></picture><p class="c-reading-companion__figure-links"><a href="#Fig6" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://plantmethods.biomedcentral.com/articles/10.1186/s13007-019-0475-z/figures/6" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link">Full size image<svg width="16" height="16" class="u-icon"><use href="#global-icon-chevron-right"></use></svg></a></p></figure></li></ul></div></div>
                                <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" aria-labelledby="tab-references"><div class="c-reading-companion__scroll-pane"><ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric"><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR1">FAO.
 Banana market review and banana statistics 2012–2013. Market and policy
 analyses of raw materials, horticulture and tropical (RAMHOT) Products 
Team. Rome; 2014.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR2">Lescot
 T. World plantain and banana production systems. In: Proceedings XX 
international meeting ACORBAT: 9–13 September 2013; Fortaleza; 2013. p. 
26–34.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR3">Abele
 S, Twine E, Legg C. Food security in eastern Africa and the great 
lakes. Crop Crisis Control Project final report. Ibadan: Int Instit Trop
 Agric; 2007.</p><ul class="c-reading-companion__reference-links"><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Food%20security%20in%20eastern%20Africa%20and%20the%20great%20lakes.%20Crop%20Crisis%20Control%20Project%20final%20report&amp;publication_year=2007&amp;author=Abele%2CS&amp;author=Twine%2CE&amp;author=Legg%2CC" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR4">Nagayets O. Small farms: current status and key trends. In: The future of small farms; 2005. p. 355.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR5">Blomme
 G, Dita M, Jacobsen KS, Perez Vicente L, Molina A, Ocimati W, Poussier 
S, Prior P. Bacterial diseases of bananas and enset: current state of 
knowledge and integrated approaches toward sustainable management. Front
 Plant Sci. 2017;8:1290.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.3389%2Ffpls.2017.01290" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Bacterial%20diseases%20of%20bananas%20and%20enset%3A%20current%20state%20of%20knowledge%20and%20integrated%20approaches%20toward%20sustainable%20management&amp;journal=Front%20Plant%20Sci&amp;volume=8&amp;publication_year=2017&amp;author=Blomme%2CG&amp;author=Dita%2CM&amp;author=Jacobsen%2CKS&amp;author=Perez%20Vicente%2CL&amp;author=Molina%2CA&amp;author=Ocimati%2CW&amp;author=Poussier%2CS&amp;author=Prior%2CP" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR6">Hillnhuetter
 C, Mahlein AK. Early detection and localisation of sugar beet diseases:
 new approaches. Gesunde Pflanzen. 2008;60(4):143–9.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.1007%2Fs10343-008-0196-0" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Early%20detection%20and%20localisation%20of%20sugar%20beet%20diseases%3A%20new%20approaches&amp;journal=Gesunde%20Pflanzen.&amp;volume=60&amp;issue=4&amp;pages=143-149&amp;publication_year=2008&amp;author=Hillnhuetter%2CC&amp;author=Mahlein%2CAK" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR7">Camargo
 A, Smith J. An image-processing based algorithm to automatically 
identify plant disease visual symptoms. Biosyst Eng. 2009;102(1):9–21.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.1016%2Fj.biosystemseng.2008.09.030" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=An%20image-processing%20based%20algorithm%20to%20automatically%20identify%20plant%20disease%20visual%20symptoms&amp;journal=Biosyst%20Eng&amp;volume=102&amp;issue=1&amp;pages=9-21&amp;publication_year=2009&amp;author=Camargo%2CA&amp;author=Smith%2CJ" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR8">Mohanty SP, Hughes DP, Salathe M. Using deep learning for image-based plant disease detection. Front Plant Sci. 2016;7:1419.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.3389%2Ffpls.2016.01419" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20deep%20learning%20for%20image-based%20plant%20disease%20detection&amp;journal=Front%20Plant%20Sci&amp;volume=7&amp;publication_year=2016&amp;author=Mohanty%2CSP&amp;author=Hughes%2CDP&amp;author=Salathe%2CM" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR9">Intelligence G. The mobile economy Africa 2016. London: GSMA; 2016.</p><ul class="c-reading-companion__reference-links"><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20mobile%20economy%20Africa%202016&amp;publication_year=2016&amp;author=Intelligence%2CG" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR10">Kamilaris A, Prenafeta-Boldu FX. Deep learning in agriculture: a survey. Comput Elect Agric. 2018;147:70–90.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.1016%2Fj.compag.2018.02.016" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20in%20agriculture%3A%20a%20survey&amp;journal=Comput%20Elect%20Agric&amp;volume=147&amp;pages=70-90&amp;publication_year=2018&amp;author=Kamilaris%2CA&amp;author=Prenafeta-Boldu%2CFX" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR11">Ramcharan
 A, Baranowski K, McCloskey P, Ahmed B, Legg J, Hughes DP. Deep learning
 for image-based cassava disease detection. Front Plant Sci. 
2017;8:1852.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.3389%2Ffpls.2017.01852" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20for%20image-based%20cassava%20disease%20detection&amp;journal=Front%20Plant%20Sci&amp;volume=8&amp;publication_year=2017&amp;author=Ramcharan%2CA&amp;author=Baranowski%2CK&amp;author=McCloskey%2CP&amp;author=Ahmed%2CB&amp;author=Legg%2CJ&amp;author=Hughes%2CDP" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR12">Siricharoen
 P, Scotney B, Morrow P, Parr G. A lightweight mobile system for crop 
disease diagnosis. International conference on image analysis and 
recognition. Berlin: Springer; 2016. p. 783–91.</p><ul class="c-reading-companion__reference-links"><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=International%20conference%20on%20image%20analysis%20and%20recognition&amp;pages=783-791&amp;publication_year=2016&amp;author=Siricharoen%2CP&amp;author=Scotney%2CB&amp;author=Morrow%2CP&amp;author=Parr%2CG" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR13">Wiesner-Hanks
 T, Stewart EL, Kaczmar N, DeChant C, Wu H, Nelson RJ, Lipson H, Gore 
MA. Image set for deep learning: field images of maize annotated with 
disease symptoms. BMC Res Notes. 2018;11(1):440.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.1186%2Fs13104-018-3548-6" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20set%20for%20deep%20learning%3A%20field%20images%20of%20maize%20annotated%20with%20disease%20symptoms&amp;journal=BMC%20Res%20Notes.&amp;volume=11&amp;issue=1&amp;publication_year=2018&amp;author=Wiesner-Hanks%2CT&amp;author=Stewart%2CEL&amp;author=Kaczmar%2CN&amp;author=DeChant%2CC&amp;author=Wu%2CH&amp;author=Nelson%2CRJ&amp;author=Lipson%2CH&amp;author=Gore%2CMA" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR14">Mwebaze
 E, Owomugisha G. Machine learning for plant disease incidence and 
severity measurements from leaf images.&nbsp;2016 15th IEEE 
international conference on machine learning and applications (ICMLA). 
New York: IEEE; 2016. p. 158–63.</p><ul class="c-reading-companion__reference-links"><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=2016%2015th%20IEEE%20international%20conference%20on%20machine%20learning%20and%20applications%20%28ICMLA%29&amp;pages=158-163&amp;publication_year=2016&amp;author=Mwebaze%2CE&amp;author=Owomugisha%2CG" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR15">Hughes
 D, Salathe M. An open access repository of images on plant health to 
enable the development of mobile disease diagnostics. arXiv preprint <a href="http://arxiv.org/abs/1511.08060">arXiv:1511.08060</a>; 2015.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR16">LabelImg Software. <a href="https://github.com/tzutalin/labelImg/">https://github.com/tzutalin/labelImg/</a>. Accessed 1 Feb 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR17">ImageNet Data Set. <a href="http://www.image-net.org/">http://www.image-net.org/</a>. Accessed 12 Mar 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR18">He
 K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition.
 In: Proceedings of the IEEE conference on computer vision and pattern 
recognition; 2016. p. 770–8.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR19">Ioffe
 S, Szegedy C. Batch normalization: Accelerating deep network training 
by reducing internal covariate shift. arXiv preprint <a href="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</a>. 2015.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR20">Howard
 AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam 
H. Mobilenets: efficient convolutional neural networks for mobile vision
 applications. arXiv preprint <a href="http://arxiv.org/abs/1704.04861">arXiv:1704.04861</a>. 2017.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR21">Huang
 J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A, Fischer I, Wojna Z, 
Song Y, Guadarrama S. Speed/accuracy trade-offs for modern convolutional
 object detectors. In: Proceedings of the IEEE conference on computer 
vision and pattern recognition; 2017. p. 7310–1.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR22">TensorFlow Python API. <a href="https://www.tensorflow.org/api_docs/python">https://www.tensorflow.org/api_docs/python</a>. Accessed 10 Feb 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR23">COCO Data Set. <a href="http://cocodataset.org/">http://cocodataset.org/</a>. Accessed 15 Feb 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR24">Reitermanova Z. Data splitting. In: WDS’10 proceedings of contributed papers, Part I, vol 10; 2010. p. 31–6.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR25">Liu
 W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC. Ssd: Single 
shot multibox detector. In: European conference on computer vision. 
Springer; 2016. p. 21–37.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR26">Object Detection API Loss Functions Implementation, Tensorflow. <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/core/losses.py">https://github.com/tensorflow/models/blob/master/research/object_detection/core/losses.py</a>. Accessed 5 Mar 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR27">Confusion Matrix for Object Detection. <a href="https://github.com/svpino/tf_object_detectioncm/blob/master/confusion_matrix.py">https://github.com/svpino/tf_object_detectioncm/blob/master/confusion_matrix.py</a>. Accessed 10 Mar 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR28">Object Detection API, Tensorflow. <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</a>. Accessed 20 Feb 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR29">Dandawate
 Y, Kokare R. An automated approach for classification of plant diseases
 towards development of futuristic decision support system in Indian 
perspective. In: 2015 international conference on advances in computing,
 communications and informatics (ICACCI), IEEE; 2015. p. 794–9.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR30">Mokhtar
 U, El Bendary N, Hassenian AE, Emary E, Mahmoud MA, Hefny H, Tolba MF. 
Svm-based detection of tomato leaves diseases. In: Intelligent Systems’ 
2014. Springer; 2015. p. 641–52.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR31">Brahimi
 M, Arsenovic M, Laraba S, Sladojevic S, Boukhalfa K, Moussaoui A. Deep 
learning for plant diseases: detection and saliency map visualisation. 
In: Human and machine learning. springer; 2018. p. 93–117.</p><ul class="c-reading-companion__reference-links"><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20and%20Machine%20Learning&amp;pages=93-117&amp;publication_year=2018&amp;author=Brahimi%2CMohammed&amp;author=Arsenovic%2CMarko&amp;author=Laraba%2CSohaib&amp;author=Sladojevic%2CSrdjan&amp;author=Boukhalfa%2CKamel&amp;author=Moussaoui%2CAbdelouhab" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR32">Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.1109%2FTKDE.2009.191" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20transfer%20learning&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=22&amp;issue=10&amp;pages=1345-1359&amp;publication_year=2010&amp;author=Pan%2CSJ&amp;author=Yang%2CQ" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR33">Fuentes
 A, Yoon S, Kim S, Park D. A robust deep-learning-based detector for 
real-time tomato plant diseases and pests recognition. Sensors. 
2017;17(9):2022.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.3390%2Fs17092022" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20robust%20deep-learning-based%20detector%20for%20real-time%20tomato%20plant%20diseases%20and%20pests%20recognition&amp;journal=Sensors&amp;volume=17&amp;issue=9&amp;publication_year=2017&amp;author=Fuentes%2CA&amp;author=Yoon%2CS&amp;author=Kim%2CS&amp;author=Park%2CD" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR34">Sun
 J, He X, Ge X, Wu X, Shen J, Song Y. Detection of key organs in tomato 
based on deep migration learning in a complex background. Agriculture. 
2018;8(12):196.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.3390%2Fagriculture8120196" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=Detection%20of%20key%20organs%20in%20tomato%20based%20on%20deep%20migration%20learning%20in%20a%20complex%20background&amp;journal=Agriculture&amp;volume=8&amp;issue=12&amp;publication_year=2018&amp;author=Sun%2CJ&amp;author=He%2CX&amp;author=Ge%2CX&amp;author=Wu%2CX&amp;author=Shen%2CJ&amp;author=Song%2CY" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR35">Everingham
 M, Eslami SA, Van Gool L, Williams CK, Winn J, Zisserman A. The pascal 
visual object classes challenge: a retrospective. Int J Comput Vision. 
2015;111(1):98–136.</p><ul class="c-reading-companion__reference-links"><li><a href="https://doi.org/10.1007%2Fs11263-014-0733-5" data-track="click" data-track-action="outbound reference" data-track-label="link">Article</a></li><li><a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20pascal%20visual%20object%20classes%20challenge%3A%20a%20retrospective&amp;journal=Int%20J%20Comput%20Vision.&amp;volume=111&amp;issue=1&amp;pages=98-136&amp;publication_year=2015&amp;author=Everingham%2CM&amp;author=Eslami%2CSA&amp;author=Gool%2CL&amp;author=Williams%2CCK&amp;author=Winn%2CJ&amp;author=Zisserman%2CA" data-track="click" data-track-action="outbound reference" data-track-label="link">
                        Google Scholar</a></li></ul></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR36">Zhang
 L, Lin L, Liang X, He K. Is faster r-cnn doing well for pedestrian 
detection? In: European conference on computer vision. Springer; 2016. 
p. 443–57.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation" id="rc-ref-CR37">Cuellar
 W, Mwanzia L, Lourido D, Garcia C, Martínez A, Cruz P, Pino L, Tohme J.
 PestDisPlace: monitoring the distribution of pests and diseases, 
version 2.0. International Center for Tropical Agriculture (CIAT); 2018.</p></li></ol></div></div>
                            </div>
                        </div>
                    </aside>
                </div>
            </div>
        </div>


        
            <div class="c-journal-footer">
                <div class="c-journal-footer__inner">
                    <div class="c-journal-footer__summary">
                        <h4 class="c-journal-title c-journal-title--footer">
                            <img class="c-journal-title__logo" src="logo.svg">
                            <span class="c-journal-title__text">Plant Methods</span>
                        </h4>
                         <p class="c-journal-footer__issn">ISSN: 1746-4811</p>
                    </div>
                    
                        <div class="c-journal-footer__contact">
                            <h4 class="c-journal-footer__contact-title c-journal-footer__contact-title--adjust-for-logo">Contact us</h4>
                            <ul class="c-journal-footer__contact-list">
                                
                                    <li class="c-journal-footer__contact-item">Submission enquiries: <a href="https://www.editorialmanager.com/plme" target="_blank">Access here and click Contact Us</a></li>
                                
                                
                                    <li class="c-journal-footer__contact-item">General enquiries: <a href="mailto:info@biomedcentral.com">info@biomedcentral.com</a></li>
                                
                            </ul>
                        </div>
                    
                </div>
            </div>
        
        
    <img rel="nofollow" class="tracker" style="display:none" src="s13007-019-0475-z.png" alt="">

         
    <footer>
        
            <div class="c-publisher-footer" data-test="publisher-footer">
    <div class="u-container">
        
        <div class="u-display-flex u-flex-wrap u-flex-justify-space-between" data-test="publisher-footer-menu">
            <div class="u-display-flex">
                
                    
                        <ul class="c-list-group c-list-group--sm u-margin-right-lg u-margin-bottom-md">
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="http://blogs.biomedcentral.com/">Read more on our blogs</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://www.biomedcentral.com/login">Receive BMC newsletters</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://www.biomedcentral.com/account">Manage article alerts</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://authorservices.springernature.com/go/10BMC">Language editing for authors</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="http://authorservices.springernature.com/scientific-editing/">Scientific editing for authors</a>
                                </li>
                            
                        </ul>
                    
                        <ul class="c-list-group c-list-group--sm u-margin-right-lg u-margin-bottom-md">
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://www.biomedcentral.com/about/policies">Policies</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://www.biomedcentral.com/accessibility">Accessibility</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://www.biomedcentral.com/about/press-centre">Press center</a>
                                </li>
                            
                        </ul>
                    
                        <ul class="c-list-group c-list-group--sm u-margin-right-lg u-margin-bottom-md">
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://support.biomedcentral.com/support/home">Support and Contact</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://biomedcentral.typeform.com/to/VLXboo">Leave feedback</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://www.biomedcentral.com/about/jobs">Careers</a>
                                </li>
                            
                        </ul>
                    
                
            </div>
            <div class="u-margin-bottom-lg">
                <h3 id="social-menu" class="u-text-sm u-reset-margin u-text-normal">Follow BMC</h3>
                <ul class="u-display-flex u-list-reset" data-test="footer-social-links">
                    
                        <li class="u-margin-top-xs u-margin-right-xs">
                            <a href="https://twitter.com/biomedcentral" class="u-gray-link">
                                <span class="u-visually-hidden">BMC Twitter page</span>
                                <svg class="c-icon" width="24" height="24" aria-hidden="true">
                                    <use xlink:href="#icon-twitter-bordered"></use>
                                </svg>
                            </a>
                        </li>
                    
                        <li class="u-margin-top-xs u-margin-right-xs">
                            <a href="https://www.facebook.com/BioMedCentral" class="u-gray-link">
                                <span class="u-visually-hidden">BMC Facebook page</span>
                                <svg class="c-icon" width="24" height="24" aria-hidden="true">
                                    <use xlink:href="#icon-facebook-bordered"></use>
                                </svg>
                            </a>
                        </li>
                    
                        <li class="u-margin-top-xs u-margin-right-xs">
                            <a href="http://www.weibo.com/biomedcentral" class="u-gray-link">
                                <span class="u-visually-hidden">BMC Weibo page</span>
                                <svg class="c-icon" width="24" height="24" aria-hidden="true">
                                    <use xlink:href="#icon-weibo-bordered"></use>
                                </svg>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
        <p class="u-reset-margin">
            By using this website, you agree to our
            <a class="u-gray-link" href="https://www.biomedcentral.com/terms-and-conditions">Terms and Conditions</a>,
            <a class="u-gray-link" href="https://www.biomedcentral.com/privacy-statement">Privacy
                statement</a> and
            <a class="u-gray-link" href="https://www.biomedcentral.com/cookies" data-test="cookie-link">Cookies</a> policy.
            
                <a class="optanon-toggle-display u-gray-link" href="javascript:void(0);">Manage the cookies</a> we use in the preference centre.
            
        </p>
    </div>
</div>

        
        <div class="c-corporate-footer">
    <div class="u-container">
        <img src="logo-springernature-44af1f90df.svg" class="c-corporate-footer__logo" alt="Springer Nature" itemprop="logo" role="img">
        <p class="c-corporate-footer__legal" data-test="copyright"> © 2019 BioMed Central Ltd unless otherwise stated. Part of
            <a class="c-corporate-footer__link" href="https://www.springernature.com/" itemscope="" itemtype="http://schema.org/Organization" itemid="#parentOrganization">Springer Nature</a>.
        </p>
    </div>
</div>

        
    </footer>

    </div>

    <noscript>
	<img src="nature.png" style="display: none" width="0" hidden="" height="0" border="0">
</noscript>



        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"></path>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"></path>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"></path>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"></path>
        </symbol>
    </svg>

        
<script data-test="app-bundle">
    (function() {
        if (window.config && window.config.mustardcut) {
            var appScript = document.createElement('script');
            
            appScript.src = '/static/js/app-bundle-94a94518a3.js';
            
            appScript.async = false;
            document.body.appendChild(appScript);
        }
    })();
</script><script src="app-bundle-94a94518a3.js"></script>




    
    
    <script>
        window.Component = {};
    </script>
    <script src="global-article-bundle-3dac6afee8.js"></script>


    




<script type="text/javascript" id="polyfill-matches">Element.prototype.matches||(Element.prototype.matches=Element.prototype.matchesSelector||Element.prototype.mozMatchesSelector||Element.prototype.msMatchesSelector||Element.prototype.oMatchesSelector||Element.prototype.webkitMatchesSelector||function(a){a=(this.document||this.ownerDocument).querySelectorAll(a);for(var b=a.length;0<=--b&&a.item(b)!==this;);return-1<b});</script><script type="text/javascript" id="gpt-control" src="gpt.js"></script>
<script type="text/javascript" id="krux-control-bmc">window.Krux||((Krux=function(){Krux.q.push(arguments)}).q=[]);(function(){var a=document.createElement("script");a.type="text/javascript";a.async=!0;a.src=("https:"===location.protocol?"https:":"http:")+"//cdn.krxd.net/controltag/KDqylSLE.js";var b=document.getElementsByTagName("script")[0];b.parentNode.insertBefore(a,b)})();</script>



<script type="text/javascript" id="krux-interchange-bmc">window.Krux||((Krux=function(){Krux.q.push(arguments)}).q=[]);(function(){function c(a){a="kxmacmillan_"+a;try{var b=window.localStorage}catch(e){b=null}return b?b[a]||"":navigator.cookieEnabled?(b=document.cookie.match(a+"\x3d([^;]*)"))&&unescape(b[1])||"":""}Krux.user=c("user");Krux.segments=c("segs")?c("segs").split(","):[];var a=[];Krux.user&&a.push("kuid\x3d"+Krux.user);for(var d=0;d<Krux.segments.length;d++)a.push("ksg\x3d"+Krux.segments[d]);Krux.dfppKeyValues=a.length?a.join(";")+";":""})();</script>





<script type="text/javascript" id="krux-consent">var allowed=google_tag_manager["GTM-TDGJHK"].macro(36);allowed=!0===allowed?1:0;Krux("consent:set",{dc:allowed,al:allowed,tg:allowed,cd:!1,sh:!1,re:!1},function(a,b){});</script>



<script type="text/javascript" id="">window.dataLayer.push({event:"kruxComplete"});</script>
<script type="text/javascript" id="ga-data-track-event-listeners">var setupHandler=google_tag_manager["GTM-TDGJHK"].macro(58);setupHandler("click");setupHandler("submit");</script><script type="text/javascript" id="crossmark-script" src="widget.js"></script><script type="text/javascript" id="recommended-script" src="entry-point"></script><script type="text/javascript" id="gpt-retrieve-ads">function splitKeys(a){var f=[],g="",h=[],k=a.split(";");for(a=0;a<k.length;++a){void 0!==l&&(g=l);var d=k[a].split("\x3d");var l=d[0];l!==g&&(0<g.length&&0<f.length&&h.push([g,f]),f=[]);if(2===d.length&&""!==d[0]&&""!==d[1]){var m=d[1].split(",");for(d=0;d<m.length;++d)f.push(m[d])}}0<l.length&&0<f.length&&h.push([l,f]);return h}
function splitSizes(a){var f=[];if(null!==a){var g=0<=a.indexOf("|")?a.split("|"):a.split(",");for(a=0;a<g.length;++a){var h=g[a].split("x");var k=parseInt(h[0],10);var d=parseInt(h[1],10);2===h.length&&!isNaN(k)&&!isNaN(d)&&0<=k&&0<=d&&f.push([k,d])}}return f}function debounce(a,f){var g=null,h=null;return function(){var k=this,d=Number(new Date),l=arguments;g&&d<g+f?(clearTimeout(h),h=setTimeout(function(){g=d;a.apply(k,l)},f)):(g=d,a.apply(k,l))}}
function addScrollEvent(a){window.addEventListener?window.addEventListener("scroll",a,!1):window.attachEvent("onscroll",a)}function removeScrollEvent(a){window.removeEventListener?window.removeEventListener("scroll",a,!1):window.detachEvent("scroll",a)}function getAdContainers(){return document.querySelectorAll("div[data-gpt-unitpath]")}function isLoggedIn(){return 0<=document.cookie.indexOf("OSCAR_SESSION_COOKIE")?"logged\x3dy;":"logged\x3dn;"}
(function(a,f){function g(e){var c="test";c=c.replace(/[\[\]]/g,"\\$\x26");c=new RegExp("[?\x26]"+c+"(\x3d([^\x26#]*)|\x26|#|$)");c=(c=c.exec(window.location.href))?c[2]?decodeURIComponent(c[2].replace(/\+/g," ")):"":null;c=c&&"ads"===c?"adtype\x3dtest;":"";e=e.getAttribute("data-gpt-targeting");var a=document.querySelector("[data-ad-targeting-search-terms]")?"search\x3d"+document.querySelector("[data-ad-targeting-search-terms]").innerText+";":!1;var b=isLoggedIn();c&&-1===e.indexOf(c)&&(e+=c);a&&
(e+=a);return e+b+q}function h(e,c){for(var a=e.length,b=[];a--;)c(e[a],a)&&(b.push(e[a].slot),e.splice(a,1));b.length&&googletag.pubads().refresh(b);return e}function k(e){var c=Math.max(document.documentElement.clientHeight,a.innerHeight||0);return h(e,function(a){var e=document.getElementById(a.divId),b=e.getBoundingClientRect();b=b.top-300;return c>b&&(0<e.offsetWidth||a.sizeArray&&a.sizeArray.length&&a.sizeArray[0].length&&2===a.sizeArray[0][0])})}function d(){googletag.cmd.push(function(){for(var a=
0;b[a];++a)googletag.display(b[a].divId)});googletag.cmd.push(function(){b=k(b)});var a=debounce(function(){googletag.cmd.push(function(){b=k(b);b.length||removeScrollEvent(a)})},250);addScrollEvent(a)}function l(a){var e=document.createElement("script");e.onload=function(){a(window.PubGrade)};e.async=!0;e.src="https://cdn.pbgrd.com/core-bmc.js";document.head.appendChild(e)}function m(a){!a||a&&a.fired?setTimeout(d,1):document.addEventListener("pbgrdFinished",d)}var q,r=getAdContainers(),b=[];f&&
(q=f.dfppKeyValues?f.dfppKeyValues:"");a.googletag=a.googletag||{};a.googletag.cmd=a.googletag.cmd||[];for(var p=0;r[p];++p){var n=r[p];splitKeys(g(n));b.push({divId:n.getAttribute("id"),adUnitPath:n.getAttribute("data-gpt-unitpath"),sizeArray:splitSizes(n.getAttribute("data-gpt-sizes")),targeting:splitKeys(g(n))})}googletag.cmd.push(function(){googletag.pubads().setRequestNonPersonalizedAds(google_tag_manager["GTM-TDGJHK"].macro(70)?0:1);googletag.pubads().disableInitialLoad();googletag.enableServices()});googletag.cmd.push(function(){for(var a=
0;b[a];++a)try{b[a].slot=googletag.defineSlot(b[a].adUnitPath,b[a].sizeArray,b[a].divId).addService(googletag.pubads());for(var c=0,d=b[a].targeting.length;c<d;++c)2===b[a].targeting[c].length&&""!==b[a].targeting[c][0]&&""!==b[a].targeting[c][1]&&b[a].slot.setTargeting(b[a].targeting[c][0],b[a].targeting[c][1])}catch(t){console.log("failed to create slot for",b[a])}});-1!==window.location.href.indexOf("/articles/")?l(m):m(null)})(window,window.Krux);</script><div style="display: none; visibility: hidden;">
<script type="text/javascript">window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"\x26":"?")+"lv\x3d1");c[b]||function(){var k=window,h=document,l=b,g=h.location.protocol,n="load",m=0;(function(){function b(){a.P(n);a.w=1;c[l]("_load")}c[l]=function(){function p(){p.id=e;return c[l].apply(p,arguments)}var e=++m;var b=this&&this!=k?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);p.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],l=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&l.push(c);h&&f.push(h);
return p};return p};var a=c[l]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,("https:"==g?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();k.addEventListener?k.addEventListener(n,b,!1):k.attachEvent("on"+n,b);var t=function(){function b(){return["\x3chead\x3e\x3c/head\x3e\x3c",e,' onload\x3d"var d\x3d',q,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",k,"\x3d'",a.l,"'\"\x3e\x3c/",e,"\x3e"].join("")}var e="body",c=h[e];if(!c)return setTimeout(t,
100);a.P(1);var d="appendChild",g="createElement",k="src",m=h[g]("div"),n=m[d](h[g]("div")),f=h[g]("iframe"),q="document";m.style.display="none";c.insertBefore(m,c.firstChild).id=r+"-"+l;f.frameBorder="0";f.id=r+"-frame-"+l;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[k]="javascript:false");f.allowTransparency="true";n[d](f);try{f.contentWindow[q].open()}catch(w){a.domain=h.domain;var u="javascript:var d\x3d"+q+".open();d.domain\x3d'"+h.domain+"';";f[k]=u+"void(0);"}try{var v=f.contentWindow[q];v.write(b());
v.close()}catch(w){f[k]=u+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(t,0)})()}();c[b].lv="1";return c[b]}var r="lightningjs",m=window[r]=g(r);m.require=g;m.modules=c}({});window.usabilla_live=lightningjs.require("usabilla_live","//w.usabilla.com/c27716fee63d.js");</script>
</div>
<script type="text/javascript" id="">window.usabilla_live("data",{custom:{kruxUser:google_tag_manager["GTM-TDGJHK"].macro(71),kruxSegments:google_tag_manager["GTM-TDGJHK"].macro(72),journalTitle:google_tag_manager["GTM-TDGJHK"].macro(73),pageType:google_tag_manager["GTM-TDGJHK"].macro(74),contentType:google_tag_manager["GTM-TDGJHK"].macro(75)}});</script>
<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: STIXSizeOneSym, sans-serif;"></div></div><iframe name="GoogleSetNPA" id="GoogleSetNPA" style="display:none;position:fixed;left:-999px;top:-999px;width:0px;height:0px;"></iframe><iframe src="https://event-tracker.springernature.com" title="EventTracker" id="event-tracker" style="display:none; visibility:hidden" aria-hidden="true" tabindex="-1" width="0" height="0"></iframe><div class="kxhead" data-id="KDqylSLE" style="display:none !important;"><span class="kxtag kxinvisible" data-id="20636"><script>
// this tag is intentionally blank
</script></span><span class="kxtag kxinvisible" data-id="20637"></span><span class="kxtag kxinvisible" data-id="20638"></span><span class="kxtag kxinvisible" data-id="20651"><script type="text/javascript">Krux('social.init');</script></span><span class="kxtag kxinvisible" data-id="36251"><script>
(function() {
    /* Selective Attribute DataLayer Library Tag */
    var _, allAttr, allowedList, attr, attributes, dataLayerIngester, dataObj,
        isAllowed, keepCase, normalizeName, omitKeys, optimizeNames, pageAttr, prefix,
        toSet, trim, userAttr, value,
        hasProp = {}.hasOwnProperty;
    _ = Krux('require:underscore');
    dataLayerIngester = Krux('require:scrape').ingestDataLayer;

    /* Safe copy of dataLayer object */
    dataObj = Krux('scrape.javascript', 'dataLayer[0]');

    /* String trimming helper function */
    trim = function(attr) {
        return ("" + attr).replace(/^\s+|\s+$/g, '');
    };

    /* Attribute configs */
    pageAttr = _.map('content.article.keywords,content.journal.title,content.category.publishingSegment,content.contentInfo.imprint,content.contentInfo.title,content.category.pmc.primarySubject,content.article.doi,content.article.articleType,content.category.contentType,content.category.contentSubType1,content.contentInfo.author,content.category.subjectType,content.category.subjectSubType1,content.authorization.type,content.attributes.keywords,content.contentInfo.collection[n].collectionName,content.article.journalIssueName,product.productInfo.brand,product.category.productType,product.productInfo.productName,product.productInfo.sku'.split(','), trim);
    userAttr = _.map('user.segment.subscriberMind,user.segment.subscriberSA,user.segment.customer,user.segment.registered,user.segment.appUserSAuser.segment.appUserMind'.split(','), trim);

    /* Create a array of attributes striping any empty strings */
    allAttr = _.without(pageAttr.concat(userAttr), '');

    /* Configuration settings */
    keepCase = 'true' === 'true';
    omitKeys = 'undefined'.split(',');
    prefix = "_";
    var domain = location.host.split('.'); //Assumption that the array can have max length of 4 eg. www.subdomain.domain.com
    if (domain.length === 4 && domain[0] === 'www') {
        prefix = domain[2] + '_';
    } else if (domain.length === 2) {
        prefix = domain[0] + '_';
    } else {
        prefix = domain[1] + '_';
    }


    /* Function to varify if attribute should be used */
    isAllowed = function(value, whitelist) {
        var i, len, str, x;
        str = "" + value;
        if (!((value !== null) && str.length > 0)) {
            return false;
        }
        for (i = 0, len = whitelist.length; i < len; i++) {
            x = whitelist[i];
            if (value.match(x) !== null) {
                return true;
            }
        }
        return false;
    };

    /* Function to standardise the attribute names */
    normalizeName = function(name, optimize) {
        if (optimize === null) {
            optimize = false;
        }
        if (keepCase) {
            return name;
        }
        return ("" + name).replace(/([A-Z])([A-Z]+)/g, function(full, first, rest) {
            if (optimize === true) {
                return "" + first + (rest.toLowerCase());
            }
            return "" + full;
        }).replace(/_*([A-Z])/g, '_$1').replace(/^_/, '').toLowerCase();
    };

    /* Creates a pattern/replacement config for dataLayer tool to clean up names */
    optimizeNames = function(names) {
        var config, i, len, name;
        config = [{
            pattern: /(\.)_/,
            replacement: '$1'
        }];
        for (i = 0, len = names.length; i < len; i++) {
            name = names[i];
            config.push({
                pattern: normalizeName(name),
                replacement: normalizeName(name, true)
            });
        }
        if (!prefix.match(/^_$|null|undefined|false/)) {
            config.push({
                pattern: /((?:page|user)_attr_)/,
                replacement: "$1" + prefix
            });

        }
        return config;
    };

    /* Get a full list of attributes usting the dataLayer tool */
    attributes = dataLayerIngester(dataObj, {
        omitKeys: _.without(omitKeys.concat([/gtm\./, /sessionid/i,
            /\.phpsessid$/i, /\.sid$/i, /\.zenid$/i,
            /\.requestid$/i
        ]), ''),
        omitValues: [/.*@.*(?:\..*)+/, /gtm\./, /^(https?:)?\/\/[^\/]+/,
            /^\/[^\/]+/, /.{255}/
        ],
        caseSensitive: keepCase,
        useFullPath: 'true' === 'true',
        useLastValue: 'false' === 'true',
        customDelimited: [/./],
        altDelimiter: ',',
        userKeys: _.map(userAttr, function(exp) {
            return new RegExp("(^|\\.)" + exp + "$");
        }),
        convertAttrNames: optimizeNames(allAttr)
    });


    /* Only set Attributes in the allowed list */
    allowedList = _.map(allAttr, function(name) {
        return new RegExp("(_attr_|_attr_" + prefix + "|\\.)" + (
            normalizeName(name, true)) + "$");
    });
    toSet = {};
    for (attr in attributes) {
        if (!hasProp.call(attributes, attr)) continue;
        value = attributes[attr];
        if (isAllowed(attr, allowedList)) {
            toSet[attr] = value;
        }
    }
    Krux('set', toSet);

})();
</script></span></div><div class="usabilla_live_button_container" role="button" tabindex="0" style="width:0px;height:0px;z-index:99999990;left:0px;top:50%;margin-top:-0px;position:fixed" aria-label="Usabilla Feedback Button"><iframe src="" marginwidth="0" marginheight="0" scrolling="no" data-tags="left" title="Usabilla Feedback Button" style="width: 0px; height: 0px; margin: 0px; padding: 0px; border: 0px; overflow: hidden; z-index: 9998; position: absolute; left: 0px; top: 0px; box-shadow: 0px 0px 0px; background-color: transparent;" frameborder="0"></iframe></div><iframe scrolling="no" src="https://recommended.springernature.com/v2364/generated/client.html?source=https://plantmethods.biomedcentral.com" name="uptodate-client" id="uptodate-client" title="Personalised recommendations" allowtransparency="true" style="width: 0px; height: 0px; bottom: 0px; right: 0px; z-index: 100000; position: fixed;" frameborder="0"></iframe><img src="logger_002.gif"><img src="logger.gif"><iframe id="google_osd_static_frame_77182885675" name="google_osd_static_frame" style="display: none; width: 0px; height: 0px;"></iframe><iframe id="kx-proxy-KDqylSLE" src="https://cdn.krxd.net/partnerjs/xdi/proxy.3d2100fd7107262ecb55ce6847f01fa5.html#!kxcid=KDqylSLE&amp;kxt=https%3A%2F%2Fplantmethods.biomedcentral.com&amp;kxcl=cdn&amp;kxp=" style="display: none; visibility: hidden; height: 0; width: 0;"></iframe></body></html>